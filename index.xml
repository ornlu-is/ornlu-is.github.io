<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Luís Franco</title><link>https://ornlu-is.github.io/</link><description>This is my cool site</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 28 Sep 2023 16:17:45 +0100</lastBuildDate><atom:link href="https://ornlu-is.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Adventures in Overengineering 1: Inventory</title><link>https://ornlu-is.github.io/overengineering_1/</link><pubDate>Thu, 28 Sep 2023 16:17:45 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/overengineering_1/</guid><description>A few months ago, I started a series of posts about my attempt to completely overengineer a simple Go web server. After a few posts, I had to archive that series. I was not pleased with the result, it wasn&amp;rsquo;t&amp;hellip; enough. Last time, I began this adventure by running a Kubernetes instance on my laptop. This time, I&amp;rsquo;ve gone deeper into the overengineering madness. Keep in mind that the end goal is still to deploy a very basic Golang web server.</description></item><item><title>Why does Go's io.Reader interface take a slice of bytes as argument?</title><link>https://ornlu-is.github.io/go_io_reader_interface/</link><pubDate>Tue, 22 Aug 2023 19:19:47 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_io_reader_interface/</guid><description>I rencetly watched a GopherCon talk titled &amp;ldquo;Understanding Allocations: The Stack and the Heap&amp;rdquo; by Jacob Walker, and found it really interesting, especially the final conclusion on why the io.Reader interface is the way it is. As it turns out, it is related to how memory allocation works in Go. More specifically, where the memory is allocated.
The io.Reader interface There isn&amp;rsquo;t much to say here, everyone that has been programming in Go has surely found this interface out in the wild multiple times, and it looks like this:</description></item><item><title>Go Concurrency Patterns: Tee Channel</title><link>https://ornlu-is.github.io/go_tee_channel_pattern/</link><pubDate>Mon, 21 Aug 2023 19:39:39 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_tee_channel_pattern/</guid><description>If you&amp;rsquo;ve ever used the Linux tee command, you can probably guess what this pattern is about. At a first glance, this might seem similar to the fan-out concurrency pattern and, in a way, it is. But there is one crucial difference. The fan-out concurrency pattern splits the input from one channel into several channels for concurrent processing, while the tee channel pattern creates two channels with the exact same data as the original one.</description></item><item><title>Go Concurrency Patterns: Pipeline</title><link>https://ornlu-is.github.io/go_pipeline_pattern/</link><pubDate>Mon, 21 Aug 2023 16:24:34 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_pipeline_pattern/</guid><description>Yet another Go concurrency pattern! This particular pattern is extremely helpful in composing several transformations from data incoming from a stream, and is know as the pipeline concurrency pattern. In a pipeline, we define several stages, which are nothing more than objects that take data in, perform some operation on it, and then output the transformed data.
Link to the code https://github.com/ornlu-is/go_pipeline_pattern The Pipeline pattern in Go Pipeline pattern Translating the above definition to Go, the pipeline pattern is simply a function that takes a channel, performs some operation on the data from that channel, and outputs it to another channel.</description></item><item><title>Go Concurrency Patterns: Fan-Out</title><link>https://ornlu-is.github.io/go_fan_out_pattern/</link><pubDate>Mon, 21 Aug 2023 14:40:12 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_fan_out_pattern/</guid><description>I have written a blog post about the fan-in concurrency pattern and, unlike most texts on this matter, I left its counterpart, the fan-out concurrency pattern, to have its own post. While these two patterns are mostly used in tandem, I believe that it is fundamental to understand them separately, so as to not create any mental blockers that would coherce us to only use one pattern when the other is also required.</description></item><item><title>Go Concurrency Patterns: Fan-In</title><link>https://ornlu-is.github.io/go_fan_in_pattern/</link><pubDate>Mon, 21 Aug 2023 09:38:20 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_fan_in_pattern/</guid><description>It is not uncommon to have a piece of software that is concurrently reading from multiple streams of data. However, for a multitude of possible reasons, we might want to aggregate these streams into a single one, for example, to send the data to another service. Fortunately, this is not a new problem, and the solution for it is well known as the Fan-In pattern.
Link to the code https://github.com/ornlu-is/go_fan_in_pattern The Fan-In pattern in Go Fan-In pattern As stated before, the idea behind this is incredibly simple: the fan-in pattern combines several data streams into one.</description></item><item><title>Enforcing test coverage in Go with Makefile</title><link>https://ornlu-is.github.io/go_makefile_code_coverage/</link><pubDate>Thu, 17 Aug 2023 23:22:19 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_makefile_code_coverage/</guid><description>Makefiles are a popular way of making the development process easier, since they can be used to chain several commands that allow developers to build, test, run, etc. their code. Additionally, they can also be used to create a make-based build/test system. In this post, I&amp;rsquo;m going to cover something how to set up a Makefile rule to test Golang code and enforce test coverage, i.e., have the rule fail if a predefined test coverage threshold is not met.</description></item><item><title>Go Design Patterns: Generator</title><link>https://ornlu-is.github.io/go_design_pattern_generator/</link><pubDate>Thu, 17 Aug 2023 22:13:52 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_design_pattern_generator/</guid><description>I like the generator pattern. I hadn&amp;rsquo;t realized it, but I had already encountered this pattern before when I used to program in Python. I recently found myself requiring to loop over a large sequence of numbers. Naively, I created a slice with all the values I required and then I looped over them. However, I was not satisfied with this solution so I went digging and found the generator pattern.</description></item><item><title>Using Go build tags for defining sets of tests</title><link>https://ornlu-is.github.io/go_build_tags/</link><pubDate>Wed, 16 Aug 2023 21:37:47 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/go_build_tags/</guid><description>Some time ago, I was looking for a way to define a clear separation in a Go project between my unit tests and my integration tests. At the time, the solution I came up with involved creating a submodule and some Makefile shenanigans to separate these into two sets. I was unhappy with this approach, since it ended up being convoluted and prone to error as the code base evolved. However, today I recently learned about Go&amp;rsquo;s build tags that seemlessly allow me to separate my integration tests from my unit tests.</description></item><item><title>Kubernetes deploy job failed but the service was deployed</title><link>https://ornlu-is.github.io/kubernetes_deploy_job_failed_but_my_service_was_deployed/</link><pubDate>Tue, 15 Aug 2023 19:49:34 +0100</pubDate><author>Luís Franco</author><guid>https://ornlu-is.github.io/kubernetes_deploy_job_failed_but_my_service_was_deployed/</guid><description>A while back, I was investigating a bug where my deployment job had failed, but the service had been deployed. At first I thought this was weird, afterwards I thought this was extremely concerning: had this happened before and I was just noticing now by accident? Since I did not want to have failed deployment jobs actually deploying my services, I took a closer look at this issue.
The situation What happened was pretty simple.</description></item></channel></rss>