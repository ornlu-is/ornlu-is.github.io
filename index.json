[{"categories":["Interesting Bugs"],"content":"An investigation on why a failed Kubernetes deploy job still deployed a newer service version","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"A while back, I was investigating a bug where my deployment job had failed, but the service had been deployed. At first I thought this was weird, afterwards I thought this was extremely concerning: had this happened before and I was just noticing now by accident? Since I did not want to have failed deployment jobs actually deploying my services, I took a closer look at this issue. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:0:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"The situation What happened was pretty simple. I updated the Kubernetes manifest files for my service, opened the PR, got it reviewed and approved, the CI builds that ran on my PR were all successful, life was good. So I merged the PR and, after a few minutes, I checked the version of the running service in the staging environment and saw that it was the most recent version. Everything seemed fine (little did he know that everything was not fine). I went about my life and opened a release PR, got it approved, CI builds looked good so I proceeded with the merge and consequent deployment. After a few minutes of celebrating my victory over yet-another-JIRA-ticket, I get an automated message on the chat: “Production deployment job failed”. I had just snatched defeat from the jaws of victory. The first thing I did after getting that message, was to check what was the version of the service running in production, and, to my surprise, it was the newer version. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:1:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"What actually happened I had to dig deeper into this, meaning I had to understand how the deployment job worked. The actual deployment job pretty much only did one thing: it ran kubectl apply on the manifests from my repository. And this was the command that caused the job to fail. So the underlying action of the deployment job failed, the deployment job itself failed, but the deployment happened. Just like in the Inception movie, “we need to go deeper”. There was only one suspect: kubectl apply. For my Kubernetes resources, I had several files in my repository that were combined into a single manifest file and kubectl apply was called to apply them all. However, one of the resources specified in the Kubernetes resource failed to be created. And I assumed that kubectl rolled back on the other resource it created since it failed to create all of them. This was my mistake: kubectl apply does not undo its changes if one of the resources failed to be created. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:2:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"How could this have been prevented Ideally, the deployment job would begin by performing kubectl apply, but with the --dry-run flag set. This flag can take two main values: client or server. If used with the former, the kubectl tool will attempt to validate the manifest without actually performing any request to the Kubernetes cluster. On the other hand, if used with the server value, then an actual request is performed to the Kubernetes cluster, which will attempt to validate it without creating any of the specified resources. However, operating under the constraints of different parts of software/infrastructure being owned by different teams, means that not all can be changed. Nonetheless, we can have the next best thing. Notice that I only got a chat alert message when the production deployment job failed. What about the staging deployment job? Had it failed too? Turns out, it had, but I did not get any chat message for a failed staging deployment because we hadn’t configured such alerts for our staging environment. It is a sub-optimal solution, but I can’t force other teams to change their stuff, I can only shrug and walk it off. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:3:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Golang"],"content":"A deep dive on the internals of Go slices","date":"2023-08-14","objectID":"/go_slices/","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Slices are Go’s bread and butter. However, more often than not, small mistakes happen because it isn’t exactly clear what actually is a slice. Since they are so prevalent in Go code, I decided to dive a bit deeper into their internal structure, how they are handled in different situations, and how some of the issues that arise can be avoided. ","date":"2023-08-14","objectID":"/go_slices/:0:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"But first, arrays Before diving into slices, it is important to know that Go also has the concept of arrays. An array is basically a numbered sequence of elements of the same type, known as the element type, and are useful when you are sure of how many memory positions you require. You can declare an array in the following ways. x := [3]int{} y := [3]int{1, 2, 3} z := [...]int{4, 5, 6, 7} The first example creates an array of size three with all its elements initialised as the element type’s default value. One very important detail here is that the size of an array is part of its type, which consequently means that you cannot specify an array’s size via a variable because its type must be resolved at compile time, and it also means that you cannot write functions that work with arrays of any size. The second example creates an array of size three with the given values and the last example foregoes the explicit integer specification of the array’s size in detriment of inferring it from the number of elements in the given slice literal. The last two details that we should be aware when working with arrays is that, in Go, arrays are values, which means that if you assign one array to another, you are copying all of its elements. Moreover, arrays in Go are comparable, meaning you can write the following: x := [...]int{2, 3} y := [...]int{3, 4} if x == y { // do something } ","date":"2023-08-14","objectID":"/go_slices/:1:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"What is a slice in Go? In Go, arrays are rarely used and are primarily a building block for the concept of slices. So… what are slices? A slice isn’t one thing, it’s actually three things: A pointer to an underlying array; The length of the underlying array; And the capacity of the underlying array. When you create a slice, you declare the slice’s element type, its length, and, optionally, its capacity. The Go runtime takes the given element type and capacity and allocates a contiguous memory segment of capable of holding a number of elements equal to the given capacity, and addresses the pointer to the beginning of this memory segment. Note that if you only specify the length and not the capacity of the slice, Go will assume that the capacity is equal to the length. Unlike arrays, slices are not comparable except with the nil value, which is the default value for a slice. ","date":"2023-08-14","objectID":"/go_slices/:2:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Initialising slices There are a few ways one can declare a slice: a := []int{1, 2} b := []int{} var c []int d := make([]int, 3) e := make([]int, 0, 2) Let us go through these one by one. The first declaration uses a slice literal to initialise the slice, meaning that it creates a slice populated with the given values in the order they are presented. This will infer both the length and capacity of the slice from the number of elements in the slice literal. The second one behaves much like the first one, but we are initialising a slice with an empty slice literal, which means that, if we print out the contents of b, we get the following: [] which is an empty slice. You might think that the third slice declaration produces a similar result to the second one, and that’s where you’re wrong. The third only declares a slice of integers, it does not perform any initialisation, meaning that its value is the slice default value, which is nil. The last two use the built-in make function to create slices. When creating slices, make expects either two or three arguments. The first is the type of slice you are trying to create, the second is that slice’s length, and the third is its capacity. When you use make to create a slice with non-zero length, keep in mind that you will be creating a slice with that number of elements with their values set to the element type’s default value. In other words, the output of the fourth slice declaration is: [0, 0, 0] However, if you just wish to allocate the memory for a slice, you can specify zero length and some non-zero capacity, much like the last declaration. Note that this will create an empty slice, on par with the second declaration, but the underlying allocated array will have the specified capacity. ","date":"2023-08-14","objectID":"/go_slices/:3:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Growing slices and why you should initialise them with make Let us say you have a slice of integers that is currently holding the numbers 1 and 2, and you want to add a third one. In Go, this is performed via the built-in append function, which takes as arguments the slice to which you wish to append new elements, and a variable number of new elements to append, and returns a copy of the resulting slice, which you tipically will use to override the original one: x := []int{1, 2} x = append(x, 3) If x was initialised from a slice literal that had two elements, then x has capacity and length both equal to two. But when I appended 3 to it, it now has length three which is more than the previous capacity. Which prompts the question: so what is the role of a slice’s capacity? To understand this, we need to know what goes on under the hood when we append something to a slice that is already at its capacity limit. When you append to a slice, you are adding one or more values to it, and each of these values will logically increase the slice’s length by one. The interesting bit happend when the length is already equal to the capacity. In this case, your slice has run out of space in the underlying array’s memory to add new elements. As such, the Go runtime will allocate a new slice with larger capacity, copy the original slice to the new one, add the new elements to it, and the new slice is then returned by append. Logically, these operations all take time. You are no longer just adding one element to a slice, you are creating a new slice, allocating memory, copying all elements to the new slice and only then you add a new element to it. Moreover, Go’s garbage collector will now have the additional task of freeing up the memory used by the old slice. As such, it is a good practice to create a slice with an upper bound on its capacity whenever possible: x := make([]int, 0, upperBound) This will avoid performing all the aforementioned extra computations and help you squeeze a tiny bit more of performance from your application. ","date":"2023-08-14","objectID":"/go_slices/:4:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Reslicing, a.k.a., how you’ll probably shoot yourself in the foot One operation on slices that is supported in Go is the slicing operation. This allows you to obtain a subset of a given slice: x := []int{1, 2, 3, 4} // [1, 2, 3, 4] y := x[:2] // [1, 2] But there is one caveat with slicing: it does not create a copy of the data. Instead, the new slice object created by slicing has a new length, but the same capacity as the original slice and its pointer is pointing to an element of the same underlying array. This effectively means that if you rewrite one of the elements of y, e.g., y[1]=666, it will also change the element in the same position for x. And the same holds true if you take a slice of a slice of a slice (and so on). Slicing is a powerful tool, but must be used with care, espectially when performing value assignments, since it might result in some unexpected behaviour. ","date":"2023-08-14","objectID":"/go_slices/:5:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Using copy We have seen slicing as a way of copying parts of a slice and potentially shoot ourselves in the foot. The obvious question that arises is: how can I safely copy the contents of a slice, to another slice, without having to deal will all these pointer shenanigans? Go has got your back with the built-in copy function. This function takes two arguments, a destination slice and a source slice, and copies as many elements of the source slice to the destination slice as the destination slice’s length allows. Additionally, it also returns the number of copied elements. For example, if you want to copy just the first two elements of a given slice into an entirely independent slice, you’d simply write: x := []int{1, 2, 3, 4} y := make([]int, 2) _ = copy(y, x) Now you can freely manipulate y without having to worry about what will happen to x. ","date":"2023-08-14","objectID":"/go_slices/:6:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"References https://go.dev/ref/spec#Slice_types https://go.dev/blog/slices-intro https://go.dev/doc/effective_go#slices Jon Bodner, “Learning Go”, O’Reilly, 1st Edition ","date":"2023-08-14","objectID":"/go_slices/:7:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Computer Networks"],"content":"An overview of the NetFlow v5 protocol","date":"2023-08-13","objectID":"/netflow_v5/","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"NetFlow is a protocol developed by Cisco that has had several iterations over the years. This network protocol’s main focus is to collect IP traffic information and export it for monitoring purposes. A router configured to collect NetFlow will aggregate data packets into what is known as a flow, which is basically a summary of the traffic passing through the device in a given time span. Of the multiple iterations of the NetFlow protocol that Cisco has produced, two have cemented themselves as the most commonly used protocols: NetFlow v5 and NetFlow v9. Today, I’ll focus on the NetFlow v5 description. ","date":"2023-08-13","objectID":"/netflow_v5/:0:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"The NetFlow v5 datagram When a device is configured to collect and send (which is usually performed via UDP) flow data according to the NetFlow v5 protocol, that device will periodically send NetFlow v5 datagrams which as composed of a header and a variable number of flows, as depicted in the image below: NetFlow v5 datagram Meaning that for each UDP packet you receive from your collecting device, you might get several flows worth of information. ","date":"2023-08-13","objectID":"/netflow_v5/:1:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"Header datagram specification The NetFlow v5 header is composed of 24 bytes, with each byte corresponding to the following information: Bytes Field Description 0-1 version NetFlow export format version number 2-3 count Number of flows exported in this packet 4-7 SysUptime Current time in milliseconds since the export device booted 8-11 unix_secs Current count of seconds since 0000 UTC 1970 12-15 unix_nsecs Residual nanoseconds since 0000 UTC 1970 16-19 flow_sequence Sequence counter of total flows seen 20 engine_type Type of flow-switching engine 21 engine_id Slot number of the flow-switching engine 22-23 sampling_interval First two bits hold the sampling mode; remaining 14 bits hold value of sampling interval While some of these fields may seem obvious, other require a bit more clarification. Obviously, since we are dealing with NetFlow v5, the version field will always correspond to the number five. The count field, while it is made out of two bytes, which might lead some to deduce that it is a number between zero and 65,536, this is not the case. In fact, the maximum number of flow in each NetFlow v5 datagram is 30. The engine_type field provides an identifier of how the collecting device handles flow-switching. Since NetFlow v5 is a proprietary protocol from Cisco, this is mapped into a Cisco-specific flow-switching technology. For monitoring purposes, this field requires a great deal of understanding of the underlying Cisco technologies and how your devices are configured to extract any type of meaningful information. For that reason, flow monitoring services tend to ignore this field. The engine_id field is a configurable identifier for the flow collector. Finally, the last field that requires a bit more understanding is the sampling_interval. A NetFlow collector doesn’t necessarily export flows that contain absolutely exact information on the observed traffic. For devices that handle large loads, this is usually not computationally efficient. As such, NetFlow collectors can be configured to employ sampling, where the flows will be calculated based on a reduced sample of the network’s IP traffic. Broadly speaking, there are two main sampling modes: Sampled NetFlow - flows are calculated based on x-th packet, i.e., if this is set to an interval of 10, then the flows reported contain are calculated based on the 1st, 10th, 20th, etc., packets. This sampling method is usually advised against, since it might hide periodic traffic patterns; Random Sampled NetFlow - flows are calculated based on random draws of incoming packets, much like random sampling. It provides on average consistency on the number of packets used to calculate the flows and is overall more statistically accurate than the Sampled NetFlow scheme. ","date":"2023-08-13","objectID":"/netflow_v5/:2:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"Flow datagram specification The flow datagram is a fair bit larger than the header, with each flow containing 48 bytes of data organised as follows: Bytes Field Description 0-3 srcaddr Source IP address 4-7 dstaddr Destination IP address 8-11 nexthop IP address of next hop router 12-13 input SNMP index of input interface 14-15 output SNMP index of output interface 16-19 dPkts Packets in the flow 20-23 dOctets Total number of Layer 3 bytes in the packets of the flow 24-27 First SysUptime at start of flow 28-31 Last SysUptime at the time the last packet of the flow was received 32-33 srcport TCP/UDP source port number or equivalent 34-35 dstport TCP/UDP destination port number or equivalent 36 pad1 Unused (zero) bytes 37 tcp_flags Cumulative OR of TCP flags 38 prot IP protocol type 39 tos IP type of service (ToS) 40-41 src_as Autonomous system number of the source, either origin or peer 42-43 dst_as Autonomous system number of the destination, either origin or peer 44 src_mask Source address prefix mask bits 45 dst_mask Destination address prefix mask bits 46-47 pad2 Unused (zero) bytes Let us clarify some of these fields. As you might’ve already noticed, the srcaddr and dstaddr fields, which correspond, respectively, to the source and destination IP addresses of the traffic, are only four bytes long. This means that NetFlow v5 is limited uniquely to IPv4 traffic, which is a testament to the fact that it was released a long time ago. The tcp_flags field is a single byte used to represent TCP flags via a cumulative OR. If you are not familiar with TCP, it is sufficient to understand that each TCP datagram contains information on the TCP flags, which describe what operation or outcome was performed/observed. There are eight possible flags, which makes it fairly straight-forward to one-hot encode these. Then, to get a single byte representation of all possible combinations, we simply take the cumulative OR function of their bytes. One of the most important fields in NetFlow v5 is the prot field, which pertains to the possible protocol. While it might seem confusing that the protocol is a single number, we can easily get the corresponding name via the mapping published by the Internet Assigned Numbers Authority (IANA). ","date":"2023-08-13","objectID":"/netflow_v5/:3:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"Some considerations on using NetFlow v5 for analytics There are two major considerations to take when building analytics with NetFlow v5. The first one is to understand that a flow is not an instant snapshot of your traffic. It is a summary of your traffic, collected over a given period of time, which is where the First and Last fields come in. These allow you to know for how long was data collected to calculate the given flow. However, you get absolutely no information on how the traffic was distributed inside that time range. This means that you have to make an assumption. A sensible one is to assume that, for short flow collection periods, traffic was more or less constant and, as such, you can uniformly distribute it across the time granularity you are using. There is nothing stopping you from considering more complicated schemes, but know that there is very little to gain from not taking the uniformly distributed approach. The last consideration pertains to the sampling_interval. This value is basically the inverse of a sampling rate applied by the flow collector, and it is important because you cannot calculate the amount of bytes or packets in a flow without it. If a flow has dOctets = 24 and the header has sampling_interval=10, then the number of bytes you have to report is 240, because your flow was calculated using only 10% of your network’s traffic. You will obviously incur in an approximation error with this, but your scope of operation is after the flow is collected, so you have no control over its sampling strategy. And even if you had, keep in mind that flow exports have to be produced at very high speeds and with minimal computational effort, which immediately discards the vast majority, if not all, sampling schemes that are not simple random sampling. ","date":"2023-08-13","objectID":"/netflow_v5/:4:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Docker","Grafana"],"content":"How to add Loki, promtail, and Grafana to a local docker compose stack","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"When testing software locally, one of the main tools at a software engineer’s disposal is Docker, more particularly, the Docker Compose tool. This tool allows engineers to define and run a multi-container setup using YAML files. The vast majority of software produces a form of output known as logs, which provide information on what is happening in a running application, such as errors, possible warnings, general information, and more. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:0:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"The issue with inspecting Docker Compose log messages When you run a Docker Compose setup locally, you’ll usually see all container logs being printed in rapid succession in your terminal, and, depending on the amount of logs your multi-container stack produces, this might be very difficult to navigate, reason about, and correlate log messages from different containers. Moreover, if you are interested in a single container’s logs, you’d have to get the container ID from the output of docker ps and then get the logs with docker logs \u003cCONTAINER ID\u003e. And even then, you’d only see them in plaintext format printed on the terminal, which is a poor way of navigating logs if there are many of them. Surely there must be a better way. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:1:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Your favourite super hero to the rescue: Loki and his sidekick Promtail Loki is a log aggregation system designed by Grafana Labs and, most of all, it’s open source software, which is important because I don’t like paying for stuff that I can get for free. To add Loki to a Docker Compose stack, we have to first understand how it works. In itself, Loki is just an aggregation and storage system, it does not collect the logs by itself, nor does it provide a nice user interface to explore them. This means that we need to other components: one for collecting the logs, and another to query and visualise them. To collect the logs, we’ll use a piece of software called Promtail, which is an agent that takes local logs and sends them to a Loki instance. Additionally, it also handles target discovery and attaching labels to log streams. Finally, to query and visualise the logs, we’ll simply use a Grafana instance where we’ll use our Loki instance as a data source. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:2:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"The application we are monitoring Since this is just an example to showcase how to set up this particular monitoring stack, I created the most basic application I could think of. All it does is start and print a Monty Python quote every second: package main import ( \"log\" \"time\" ) func main() { for { log.Println(\"Oh! Now we see the violence inherent in the system! Help, help, I'm being repressed!\") time.Sleep(1 * time.Second) } } Containerising the application To run this application with Docker Compose, it has to be containerised. For this, I used a build-step container and you can read more about this on a previous post that I wrote: https://ornlu-is.github.io/slim_docker_images/ ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:3:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Setting up the log monitoring stack with Docker Compose There are several steps to this process. The first one is to create a docker-compose.yaml file with the following contents: version: \"3.8\" services: loki: container_name: loki hostname: loki image: grafana/loki ports: - 3100:3100 command: -config.file=/etc/loki/local-config.yaml volumes: - ./loki:/etc/loki/ promtail: container_name: promtail hostname: promtail image: grafana/promtail command: -config.file=/etc/promtail/docker-config.yaml volumes: - ./promtail/docker-config.yaml:/etc/promtail/docker-config.yaml - /var/lib/docker/containers:/var/lib/docker/containers:ro - /var/run/docker.sock:/var/run/docker.sock depends_on: - loki grafana: container_name: grafana hostname: grafana image: grafana/grafana environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin - GF_AUTH_DISABLE_LOGIN_FORM=true ports: - 3000:3000 volumes: - ./grafana/provisioning/:/etc/grafana/provisioning/ depends_on: - promtail app: container_name: app hostname: app build: context: . dockerfile: Dockerfile labels: logging: \"promtail\" logging_jobname: \"container_logs\" depends_on: - grafana If you didn’t limit yourself to copying the YAML file above and actually read it, you probably have some questions. There are a bunch of bind mounts defined in our docker-compose.yaml file. Each of them serves a different purpose and we’ll go through them one by one. Bind mounts Bind mounts are basically files or directories on the host machine that are mounted into a container, meaning that the container can access these files/directories. For Loki to know how it is configured, it needs, you’ve guessed it, a configuration file! So we create a directory aptly named loki, and place a local-config.yaml file inside with the following configuration: auth_enabled: false server: http_listen_port: 3100 common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2020-10-24 store: boltdb-shipper object_store: filesystem schema: v11 index: prefix: index_ period: 24h Note that this is just the default Loki configuration for a Loki instance running locally, so there is nothing particularly noteworthy here. However, the same is not true for Promtail! As before, we create a promtail directory with a docker-config.yaml inside it: server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: scraper docker_sd_configs: - host: unix:///var/run/docker.sock refresh_interval: 5s filters: - name: label values: [\"logging=promtail\"] relabel_configs: - source_labels: [\"__meta_docker_container_name\"] regex: \"/(.*)\" target_label: \"container\" - source_labels: [\"__meta_docker_container_log_stream\"] target_label: \"logstream\" - source_labels: [\"__meta_docker_container_label_logging_jobname\"] target_label: \"job\" Let us unpack this. In our docker-compose.yaml file we have added two labels to our application: the logging and the logging_jobname labels, with values promtail and container_logs, respectively. This is what is going to be used by Promtail to know what to scrape, i.e., what logs to collect, and how to relabel them. Additionally, we also have to specify the host under docker_sd_configs carefully, because it must be unix:///var/run/docker.sock, which is the socket to which Docker writes the application logs. There is only one thing left to configure, and that is Grafana. We will create yet another directory with the most unexpected name ever, grafana, and we’ll create two additional directories inside it: datasources and provisioning. Inside provisioning, we’ll create a dashboard.yaml file with the following contents: apiVersion: 1 providers: - name: 'Loki' orgId: 1 folder: '' type: file editable: true options: path: /etc/grafana/provisioning/dashboards And, inside the datasources direc","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:4:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Pre-built dashboards in a Docker Compose Grafana instance It is obviously very boring and laborious to always have to create the dashboard from scratch just so we can explore the log messages properly. Fortunately, this actually only needs to be done once! Spin up the Docker Compose environment and navigate to your Grafana instance at http://localhost:3000. Create a dashboard as you normally would and then, when you save it, copy its JSON definition to the clipboard. To have this dashboard created by default, just paste this JSON into a my_dashboard.json file and place it inside the grafana/provisioning directory. And you’re done, you’ll never have to create that dashboard ever again. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:5:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Dealing with the ‘Unauthorized’ error from the Grafana dashboard There is a chance that you get an “Unauthorized” error when attempting to access the Grafana dashboard on your Docker Compose Grafana instance. Fret not because this is exceedingly easy to solve. Simply open http://localhost:3000 in a private/incognito browser and you’ll no longer have this issue. This error pops up whenever you have already opened another Grafana instance in your browser before and thus your browser saved some Grafana session cookies that it then attempts to use for the instance you are running with Docker Compose. Obviously/hopefully this fails because these cookies are not a valid authorization method for the Grafana instance you just spun up. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:6:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Solving the ’too many outstanding requests’ error At the time of writing this, Loki is not shipped with sensible defaults. This translates to, if you build a few panels in a single dashboard, you start getting empty panels and a “Too many outstanding requests” error in Grafana (I got this error after adding 6 panels, which is very little). This is because Loki is shipped with a few limits on how many requests from Grafana it can handle. After tinkering with these limits for a while, I found that adding the following lines to the Loki config file solved that issue: limits_config: split_queries_by_interval: 24h max_query_parallelism: 100 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 And now you’re all set to efficiently explore logs from Docker Compose environments. Link to the code https://github.com/ornlu-is/docker_compose_loki_example ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:7:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Clickhouse"],"content":"A simple way to run SQL scripts on Clickhouse Docker container startup.","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"This is a neat little trick I learned recently. I was in need of creating a Clickhouse container to attach to another application that would write to it. And I found myself thinking: it would be really convenient if there were a way to create a Clickhouse container that already comes with a set of tables created. Most of the solutions I found only were severely convoluted and relied on scripts to make this happen, so I thought that there must surely be a better way. As it turns out, there is, and this post is precisely about that. ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:0:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"The Docker image The first step is to specify our Clickhouse Dockerfile: FROM yandex/clickhouse-server ADD ./docker-entrypoint-initdb.d/ /docker-entrypoint-initdb.d Basically, all we do is take the base Clickhouse image and copy a local directory called docker-entrypoint-initdb.d into a folder of the same name inside the container. This is a special folder that Clickhouse will peer inside when being started as a Docker container. It will look for SQL scripts inside this directory and execute them on start up. Which means that, if we want a Clickhouse Docker image that already has a few existing tables, all we have to do is specify a few SQL scripts and write an exceedingly simple Dockerfile! ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:1:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"The SQL script This brings us to the second step: writing a basic SQL script so we can see this in action. Inside the docker-entrypoint-initdb.d directory, let’s write the following script: CREATE TABLE IF NOT EXISTS random_table ( `field1` Float, `field2` Float, `field3` Float ) ENGINE = Memory() We will name our script 1_create_random_table.sql so that we know what it does and we can be sure that it is the first script to run (assuming other scripts are also prefixed by numbers). Another important detail is that we added added the IF NOT EXISTS modifier to our CREATE TABLE statement. This was no accident. To understand its purpose, consider the following scenario. You build and start your container and you see your tables have been written to Clickhouse, and, to celebrate this success, you stop the container a go grab a quick beer. While drinking the aforementioned beer, you brag to someone about your most recent accomplishment, but they do not believe you. With a fiery smirk, you prompt that someone to follow you to your computer. And there you go, half empty beer in hand, confidently walking to your computer. You sit down, type the command to start the container and… your container exits with a failure code immediately after starting. Your beer is suddenly warm, your confidence is gone, and the embarassment you feel from this debacle will keep you awake at night and eventually be the main cause of your divorce. Now, the lesson here is simple: without the IF NOT EXISTS modifier, when you run the container a second time without removing the leftover container from the first time you ran it, it will attempt to create the table again, and fail because it already exists, and then you’ll end up going through a ravaging divorce. Nobody wants that, so use this modifier. ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:2:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"Inspecting the container Considering our SQL script is now divorce-proof, all that is left is for us is to test this out. In the same directory as our Dockerfile, simply run: docker build . -t 'ch-sql' where ch-sql is the tag I decided to give this image, but you can call it whatever you’d like. Now, when Docker finishes building the image, we can check its existence by inspecting the output of the docker images command: REPOSITORY TAG IMAGE ID CREATED SIZE ch-sql latest c3ec2b44d1e8 46 seconds ago 826MB Great, since our image is there, we are all set to create the container using docker run ch-sql, which outputs the following: Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Logging trace to /var/log/clickhouse-server/clickhouse-server.log Logging errors to /var/log/clickhouse-server/clickhouse-server.err.log Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/config.xml'. Processing configuration file '/etc/clickhouse-server/users.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/users.xml'. /entrypoint.sh: running /docker-entrypoint-initdb.d/1_create_random_table.sql Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Logging trace to /var/log/clickhouse-server/clickhouse-server.log Logging errors to /var/log/clickhouse-server/clickhouse-server.err.log Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/config.xml'. Processing configuration file '/etc/clickhouse-server/users.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/users.xml'. I did not add the empty lines that separate the our script’s execution, Clickhouse did that itself, which is pretty convenient. All that is left is to exec into the container and examine if our table was actually created. Grab the container ID from the output of docker ps, and open an interactive terminal session inside the container with: docker exec -it \u003cCONTAINER-ID\u003e bash If you’ve managed to get here without any mistakes, you should now be inside the container image we built. Starting the Clickhouse client is very simple, all you have to do is type clickhouse-client and press Enter, and you’ll be presented with a Clickhouse prompt: ClickHouse client version 22.1.3.7 (official build). Connecting to localhost:9000 as user default. Connected to ClickHouse server version 22.1.3 revision 54455. CONTAINER_ID :) To check if our table exists, simply write SHOW TABLES FROM default, where default is the default database where we’ve created our table, well, by default. This will give us the following: SHOW TABLES FROM default Query id: 8052df88-1463-487a-a31d-9e8c504181c3 ┌─name─────────┐ │ random_table │ └──────────────┘ 1 rows in set. Elapsed: 0.005 sec. So now you’re all set to execute start up SQL scripts on a Clickhouse Docker container, hopefully this will end up saving your marriage! Link to the code https://github.com/ornlu-is/clickhouse_docker_start_up_scripts ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:3:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Golang"],"content":"A Git hook example for Go developers","date":"2023-07-31","objectID":"/git_hooks_for_go/","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"I mostly code in Go, which comes with the handy go tool. This tool has a bunch of functionalities with one of the most important (at least for me) being the ability to run tests. I love writing tests for my code because I hate being paged when I am on call. However, whenever I open a PR, sometimes I forget to run the tests locally before pushing my code and then my code ends up failing the CI builds, which overall results in a slower development process. Fortunately, I’m in the business of automation, and there is one particular tool that I can leverage so that I do not have to remember to run the tests every time: `git`` hooks. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:0:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"git hooks git hooks is just a fancy name for scripts that get executed when certain git events occur. It really is that simple. However, it is important to take a while to understand when these scripts are triggered. We will focus our attention on local hooks, which are ones that run on git events that happen in our machine and not in the remote repository. There are four hooks that allow us to have scripts being executed at each step of a commit’s lifecycle: pre-commit - after running git commit but before being prompted for a commit message; prepare-commit-msg - after the previous hook and is used to populated the commit message; commit-msg - after the commit message is entered; post-commit - immediately after the previous hook, but has the downside of not being able to change the outcome of git commit. Out of all of these options, we want one that will block the git commit command if any of the tests fails, which immediately excludes the last hook. And, to be honest, it is not necessary to have a the hook run after entering the commit message because if then the tests fail, I’ll have to fix them and enter the commit message once more. Since there is no need to populate the commit message with anything special, I’ll be going with a pre-commit hook. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:1:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"A Go example I need an example project to show this in action. So I came up with the following main.go file: package main import \"fmt\" func isEven(num int) bool { if num%2 == 0 { return true } return false } func main() { fmt.Println(\"1 is even?\", isEven(1)) fmt.Println(\"32 is even?\", isEven(32)) } As you can see, this Go application simply prints if 1 and 32 are even numbers. But it has an isEven function, for which we have written the following unit tests in the main_test.go file: package main import \"testing\" func TestIsEven(t *testing.T) { for _, tc := range []struct { name string givenNumber int expectedResult bool }{ { name: \"given an even number returns true\", givenNumber: 666, expectedResult: true, }, { name: \"given an odd number returns false\", givenNumber: 333, expectedResult: false, }, } { t.Run(tc.name, func(t *testing.T) { res := isEven(tc.givenNumber) if res != tc.expectedResult { t.Errorf(\"expected result %t, but got %t\", tc.expectedResult, res) } }) } } This is just an illustrative example, hence why the Go application is so uninteresting. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:2:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"Implementing a Python git hook We can can choose from several scripting languages to implement our hook. For this example, I chose to implement it in Python, for no particular reason. For that matter, I created a directory hooks/ and a file within it called pre-commit with the following code: #!/usr/bin/python3 import sys import subprocess process_output = subprocess.run([\"go\", \"test\", \"github.com/ornlu-is/go_git_hooks_example\"], text=True, capture_output=True) print(process_output.stdout) sys.exit(process_output.returncode) If you are not familiar with this, the first line of the script tells our computer which interpreter the script has to be ran with. In this case, it is Python 3, so if you do not have Python 3 installed, this Git hook will fail. Keep in mind that git hooks fails for any non-zero exit code, hence the last line of the script. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:3:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"Adding Git hooks to our Go program But how do we actually run this? If you’ve explored the .git/ directory before, you probably know that it contains a hooks/ directory where, you’ve guessed it, git hooks usually live. But this makes it very hard to share hooks with your team members, since the contents of the .git folder are not added to the remote repository. My hooks are in the ./hooks/ directory, so I have to somehow tell git that this is where they are. Most recommendations for this paradigm revolve around using creating symlinks, but there is one very neat (and simple) one-liner that completely solves our issue: git config core.hooksPath hooks/ This line basically rewires git so that it looks for hooks in the ./hooks directory instead of in the ./.git/hooks/ directory! Moreover, this only affects the repository you are working on, meaning that if you have some different behaviour for another repository, it will be preserved. There is only one last step: our need to be executable, meaning that we just need to run: chmod +x hooks/pre-commit And we now have a functioning pre-commit git hook that will run our Go tests for us whenever we use the git commit command! Link to the code https://github.com/ornlu-is/go_git_hooks_example ","date":"2023-07-31","objectID":"/git_hooks_for_go/:4:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Interesting Bugs"],"content":"CIDR handling in Go and Postgres do not match and that might cause bugs","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"I came across an interesting bug in the past few days. I had a very simple Go program that had a single purpose: it would take some user input, process that input, and then write it to a database. However, the program would sometimes fail, when given input that apparently was valid. And I thought that this bug was interesting enough to write about it, so here we are. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:0:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"Understanding CIDR notation Before diving into the actual bug, it is fundamental that we understand CIDR notation. Classless Internet Domain Routing, or CIDR, is notation used to define subnetworks, which is composed of an IP address followed by a forward slash and a number between zero and thirty two, e.g., 6.6.6.0/24. From a given CIDR, we can extract some information about the subnetwork: Network ID - this is what allows us to identify the network and corresponds to the IP address represented in the CIDR. In the example CIDR given above, this would be 6.6.6.0; Broadcast IP - this is the IP address that is used to broadcast messages to IP addresses in the network and corresponds to the last IP address in the IP range. for our example, it would be 6.6.6.255; First useable host IP - obviously, the actual first IP is the network ID, but that IP is not useable. However, the IP immediately after that one can actually be used and, in the context of our example, it would 6.6.6.1; Last useable host IP - similartly to the first useable host IP, the actual last IP is not useable as a host, but the one immediately before it is! So, for our running example, the last useable host IP would be 6.6.6.255; Number of IP addresses - this is fairly easy to calculate from the CIDR notation. Simply take the number that follows the forward slash, subtract it to 32, then take the power of 2 of the corresponding result. E.g., $32 - 24 = 8$, $2^8 = 256$, so for a /24 subnetwork, we have 256 IP addresses. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:1:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"The bug The functioning of my program was very simple. Programmed in Go, it would take a user given CIDR, parse it, and then write that CIDR into a Postgres database. Most of the times, this program would work perfectly, as expected. But, sometimes, it would fail. For example, for the input “6.6.6.0/24”, the program would function normally, but, for the input “3.3.1.0/16”, Postgres would throw an error. Now, there are two possible places where this error might be coming from: either from the Go application or the Postgres database. The Postgres table where the data was being written had a column of type cidr. From inspecting the documentation on Postgres network address types, we see that we really only had two options for this: cidr or inet, with the main difference between the two of them being that cidr does not allow for non-zero bits to the right of the network mask. With this bit of information, it should already be obvious what was going on: “3.3.1.0/16” is not a proper CIDR! The correct CIDR for the network it is trying to represent would be “3.3.0.0/16”. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:2:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"Golang CIDR parsing It is now very clear why Postgres was throwing an error, but why did this get through our Go application? The code snippet responsible for checking the CIDRs looks something like this: func checkCIDR(givenCIDR string) (*net.IPNet, error) { addr, inet, err := net.ParseCIDR(givenCIDR) if err != nil { return nil, fmt.Errorf(\"error parsing CIDR: %w\", err) } return \u0026net.IPNet{ IP: addr, Mask: inet.Mask, }, nil } If you take this code snippet and try to run it with the improper CIDR “3.3.1.0./16”, you’ll see that the code runs without returning an error! This is because the ParseCIDR function of the net package infers the subnet the CIDR refers to even if it is not a proper CIDR. If you look carefully at its returned values, we get both an IP address and an IP subnet. The IP address will correspond to the IP address given in the, sometimes improper, CIDR while the IP network will correspond to the inferred network. Meaning that Go would calculate the inferred network, but when we returned the value, we would return the improper CIDR, which then led to a Postgres error. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:3:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"The fix There were two possible ways of fixing this: either allowing users to specify improper CIDRs and having the application infer the correct network, or have the application reject all improper CIDRs. Ultimately, this is more of a user experience question. I prefer forcing users to know what they are playing around with, so, in my opinion, the fix should be to reject all improper CIDRs. This means that, we need to change the function that checks the CIDR to the following: func checkCIDR(givenCIDR string) (*net.IPNet, error) { addr, inet, err := net.ParseCIDR(givenCIDR) if err != nil { return nil, fmt.Errorf(\"error parsing CIDR: %w\", err) } if !addr.Equal(inet.IP) { return nil, fmt.Errorf(\"invalid CIDR provided\") } return \u0026net.IPNet{ IP: addr, Mask: inet.Mask, }, nil } You might, obviously, not want to take the same approach to fix this as I did, and that is completely fine: there is no universally correct approach to tackle this bug. Nonetheless, I thought this bug was a great learning opportunity and hopefully so do you. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:4:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"References Postgres documentation on network address types - https://www.postgresql.org/docs/current/datatype-net-types.html Golang net package documentation - https://pkg.go.dev/net ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:5:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Golang"],"content":"When writing software, one might, accidentally (or not), ship the software with vulnerabilities, which are broadly defined as flaws or weaknesses in code that can be exploited by an attacker. We do not want to have those in our Go code so we need some way of minimizing the number of vulnerabilities our code has. Fortunately there are tools built by the Go community and team that can be leveraged for this. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:0:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"CWEs and CVEs Before introducing Go tools to make your code more secure, it is important to clarify two concepts that are commonly used in the infosec industry: CWE and CVE. The first stands for Common Weakness Enumeration and refers to a catalog of weaknesses in software components. It is important to keep in mind that this does not refer to specific instances of vulnerabilities, but rather types of weaknesses that are usually found in software, hardware or firmware. The second term stands for Common Vulnerabilities and Exposures, and is a standard for identifying and distributing information on vulnerabilities on specific instances of products or systems. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:1:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"gosec A popular Go tool that is sometimes ran as a linter, gosec scans your code’s abstract syntax tree for security problems. Since this works by examining an AST and is programmed as a set of rules, it is limited in its detection scope. In fact, at the time of writing this article, it can only detect 34 issues. Moreover, each of these possible detections is mapped into a CWE, as described in gosec’s GitHub repository. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:2:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"govulncheck Created by the Go security team, govulncheck is a bit more sophisticated than gosec. The Go security team gather data on known CVEs from multiple sources, puts these through a curation process, and makes this information publicly available. Moveover, this team also built the govulncheck tool, which allows us to check for these known vulnerabilities via source code inspection. However, if we really want to, it can also analyse binaries, but at the expense of information on call stacks. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:3:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"Why you should run both these tools gosec looks for known CWEs, which may or may not result in CVEs, while govulncheck looks for known CVEs, making this pair a powerful stack to improve the security of you code. Let us craft an example where we can see these tools in action and how they differ. Create a new Go project, with version 1.19 (this is important, otherwise you will not be able to reproduce this), and place the following code in your main.go file: package main import ( \"fmt\" \"math/rand\" \"golang.org/x/text/unicode/norm\" ) func main() { word := \"cão\" // \"dog\" in portuguese nfc := norm.NFC.String(word) nfd := norm.NFD.String(word) fmt.Printf(\"NFC/NFD: %s/%s\\n\", nfc, nfd) fmt.Printf(\"This is a random number: %f\", rand.Float64()) } This will be our test subject. At a glance, there seems to be nothing wrong with our code, but let us see what gosec and govulncheck have to say about that. After following the documentation and installing these tools, they are fairly easy to run. Let us start by looking for CWEs with gosec, which can be done via the following command: gosec ./... When we run this, we get the following output: [gosec] 2023/07/26 23:44:50 Including rules: default [gosec] 2023/07/26 23:44:50 Excluding rules: default [gosec] 2023/07/26 23:44:50 Import directory: /home/luis/Projects/go_vulnerabilities_example [gosec] 2023/07/26 23:44:51 Checking package: main [gosec] 2023/07/26 23:44:51 Checking file: /home/luis/Projects/go_vulnerabilities_example/main.go Results: [/home/luis/Projects/go_vulnerabilities_example/main.go:17] - G404 (CWE-338): Use of weak random number generator (math/rand instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH) 16: fmt.Printf(\"NFC/NFD: %s/%s\\n\", nfc, nfd) \u003e 17: fmt.Printf(\"This is a random number: %f\", rand.Float64()) 18: } Summary: Gosec : 2.16.0 Files : 1 Lines : 18 Nosec : 0 Issues : 1 This tool immediately flagged the piece of code we were using to display a random number. Of course, for what the code does, this is a false positive result, but that isn’t always the case (and this is for illustrative purposes only). We can just as easily run govulncheck with: govulncheck ./... Which, in turn, outputs the following report: Using go1.19.1 and govulncheck@v1.0.0 with vulnerability data from https://vuln.go.dev (last modified 2023-07-24 16:24:24 +0000 UTC). Scanning your code and 44 packages across 1 dependent module for known vulnerabilities... Vulnerability #1: GO-2023-1840 Unsafe behavior in setuid/setgid binaries in runtime More info: https://pkg.go.dev/vuln/GO-2023-1840 Standard library Found in: runtime@go1.19.1 Fixed in: runtime@go1.20.5 Example traces found: #1: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.Callers #2: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.CallersFrames #3: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.Frames.Next #4: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.GOMAXPROCS #5: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.KeepAlive #6: main.go:4:2: go_vulnerabilities_example.init calls fmt.init, which eventually calls runtime.SetFinalizer #7: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.TypeAssertionError.Error #8: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.defaultMemProfileRate #9: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.efaceOf #10: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.findfunc #11: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.float64frombits #12: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.forcegche","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:4:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"References gosec - https://github.com/securego/gosec govulncheck - https://go.googlesource.com/vuln ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:5:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"How to implement the functional options design pattern in Golang","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Golang"],"content":"The Functional Options pattern is a rather elegant manner of implementing a Golang struct constructor that allows for custom default values, which means that users of the API we are implementing will only need to specify the struct attribute values that the users deem that shouldn’t take their default values. For our example, let us consider the very simple use case where we have a package named person containing a Person struct, which will look like this: type Person struct { ID int Name string Age int Email string Address string } ","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/:0:0","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Golang"],"content":"Before Functional Options To initialize such an instance of the aforementioned struct, one would usually implement a function called New, which would take a bunch of values as parameters and return the struct with the given values, like so: func New(id int, name string, age int, email string, address string) *Person { return \u0026Person{ ID: id, Name: name, Age: age, Email: email, Address: address, } } To create a new Person, one would have to call that function and pass in every single value: p := person.New(1, \"John Doe\", 25, \"johndoe@example.com\", \"Nowhere\") This might get cumborsome in the case where there are a lot of attributes and some of them might require more intricate knowledge of the inner workings of the package. ","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/:1:0","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Golang"],"content":"Functional Options Pattern Functional options to the rescue. The idea behind this pattern is fairly simple, we will have New become a variadic function which will accept any number of arguments of the type Option, which we define as: type Option func(*Person) Then, for every struct attribute that should have a default value, we implement a function of the following form: func WithAttributeName(attributeValue attributeType) Option { return func(p *Person) { p.AttributeName = attributeValue } } For our case, let us say we want all attributes except ID to have default values. In that case, we’d end up with something like: func WithName(name string) Option { return func(p *Person) { p.Name = name } } func WithAge(age int) Option { return func(p *Person) { p.Age = age } } func WithEmail(email string) Option { return func(p *Person) { p.Email = email } } func WithAddress(address string) Option { return func(p *Person) { p.Address = address } } Finally, all we need is adapt our New function to handle these other functions as parameters and to set some default values. This is simply boils down to creating the Person struct with the default values we want and then looping over and calling the given Options: func New(id int, opts ...Option) *Person { p := \u0026Person{ ID: id, Name: \"John Doe\", Age: 25, Email: \"johndoe@example.com\", Address: \"Nowhere\", } for _, opt := range opts { opt(p) } return p } Note that this method sets all attributes as optional except for the ID of the Person. Below is an example of initializing Person instances with functional options: func main() { unknownPerson := person.New(1) aragorn := person.New( 2, person.WithName(\"Aragorn II\"), person.WithAddress(\"Rivendell\"), person.WithAge(118), person.WithEmail(\"aragorn@mithrilmail.com\"), ) fmt.Printf(\"%+v\\n\", *unknownPerson) fmt.Printf(\"%+v\\n\", *aragorn) } This produces the following output: {ID:1 Name:John Doe Age:25 Email:johndoe@example.com Address:Nowhere} {ID:2 Name:Aragorn II Age:118 Email:aragorn@mithrilmail.com Address:Rivendell} Link to the code https://github.com/ornlu-is/go_functional_options ","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/:2:0","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Docker"],"content":"How to get slim Docker images using build-step containers.","date":"2023-07-15","objectID":"/slim_docker_images/","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Docker"],"content":"Docker images are supposed to be as small as possible, containing only what is absolutely required for the application inside them to run. In this post, I’ll go over build-step containers and how to use them with Docker. For that matter let us consider an example Go application, nothing fancy, like the one given by the code snippet below: package main import ( \"fmt\" \"net/http\" ) func rootPathHandler(w http.ResponseWriter, req *http.Request) { fmt.Fprintf(w, \"Strange women lying in ponds distributing swords is no basis for a system of government.\\n\") } func main() { http.HandleFunc(\"/\", rootPathHandler) err := http.ListenAndServe(\":8090\", nil) if err != nil { panic(err) } } This application simply starts a web server on port 8090 that prints a simple Monty Python quote. ","date":"2023-07-15","objectID":"/slim_docker_images/:0:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Docker"],"content":"Containerizing the application The first step is to write a Dockerfile in our root directory. I am going to be using the image for Golang 1.19, to match the version I am running locally, and have the image simply build and run the application. FROM golang:1.19 ADD . /askeladden WORKDIR /askeladden RUN go build ENTRYPOINT ./askeladden Firstly, we have to build our image, which is as simple as being in the same directory of the Dockerfile and running docker build . -f Dockerfile -t 'not-so-slim-shady' docker build . -f Dockerfile -t 'not-so-slim-shady' When we give the Docker CLI the build command, we are telling it to build an image. Images are later used to create containers, which are running instances of a given image. The . refers to the path of the context, which, in the context (ha-ha, funny guy) of Docker, refers to the set of files that can be used by the build process. The -f is shorthand notation for --file and is the path to the Dockerfile, while -t is short for --tag and allows us to name/tag our image. In this case, I only named it. We can check our image by listing all images built by Docker using docker images: REPOSITORY TAG IMAGE ID CREATED SIZE not-so-slim-shady latest 3673b4f2f4c8 14 seconds ago 1.07GB Great, the image is there and taking up 1.07GB! So now we have to create and run a container based on this image to check if our web server is working. This can be achieved by running: docker run -d -p 88:8090 not-so-slim-shady docker run -d -p 88:8090 not-so-slim-shady The run command creates and runs a container. The -d flag, which stands for --detach, will run the container in the background of the terminal, meaning it will not block the terminal window. Since the container has its own network, we have to expose the port 8090 where our web server can be accessed inside the container to outside the container. This is achieved by publishing the port and mapping it into a port on the host machine, i.e., my computer, with the -p flag followed by the specification of the host port and container port in the \u003chost_port\u003e:\u003ccontainer_port\u003e format. Finally, we specify the name of the image we want to build our container from which, in this case, is just not-so-slim-shady. Docker will first look for this image locally and, if it doesn’t find it, it will attempt to retrieve it from a public image repository. We can see the list of running containers by typing docker ps, and we can check that it is properly running by navigating to http://localhost:88/. So, now we’re fairly certain that our web server is containerized as desired, so we can stop the container using the docker stop \u003ccontainer_ID\u003e command (you can retrieve the container ID from the output of docker ps). Now the output of docker ps doesn’t show anything but the container wasn’t actually deleted, it was merely stopped. If we type docker ps -a, where -a stands for --all, we can see that our container is in an Exited status. To avoid taking up resources, let’s just delete it with docker rm \u003ccontainer_ID\u003e. ","date":"2023-07-15","objectID":"/slim_docker_images/:1:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Docker"],"content":"Using a build-step container The procedure above resulted in a Docker image based on the official Golang 1.19 image with our application shipped alongside it and, as we’ve seen above, our Docker image is about 1Gb in size, which is a tad bit too much. The reason the image is so large is because it has Golang installed, which we don’t actually require for any purpose after the web server is built. The idea is simple: we create a container with Golang 1.19 and build our binary, just as before. But after building it, we create another container, based on a Debian image, and simply inject our binary into it. We can then discard the container we used for building the web server and we are left with a slimmer Docker image. So the first step is to pick the Debian image that we’ll use. We have no special preference for the Debian version, so we can use the latest version, which is called bookworm. However, we will pick the bookworm-slim variant, which is a Debian bookworm version, but without extra files that aren’t usually required by containers, such as manual pages and documentation. FROM golang:1.19 AS builder ADD . /repo WORKDIR /repo RUN go build -o bin/example FROM debian:bookworm-slim COPY --from=builder /repo/bin/example /usr/bin/example ENTRYPOINT ./usr/bin/example Note that we gave the alias builder to our first container by specifying AS builder and then injected the binary built inside it into the slimmed docker image with COPY --from=builder. As before, we can build the image with: docker build . -f Dockerfile.buildstep -t 'slim-shady' Running docker images now gives us: REPOSITORY TAG IMAGE ID CREATED SIZE slim-shady latest 7c20d0afa6c3 4 seconds ago 81.3MB not-so-slim-shady latest 74f9c0e9c60d 15 minutes ago 1.07GB Which means that our new image is only 81.3Mb in size, 10 times less than what the previous image was. To check if this image is working as excepted, let us create a container based on it and publish it’s port so we can see our web server working by running: docker run -d -p 88:8090 slim-shady Navigating to http://localhost:8080/ in a browser shows that this is working as expected. Link to the code https://github.com/ornlu-is/slim-docker-image-example ","date":"2023-07-15","objectID":"/slim_docker_images/:2:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":null,"content":"Hi, I’m Luís! This is where I write about projects I’ve been working on, and also where I post some of my study notes, to keep everything organized and easily shareable. ","date":"2023-02-16","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work I currently work as a Systems Engineer for Cloudflare and previously I worked for freiheit.com technologies as a Software Engineer. ","date":"2023-02-16","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Academic studies I have a BSc in Engineering Physics and a MSc in Applied Mathematics, both granted by University of Lisbon - Instituto Superior Técnico. ","date":"2023-02-16","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work-related interests My main interests are backend engineering, infrastructure, and statistical algorithms for problems at scale. ","date":"2023-02-16","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Socials GitHub LinkedIn ","date":"2023-02-16","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"}]