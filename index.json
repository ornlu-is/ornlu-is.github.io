[{"categories":["Study Notes"],"content":"How to get slim Docker images using build-step containers.","date":"2023-07-14","objectID":"/slim_docker_images/","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Study Notes"],"content":"Docker images are supposed to be as small as possible, containing only what is absolutely required for the application inside them to run. In this post, I’ll go over build-step containers and how to use them with Docker. For that matter let us consider an example Go application, nothing fancy, like the one given by the code snippet below: package main import ( \"fmt\" \"net/http\" ) func rootPathHandler(w http.ResponseWriter, req *http.Request) { fmt.Fprintf(w, \"Strange women lying in ponds distributing swords is no basis for a system of government.\\n\") } func main() { http.HandleFunc(\"/\", rootPathHandler) err := http.ListenAndServe(\":8090\", nil) if err != nil { panic(err) } } This application simply starts a web server on port 8090 that prints a simple Monty Python quote. ","date":"2023-07-14","objectID":"/slim_docker_images/:0:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Study Notes"],"content":"Containerizing the application The first step is to write a Dockerfile in our root directory. I am going to be using the image for Golang 1.19, to match the version I am running locally, and have the image simply build and run the application. FROM golang:1.19 ADD . /askeladden WORKDIR /askeladden RUN go build ENTRYPOINT ./askeladden Firstly, we have to build our image, which is as simple as being in the same directory of the Dockerfile and running docker build . -f Dockerfile -t 'not-so-slim-shady' docker build . -f Dockerfile -t 'not-so-slim-shady' When we give the Docker CLI the build command, we are telling it to build an image. Images are later used to create containers, which are running instances of a given image. The . refers to the path of the context, which, in the context (ha-ha, funny guy) of Docker, refers to the set of files that can be used by the build process. The -f is shorthand notation for --file and is the path to the Dockerfile, while -t is short for --tag and allows us to name/tag our image. In this case, I only named it. We can check our image by listing all images built by Docker using docker images: REPOSITORY TAG IMAGE ID CREATED SIZE not-so-slim-shady latest 3673b4f2f4c8 14 seconds ago 1.07GB Great, the image is there and taking up 1.07GB! So now we have to create and run a container based on this image to check if our web server is working. This can be achieved by running: docker run -d -p 88:8090 not-so-slim-shady docker run -d -p 88:8090 not-so-slim-shady The run command creates and runs a container. The -d flag, which stands for --detach, will run the container in the background of the terminal, meaning it will not block the terminal window. Since the container has its own network, we have to expose the port 8090 where our web server can be accessed inside the container to outside the container. This is achieved by publishing the port and mapping it into a port on the host machine, i.e., my computer, with the -p flag followed by the specification of the host port and container port in the \u003chost_port\u003e:\u003ccontainer_port\u003e format. Finally, we specify the name of the image we want to build our container from which, in this case, is just not-so-slim-shady. Docker will first look for this image locally and, if it doesn’t find it, it will attempt to retrieve it from a public image repository. We can see the list of running containers by typing docker ps, and we can check that it is properly running by navigating to http://localhost:88/. So, now we’re fairly certain that our web server is containerized as desired, so we can stop the container using the docker stop \u003ccontainer_ID\u003e command (you can retrieve the container ID from the output of docker ps). Now the output of docker ps doesn’t show anything but the container wasn’t actually deleted, it was merely stopped. If we type docker ps -a, where -a stands for --all, we can see that our container is in an Exited status. To avoid taking up resources, let’s just delete it with docker rm \u003ccontainer_ID\u003e. ","date":"2023-07-14","objectID":"/slim_docker_images/:1:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Study Notes"],"content":"Using a build-step container The procedure above resulted in a Docker image based on the official Golang 1.19 image with our application shipped alongside it and, as we’ve seen above, our Docker image is about 1Gb in size, which is a tad bit too much. The reason the image is so large is because it has Golang installed, which we don’t actually require for any purpose after the web server is built. The idea is simple: we create a container with Golang 1.19 and build our binary, just as before. But after building it, we create another container, based on a Debian image, and simply inject our binary into it. We can then discard the container we used for building the web server and we are left with a slimmer Docker image. So the first step is to pick the Debian image that we’ll use. We have no special preference for the Debian version, so we can use the latest version, which is called bookworm. However, we will pick the bookworm-slim variant, which is a Debian bookworm version, but without extra files that aren’t usually required by containers, such as manual pages and documentation. FROM golang:1.19 AS builder ADD . /repo WORKDIR /repo RUN go build -o bin/example FROM debian:bookworm-slim COPY --from=builder /repo/bin/example /usr/bin/example ENTRYPOINT ./usr/bin/example Note that we gave the alias builder to our first container by specifying AS builder and then injected the binary built inside it into the slimmed docker image with COPY --from=builder. Running docker images now gives us: REPOSITORY TAG IMAGE ID CREATED SIZE slim-shady latest 7c20d0afa6c3 4 seconds ago 81.3MB not-so-slim-shady latest 74f9c0e9c60d 15 minutes ago 1.07GB Which means that our new image is only 81.3Mb in size, 10 times less than what the previous image was. To check if this image is working as excepted, let us create a container based on it and publish it’s port so we can see our web server working by running: docker run -d -p 88:8090 slim-shady Navigating to http://localhost:8080/ in a browser shows that this is working as expected. Link to the code https://github.com/ornlu-is/slim-docker-image-example ","date":"2023-07-14","objectID":"/slim_docker_images/:2:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Web scraping movie and actor information to build a graph of all actors, connecting them via movies they worked together on.","date":"2023-07-14","objectID":"/six_degrees_of_kevin_bacon_2/","tags":null,"title":"Six Degrees of Kevin Bacon 2: Web Scraping Movie Data","uri":"/six_degrees_of_kevin_bacon_2/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"To achieve the goal of building a social graph for movie actors, the first step is to gather data. For that matter, I will be scraping data from the Open Media Database (OMDB) , which is a free non-commercial database for film media. Other posts in this series Six Degrees of Kevin Bacon 1: Introduction Six Degrees of Kevin Bacon 3: Collecting the Data ","date":"2023-07-14","objectID":"/six_degrees_of_kevin_bacon_2/:0:0","tags":null,"title":"Six Degrees of Kevin Bacon 2: Web Scraping Movie Data","uri":"/six_degrees_of_kevin_bacon_2/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"A plan for crawling OMDB The OMDB is organized very conveniently for my purposes. The website has an encyclopedia section where we can pick any given year and it will list all movies in the chosen year,e.g., the page for the year 2023: https://www.omdb.org/en/de/encyclopedia/year/2023/index/1 The link above takes us to the first page containing media from 2023. This link also gives us a hint on how we might crawl the website. More specifically, the /year/2023/ part of the link looks very convenient. If we change this to, say /year/2022/, we get the first page of movies for 2022! In short, we have a very easy of navigating years, we can just manually pick them (and we should be safe because it takes a whole year for a year to elapse). Additionally, the link terminates with /index/1. At the bottom of this page, we have the option of navigating to the next set of movies that came out in 2023: Pagination in OMDB Clicking the box for the second page, takes us to a new webpage with the link: https://www.omdb.org/en/de/encyclopedia/year/2023/index/2 Convenience galore, all that changed in the link was just the last number! Naively, we might think that a good plan for iterating over the pages for a given year would be to start at the first page, collect the information required, find the link for the next page, navigate to it and then repeat the process until we run out of pages. To understand why this probably isn’t the most viable option, let us inspect the pagination element: \u003cdiv id=\"pagination\" class=\"pagination\"\u003e \u003cul\u003e \u003cli class=\"disablepage\"\u003e \u003cspan\u003e «\u0026nbsp;previous \u003c/span\u003e \u003c/li\u003e \u003cli class=\"currentpage\"\u003e \u003cspan\u003e 1 \u003c/span\u003e \u003c/li\u003e \u003cli\u003e \u003ca href=\"/en/de/encyclopedia/year/2023/index/2\"\u003e 2 \u003c/a\u003e \u003c/li\u003e \u003cli\u003e \u003ca href=\"/en/de/encyclopedia/year/2023/index/3\"\u003e 3 \u003c/a\u003e \u003c/li\u003e \u003cli\u003e ...\u0026nbsp; \u003ca href=\"/en/de/encyclopedia/year/2023/index/18\"\u003e 18 \u003c/a\u003e \u003c/li\u003e \u003cli class=\"nextpage\"\u003e \u003ca href=\"javascript:void(0);\" id=\"nextpage\"\u003e Next\u0026nbsp;» \u003c/a\u003e \u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e As we can see, the \u003cli class=\"nextpage\"\u003e contains an anchor element but this element’s href attribute does not contain a link. But convenience is on our side again because the pagination in OMDB always shows the last possible page we can navigate to! This means that, to get all pages for a given year, all we have to do is scrape the first one and extract the number of the last page. Then, the links all pages in between the first and last page can be obtained by simply incrementing /index/1 until we reach /index/\u003clastpage\u003e. Note that the list item for the last page does not contain any attribute that allows us to distinguish it from all others, but there is one list item that does, and that is the one corresponding to the next page, which always shows up after the link for the last page and has the attribute class with the value nextpage. So, in essence, we just need to locate the div element that has an id attribute with value pagination and, in its child elements, locate the li element with the attribute class and value nextpage and then get its previous sibling, that holds the information on the last page. ","date":"2023-07-14","objectID":"/six_degrees_of_kevin_bacon_2/:1:0","tags":null,"title":"Six Degrees of Kevin Bacon 2: Web Scraping Movie Data","uri":"/six_degrees_of_kevin_bacon_2/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Getting the movie links We have a plan to get all the pages for all the years, but we have yet to obtain the actual movie links. Fortunately, in each page, there is a div element with the attribute id and value results which holds, you guessed it, the movies in the page! For the last page of 2019, that HTML element looks like this: \u003cdiv id=\"results\"\u003e \u003cdiv id=\"search-listing-70\" style=\"float: left; overflow:hidden;\"\u003e \u003cdiv id=\"items-for-page-70\"\u003e \u003cdiv id=\"movie_search_result_172562\" class=\"result-box\"\u003e \u003cdiv class=\"image\"\u003e \u003ca href=\"/en/de/movie/172562-kase-und-blei\"\u003e \u003cimg alt=\"\" src=\"https://www.omdb.org/images/misc/no-thumbnail.png?1319773240\" width=\"92\"\u003e \u003c/a\u003e \u003cbr\u003e \u003cdiv class=\"votebar\"\u003e \u003cdiv class=\"mediumvote\" title=\"0.00\"\u003e \u003cdiv style=\"width: 0.00%\"\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003cdiv class=\"link\"\u003e \u003ca href=\"/en/de/movie/172562-kase-und-blei\" title=\"Movie: Käse und Blei\"\u003e Käse und Blei \u003c/a\u003e \u003cdiv class=\"small\"\u003e 2019 \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e The HTML element we can easily scope out is \u003cdiv class=\"link\"\u003e \u003ca href=\"/en/de/movie/172562-kase-und-blei\" title=\"Movie: Käse und Blei\"\u003e Käse und Blei \u003c/a\u003e \u003cdiv class=\"small\"\u003e 2019 \u003c/div\u003e \u003c/div\u003e because we can identify it via the class attribute that has the value link. Then, all we need to do is fetch the value of the href attribute of the anchor element to get the portion of the link that we have to append to https://www.omdb.org to get a movie link and, additionally, we can also extract this element’s data to get the movie’s name. There is one last detail about extracting a movie link: sometimes, OMDB another hyperlink associated with each movie. If you look closely at the screenshot above, you can see that some movies’ name is prefixed by a small arrow. Unfortunately, this small arrow has the exact same hyperref value as the link we are interested, meaning that we get duplicated links. While we could solve this with a slightly more complicated scraping strategy, we can also tackle this by implementing deduplication which will be sufficiently efficient since the number of movies per page is small (max. 24). ","date":"2023-07-14","objectID":"/six_degrees_of_kevin_bacon_2/:2:0","tags":null,"title":"Six Degrees of Kevin Bacon 2: Web Scraping Movie Data","uri":"/six_degrees_of_kevin_bacon_2/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Extracting cast data Now that we have a way to get all the links for all movies, we have to think about how we are extracting the cast. When we navigate to a given movie’s link, we see that there is a clickable tab named “Crew/Cast”. Clicking on it takes us to a page whose link is the same as the movie’s link but affixed with /cast, e.g. https://www.omdb.org/en/de/movie/67669-how-to-train-your-dragon-the-hidden-world/cast and is filled with information on who worked on the movie. Particularly, there is a section named “Actors” where the data we are looking for is. Inspecting this element we get: \u003cdiv id=\"movie-list-4\" class=\"list\" style=\"margin:0px; clear:left;\"\u003e \u003cdiv class=\"list-view-headline\"\u003e \u003ca class=\"edit-button\" href=\"#\" id=\"link-sort-movie-4\" onclick=\"new Ajax.Request('/en/de/movie/67669/activate_cast_sorting?type=movie-4', {asynchronous:true, evalScripts:true}); return false;\" title=\"Sort this list\"\u003esort\u003c/a\u003e \u003ca class=\"edit-button\" href=\"#\" onclick=\"try { box = new lightbox('/en/de/cast/edit?department=4\u0026amp;movie=67669-how-to-train-your-dragon-the-hidden-world', false, true) } catch (e) {}; return false;\" title=\"edit\"\u003eedit\u003c/a\u003e \u003ca name=\"department_4\"\u003e\u003c/a\u003e \u003ch3\u003eActors\u003c/h3\u003e \u003c/div\u003e \u003cul id=\"movie-4\" class=\"sortable-list\"\u003e \u003cli id=\"actor_885986\"\u003e \u003cdiv class=\"freeze handle\"\u003e\u003cimg alt=\"Sort\" class=\"sort\" src=\"https://www.omdb.org/images/icons/sort.png?1319773240\" width=\"25\" height=\"11\"\u003e\u003c/div\u003e \u003cdiv class=\"person\"\u003e \u003cstrong\u003e\u003ca href=\"/en/de/person/449-jay-baruchel\" title=\"Person: Jay Baruchel\"\u003eJay Baruchel\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e as \u003cspan class=\"bold\"\u003e \u003ca class=\"edit\" href=\"#\" onclick=\"try { box = new lightbox('/en/de/cast/885986/edit_character', false, true) } catch (e) {}; return false;\"\u003eHiccup\u003c/a\u003e (Voice) \u003c/span\u003e \u003c/li\u003e \u003cli id=\"actor_885987\"\u003e \u003cdiv class=\"freeze handle\"\u003e\u003cimg alt=\"Sort\" class=\"sort\" src=\"https://www.omdb.org/images/icons/sort.png?1319773240\" width=\"25\" height=\"11\"\u003e\u003c/div\u003e \u003cdiv class=\"person\"\u003e \u003cstrong\u003e\u003ca href=\"/en/de/person/59174-america-ferrera\" title=\"Person: America Ferrera\"\u003eAmerica Ferrera\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e as \u003cspan class=\"bold\"\u003e \u003ca class=\"edit\" href=\"#\" onclick=\"try { box = new lightbox('/en/de/cast/885987/edit_character', false, true) } catch (e) {}; return false;\"\u003eAstrid\u003c/a\u003e (Voice) \u003c/span\u003e \u003c/li\u003e \u003cli id=\"actor_967703\"\u003e \u003cdiv class=\"freeze handle\"\u003e\u003cimg alt=\"Sort\" class=\"sort\" src=\"https://www.omdb.org/images/icons/sort.png?1319773240\" width=\"25\" height=\"11\"\u003e\u003c/div\u003e \u003cdiv class=\"person\"\u003e \u003cstrong\u003e\u003ca href=\"/en/de/person/1164-f-murray-abraham\" title=\"Person: F. Murray Abraham\"\u003eF. Murray Abraham\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e as \u003cspan class=\"bold\"\u003e \u003ca class=\"edit\" href=\"#\" onclick=\"try { box = new lightbox('/en/de/cast/967703/edit_character', false, true) } catch (e) {}; return false;\"\u003eGrimmel\u003c/a\u003e (Voice) \u003c/span\u003e \u003c/li\u003e \u003cli id=\"actor_885988\"\u003e \u003cdiv class=\"freeze handle\"\u003e\u003cimg alt=\"Sort\" class=\"sort\" src=\"https://www.omdb.org/images/icons/sort.png?1319773240\" width=\"25\" height=\"11\"\u003e\u003c/div\u003e \u003cdiv class=\"person\"\u003e \u003cstrong\u003e\u003ca href=\"/en/de/person/112-cate-blanchett\" title=\"Person: Cate Blanchett\"\u003eCate Blanchett\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e as \u003ca class=\"edit\" href=\"#\" onclick=\"try { box = new lightbox('/en/de/cast/885988/edit_character', false, true) } catch (e) {}; return false;\"\u003eValka\u003c/a\u003e (Voice) \u003c/li\u003e \u003cli id=\"actor_885992\"\u003e \u003cdiv class=\"freeze handle\"\u003e\u003cimg alt=\"Sort\" class=\"sort\" src=\"https://www.omdb.org/images/icons/sort.png?1319773240\" width=\"25\" height=\"11\"\u003e\u003c/div\u003e \u003cdiv class=\"person\"\u003e \u003cstrong\u003e\u003ca href=\"/en/de/person/17276-gerard-butler\" title=\"Person: Gerard Butler\"\u003eGerard Butler\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e as \u003ca class=\"edit\" href=\"#\" onclick=\"try { box = new lightbox('/en/de/cast/885992/edit_character', false, true) } catch (e) {}; return false;\"\u003eStoick\u003c/a\u003e (Voice) \u003c/li\u003e \u003cli id=\"actor_885989\"\u003e \u003cdiv class=\"freeze handle\"\u003e\u003cimg alt=\"Sort\" class=\"sort\" src=\"https://www.omdb.org/images/icons/sort.png?1319773240\" width=\"25\" height=\"11\"\u003e\u003c/div\u003e \u003cdiv class=\"person\"\u003e \u003cstrong\u003e\u003ca href=\"/en/de/person/24264-craig-ferguson\" title=\"Person: Craig Ferguson\"\u003eCraig Ferguson\u003c/","date":"2023-07-14","objectID":"/six_degrees_of_kevin_bacon_2/:3:0","tags":null,"title":"Six Degrees of Kevin Bacon 2: Web Scraping Movie Data","uri":"/six_degrees_of_kevin_bacon_2/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Next step Now that we have a functional description on how to scrape the data we want, we need to come with an implementation plan for how we are going to pratically achieve this goal. Check out my plan for it in the next post of this series: Six Degrees of Kevin Bacon 3: Collecting the Data ","date":"2023-07-14","objectID":"/six_degrees_of_kevin_bacon_2/:4:0","tags":null,"title":"Six Degrees of Kevin Bacon 2: Web Scraping Movie Data","uri":"/six_degrees_of_kevin_bacon_2/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"From Stanley Milgram's small world experiment to six degrees of Kevin Bacon.","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Today I found out that there is parlor game named “Six Degrees of Kevin Bacon” and its rules are very simple: a player picks an actor and other players have to connect that actor to another actor via a film both starred in. This process is then repeated until Kevin Bacon is reached. The name of this game is a reference to the concept of “six degrees of separation” where, in the world’s social network, every person is, at most, six acquaintance links apart from any other person in the world. Other posts in this series Six Degrees of Kevin Bacon 2: Web Scraping Movie Data Six Degrees of Kevin Bacon 3: Collecting the Data ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:0:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Networks So what is a network and how can we visualise it? In this context, a network is a set of actors, people, or things that are connected to each other via a relation. For example, in a social network, you are connected to your parents via a parental relation and connected to your friends via a friendship relation. Similarly, two computers can also form a network where they are connected via the Internet. Networks are a wonderful concept and they are absolutely everywhere. Networks are represented as graphs, which are a set of nodes and edges. We can visualise these in the following way: Kevin Bacon and his parents as a graph Hopefully, the above picture made it abundantly clear what a graph, nodes and edges are. In our graph we have three nodes, $|N|=3$, Kevin Bacon and his parents, and two edges, $|E|=2$, which connect Kevin Bacon to his parents via a parental relation. Mathematically, we characterise a graph, $G$, by its set of nodes, $N$, and edges, $E$, as follows: $$G(N, E),$$ where $$N = \\{ \\text{Edmund Bacon}, \\text{Kevin Bacon}, \\text{Ruth Hilda Bacon} \\},$$ $$E = \\{ \\{ \\text{Edmund Bacon} , \\text{Kevin Bacon} \\}, \\{ \\text{Kevin Bacon}, \\text{Ruth Hilda Bacon} \\} \\}.$$ Technically, the above graph should be a directed graph, in the sense that edges have a given direction: Edmund Bacon is Kevin Bacon’s father, but Kevin Bacon is not Edmund Bacon’s father, such shenanigans are left to time travel movies. However, for simplicity’s sake, I decided to represent the graph as an undirected graph, where edges represent bidirectional relations. Additionally, it is also an unweighted graph, meaning that all edges have the same weight, because I am assuming Kevin Bacon loves both his parents equally. ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:1:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Degree and distance in networks In a network, any node, $i$, is associated with a quantity known as the degree, $k_i$, which simply represents the number of edges a node has. If we associate Kevin Bacon with $i=1$, and is mother and father with, respectively, $i=2$ and $i=3$, we have that: $k_1 = 2$; $k_2 = 1$; $k_3 = 1$. From a network’s degrees, we can calculate its average degree, a quantity that plays a central role in network analysis. This is given by the following formula: $$ \\langle k \\rangle = \\frac{1}{|N|} \\sum_{i=1}^{|N|} k_i = \\frac{2|E|}{|N|}. $$ While the concept of a node’s degree is very intuitive, the concept of distance in a network is a tougher concept to handle, simply because it doesn’t necessarily translate nicely into reality. In networks, the distance between two nodes depends on the path that connects them. This path is simply the set of edges that we must transverse to go from one node to another, and, the number of edges transversed is the path length. Effectively, we equate the path length to the distance between two nodes. However, in most graphs, there isn’t a single path that connects two nodes together. As such, it is more natural to refer to the shortest path, i.e., the path that consists of the minimal set of edges. Derived from this, another interesting quantity in networks is the average path length, $\\langle d \\rangle$, which is simply the average of the shortest paths between all pairs of nodes. ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:2:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Small world networks In 1967, an American social psychologist named Stanley Milgram, designed an experiment to measure distances in social networks. He began by selecting a stock broker from Boston and a divinity student from Sharon (Massachussetts) as target nodes. Simultaneously, he also randomly selected residents of Wichita and Omaha, which would act as starting nodes. These residents were sent a letter by Milgram explaining the study’s objective, a photograph, the name, address and some more information about their target person. These letters also included instructions for these people to forward the letter to the person they knew and thought would most likely know the target person. While many of the letters did not make it back, those that did led allowed Milgram to calculate the median number of people required to reach the target. Surprisingly, the median number of intermediate acquaintances was 5, which is where the expression “six degrees of separation” and the concept of small world phenomenon stems from. In network terms, this just means that the average distance between two nodes in a network is short. More precisely, it is short enough that the approximation: $$\\langle d \\rangle \\approx \\frac{\\ln |N|}{ \\ln \\langle k \\rangle },$$ is reasonably close. ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:3:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"The goal of this project I have a very straightforward goal: to verify if, and if so, how many, actors are more than six degrees apart from Kevin Bacon. This project is deceptively hard due to the data required to achieve it. The first step is to scrape movie data off the Internet (without DDoS’ing anyone) and store it efficiently. Then, this data has to be cleaned to avoid having any malformed data, e.g., duplicated entries, skewing the results. Finally, the entire graph has to be computed and the distance to Kevin Bacon must be calculated for every single node. Additionally, I also want to verify if the small world property holds true for the movie actors network and, in case it does not, when, in time, does it break down. ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:4:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Next step We have a clear goal in mind, so we need to figure out where we are going to get the data from and how we are going to do it. This is covered in the next post of the series: Six Degrees of Kevin Bacon 2: Web Scraping Movie Data ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:5:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"Trivia Stanley Milgram is mostly known for authoring another very controversial experiment, which is known as the Stanley Milgram Shock Experiment. ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:6:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":["Six Degrees of Kevin Bacon"],"content":"References Barabasi’s book Albert-László Barabási, Network Science, 2018 Kevin Bacon’s Wikipedia page: https://en.wikipedia.org/wiki/Kevin_Bacon ","date":"2023-07-13","objectID":"/six_degrees_of_kevin_bacon_1/:7:0","tags":null,"title":"Six Degrees of Kevin Bacon 1: Introduction","uri":"/six_degrees_of_kevin_bacon_1/"},{"categories":null,"content":"Hi, I’m Luís! This is where I write about projects I’ve been working on, and also where I post some of my study notes, to keep everything organized and easily shareable. ","date":"2023-02-16","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work I currently work as a Systems Engineer for Cloudflare and previously I worked for freiheit.com technologies as a Software Engineer. ","date":"2023-02-16","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Academic studies I have a BSc in Engineering Physics and a MSc in Applied Mathematics, both granted by University of Lisbon - Instituto Superior Técnico. ","date":"2023-02-16","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work-related interests My main interests are backend engineering, infrastructure, and statistical algorithms for problems at scale. ","date":"2023-02-16","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Socials GitHub LinkedIn ","date":"2023-02-16","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"}]