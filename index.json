[{"categories":["Adventures in Overengineering"],"content":"Using Terraform to create a Kubernetes namespace in a local microk8s cluster and migrating a Kubernetes application to it using Helm","date":"2023-03-29","objectID":"/overengineering_adventures_4/","tags":null,"title":"Adventures in Overengineering 4: using Terraform for Kubernetes namespace creation","uri":"/overengineering_adventures_4/"},{"categories":["Adventures in Overengineering"],"content":"This post has one very simple and short goal: to use Terraform to create a Kubernetes namespace to which we will migrate the application that we’ve been working on. Now, you might ask yourself: “Is it overkill to use Terraform just to create Kubernetes namespaces?” To which the answer is: yes, yes it is. But will that stop me? Short answer “no”, long answer “no, it will not”. The name of the series is “Adventures in Overengineering” and I plan to remain faithful to that name until I run out of ideas. Link to the code https://github.com/ornlu-is/askeladden/tree/v0.4 ","date":"2023-03-29","objectID":"/overengineering_adventures_4/:0:0","tags":null,"title":"Adventures in Overengineering 4: using Terraform for Kubernetes namespace creation","uri":"/overengineering_adventures_4/"},{"categories":["Adventures in Overengineering"],"content":"What is Terraform? A concept that quickly rose to popularity in recent times is the concept of infrastructure as code, IaC, where, as the name suggests, infrastructure is managed and provisioned via code instead of manual processes. This has a number of advantages, such as being able to fully leverage a version control system to track, and possibly rollback, infrastructure. Naturally, it also makes distributing infrastructure configuration across developers and teams much easier. One popular tool for this paradigm, is Terraform, which takes a declarative approach to IaC, by allowing developers to write files which declare the currently desired state of the infrastructure. There are many other technologies that can be used for this purpose, but Terraform is the one I am used to using, so that is what I’m going to go with. ","date":"2023-03-29","objectID":"/overengineering_adventures_4/:1:0","tags":null,"title":"Adventures in Overengineering 4: using Terraform for Kubernetes namespace creation","uri":"/overengineering_adventures_4/"},{"categories":["Adventures in Overengineering"],"content":"Setting up Terraform with a local Kubernetes cluster First things first, we have to install Terraform. For that matter, we can simply follow the official documentation , which happens to be very straightforward. Now we just need one last step before we are good to go. Since I am using microk8s, its configuration isn’t saved under ~/.kube/config. We can take care of this with one simple command: microk8s config \u003e ~/.kube/config And we are done, installation-wise. That was quick. ","date":"2023-03-29","objectID":"/overengineering_adventures_4/:2:0","tags":null,"title":"Adventures in Overengineering 4: using Terraform for Kubernetes namespace creation","uri":"/overengineering_adventures_4/"},{"categories":["Adventures in Overengineering"],"content":"Creating the necessary files to create a Kubernetes namespace with Terraform We will be using Terraform for managing infrastructure, which means that the files we create for this purpose already have one natural destination: our infrastructure/ directory. We create a new directory inside infrastructure, called terraform, and populate it with the following files: terraform/ ├── backend.tf ├── main.tf ├── provider.tf └── versions.tf Let us go over these one by one, starting with backend.tf, which, as the name suggests, specifies what backend terraform should use for remote state management and state locking. In this case, we will be using the kubernetes backend, which stores the Terraform state as a Kubernetes secret: terraform { backend \"kubernetes\" { secret_suffix = \"state\" config_path = \"~/.kube/config\" namespace = \"terraform\" } } If we look at the Terraform documentation for this backend type, we see that this backend is limited by Kubernetes maximum secret size, which is 1MB. This might be troublesome in the future but for now, our mindset is going to be “if it breaks, it breaks” and then we’ll fix it when necessary. Since we are using the Kubernetes backend and we want to use Terraform to create a Kubernetes namespace, we also need to specify a provider, in provider.tf, which, in the Terraform nomenclature, is a set of plugins to interact with resources: provider \"kubernetes\" { config_path = \"~/.kube/config\" config_context = \"microk8s\" } Providers are plugins and plugins have different versions depending on the underlying API they interact with, which means that we should specify the provider version we want in versions.tf: terraform { required_providers { kubernetes = { source = \"hashicorp/kubernetes\" } } } I didn’t really specify the version I want, so we will just end up using the latest version, if it breaks, it breaks, but I still enjoy having everything organized into different files. Finally, we will create the resources we want under main.tf which, in this case, is a Kubernetes namespace: resource \"kubernetes_namespace\" \"askeladden-staging\" { metadata { name = \"askeladden-staging\" } } Note that we are using the Kubernetes backend, which will create a Kubernetes secret storing the Terraform state files. The secret has to go somewhere and, in backend.tf, I specified that it is goes into the terraform namespace, because I do not want things going into the default namespace. The namespace needs to exist prior to applying the Terraform changes so I’m also going to add infrastructure/k8s/tf-namespace.yaml file with the following contents: apiVersion: v1 kind: Namespace metadata: name: terraform Yes, I know, this kind of defeats the purpose of having terraform generate the namespaces for me, but I do not want to use the default Kubernetes namespace and I cannot use a Kubernetes Terraform backend in a namespace that does not exist. ","date":"2023-03-29","objectID":"/overengineering_adventures_4/:3:0","tags":null,"title":"Adventures in Overengineering 4: using Terraform for Kubernetes namespace creation","uri":"/overengineering_adventures_4/"},{"categories":["Adventures in Overengineering"],"content":"Using Terraform mk If you haven’t read my previous posts, you will probably wonder what the mk command is. It is simply an alias for microk8s kubectl. Go read my previous posts, this is an adventure, you cannot start halfway. We are ready to begin! Let’s first have a look at what namespaces we have configured in our cluster using mk get namespaces: NAME STATUS AGE kube-system Active 15d kube-public Active 15d kube-node-lease Active 15d default Active 15d askeladden-dev Active 14d container-registry Active 14d The askeladden-dev namespace is where our web server is deployed. Unfortunately for this namespace, it will not live for much longer. First, let’s create the new terraform namespace, which will host the Terraform state files, using: mk apply -f infrastructure/k8s/tf-namespace.yaml Running mk get namespaces once again now shows our newly born namespace: NAME STATUS AGE kube-system Active 15d kube-public Active 15d kube-node-lease Active 15d default Active 15d askeladden-dev Active 14d container-registry Active 14d terraform Active 9s With the terraform namespace created, we can now navigate to /infrastructure/terraform/ running: terraform init terraform init terraform init initializes a working directory containing Terraform configuration files and should always be ran prior to writing a new Terraform configuration. This will create a .terraform/ directory, and a .terraform.lock.hcl file, along with .tfstate files in case the specified backend is local (in our case, we are using the Kubernetes backend, so the state files are kept in the Kubernetes cluster). The .terraform.lock.hcl file should be included in the version control repository, to be used as a discussion medium abour potential changes to external dependencies via code review. On the other hand, the .terraform/ directory is basically a local cache, and should not be commited. This will output the following: Initializing the backend... Successfully configured the backend \"kubernetes\"! Terraform will automatically use this backend unless the backend configuration changes. Initializing provider plugins... - Finding latest version of hashicorp/kubernetes... - Installing hashicorp/kubernetes v2.19.0... - Installed hashicorp/kubernetes v2.19.0 (signed by HashiCorp) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. The aforementioned command also created a directory .terraform and a .terraform.lock.hcl file. To avoid forgetting about the existence of the .terraform and accidentally commiting this to my repository, I’ll create a .gitignore file with the contents presented below: **/.terraform/ .gitignore init If you have any intentionally untracked files and you want git to ignore them, you can create a .gitignore file at the root of the project’s repository. You can then specify patterns inside this file, one patter per line, which git will use to match files the in project and ignore these matched files. This effectively means that these files will not exist in the remote repository. If we want to see what resources Terraform will create/modify/destroy, we can use the terraform plan, which provides a pretty output of stating what will happen if the Terraform changes are applied: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create T","date":"2023-03-29","objectID":"/overengineering_adventures_4/:4:0","tags":null,"title":"Adventures in Overengineering 4: using Terraform for Kubernetes namespace creation","uri":"/overengineering_adventures_4/"},{"categories":["Adventures in Overengineering"],"content":"Re-deploying the web server in the newly created namespace Note that I’ve created a namespace called askeladden-staging and we have a namespace called askeladden-dev. For reasons that will become clear in future posts, I am now deprecating the askeladden-dev namespace, meaning that I have to deploy the web server into askeladden-staging, check that it is working, and then delete the askeladden-dev namespace along with the Kubernetes resources in it. For consistency, we need to change two things in our Helm chart. The first is the deploymentLabels, in services/askeladden/chart/values.yaml. We currently have them set as: deploymentLabels: env: dev And we want to change them to: deploymentLabels: env: staging So that it matches the suffix of our namespace. Secondly, we still have our app deployed and exposed on port 30008, which means that, if we attempt to deploy our new app, we will get an error message stating that this port is already in use. As such, we have to change the nodePort field, under service, to be something else. In this case, we will change it to 30009. After these changes, we can deploy our app to the askeladden-staging namespace using: microk8s helm upgrade askeladden services/askeladden/chart/askeladden --namespace=\"askeladden-staging\" It’s impressive how much easier this is to perform with Helm than with plain Kubernetes. Now, if we navigate to http://localhost:30009, we should the see the Monty Python quote we’ve been using: Strange women lying in ponds distributing swords is no basis for a system of government. Indeed it is, Monty Python, indeed it is. We can list our Helm releases for the askeladden-staging namespace using: microk8s helm list --namespace=\"askeladden-staging\" NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION askeladden askeladden-staging 2 2023-03-29 23:17:48.168348961 +0100 WEST deployed askeladden-0.0.1 0.0.3 Which just confirms what we already knew: our web server has been deployed successfully. However, if we run the above command for the askeladden-dev namespace: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION askeladden askeladden-dev 1 2023-03-27 11:03:41.930152552 +0100 WEST deployed askeladden-0.0.1 0.0.3 We see that our web server still exists in the other namespace. Helm is also useful in case we want to remove delete old deployments, we simply have to run: microk8s helm uninstall askeladden --namespace=\"askeladden-dev\" Which outputs the message: release \"askeladden\" uninstalled. We can verify this operation with the mk get all command, which tells us exactly what we wanted to hear: No resources found in askeladden-dev namespace. I almost forgot, we have our context pointing to the askeladden-dev namespace. Let us change that before we delete the namespace so that it points to askeladden-staging. First, we create a new context with mk config set-context askeladden-staging --namespace=askeladden-staging --cluster=microk8s-cluster --user=admin Just like we have done in the first post of these adventures. We have created the new context, so now we have to use it, which is easily achieved with: mk config use-context askeladden-staging If we now type mk config current-context, we get askeladden-staging as our output. We are going to delete the askeladden-dev so there is no point in keeping the askeladden-dev context around. We delete it through: mk config delete-context askeladden-dev Finally, we will delete our namespace using: mk delete namespace askeladden-dev We can delete the infrastructure/k8s/namespace.yaml file that we had since it is no longer required. And we are done! We have successfully used Terraform to create a new Kubernetes namespace in our microk8s cluster and migrated our web server to it using Helm! ","date":"2023-03-29","objectID":"/overengineering_adventures_4/:5:0","tags":null,"title":"Adventures in Overengineering 4: using Terraform for Kubernetes namespace creation","uri":"/overengineering_adventures_4/"},{"categories":["Adventures in Overengineering"],"content":"Using helm to package kubernetes applications and deploying them on a local kubernetes microk8s cluster.","date":"2023-03-26","objectID":"/overengineering_adventures_3/","tags":null,"title":"Adventures in Overengineering 3: manning the Helm","uri":"/overengineering_adventures_3/"},{"categories":["Adventures in Overengineering"],"content":"Ahoy! It’s time to include Helm in our project! Why? Three reasons: making our life easier when it comes to managing Kubernetes resources, staying true to the overengineering game by introducing yet another tool, and will also allow me to write more pirate related puns than what should be legally allowed. I apologize in advance for my terrible sense of humour. It is recommended to read this post while listening to “Under Jolly Roger” by Running Wild or any other pirate song. Link to the code https://github.com/ornlu-is/askeladden/tree/v0.3 ","date":"2023-03-26","objectID":"/overengineering_adventures_3/:0:0","tags":null,"title":"Adventures in Overengineering 3: manning the Helm","uri":"/overengineering_adventures_3/"},{"categories":["Adventures in Overengineering"],"content":"What is this “Helm”? Do I look like a sailor to you? First, a fun fact. The name Kubernetes stands for “helmsman” or “pilot” in Greek, and its logo is actually a helm (I cannot believe I missed this before), which justifies the naming choice for the tool we are about to introduce into the project. Helm is basically a package manager for Kubernetes, and works through specification of charts (this is not a pun, this their the actual name). In this context, a chart is simply a collection of files that describe a related set of Kubernetes resources. Using this tool yields the following advantages: Reduces the complexity of deployments since Kubernetes requires resources to be created in a given order and Helm takes care of that for us; Reproducible deployments, since a Helm guarantees that what is in the chart matches the configuration in the Kubernetes cluster; Easier to rollback to previous deployments since Helm maintains a comprehensive revision history; Allows us to sail under the Jolly Roger. With all this in mind, let’s begin adapting our project to support Helm! ","date":"2023-03-26","objectID":"/overengineering_adventures_3/:1:0","tags":null,"title":"Adventures in Overengineering 3: manning the Helm","uri":"/overengineering_adventures_3/"},{"categories":["Adventures in Overengineering"],"content":"Setting sails with Helm First, let us look at our current directory tree. We can do this by running the tree command in our root directory, which outputs the following: . ├── Dockerfile ├── go.mod ├── k8s │ ├── deployment.yaml │ ├── namespace.yaml │ └── service.yaml ├── main.go └── README.md tree You have to install tree since it is not shipped with most operating systems. If, like me, you are running Ubuntu, you can install it using sudo apt install tree. I want to use Helm uniquely to control the deployment of my web server (in case you haven’t read my previous posts, this is an incredible web server that simply prints a Monty Python quote). The first step is to create our Helm chart. For that matter, we have to create the following directories and files in our project’s root directory: chart └── askeladden ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ └── service.yaml └── values.yaml helm create chart Alternatively, we could’ve used helm create chart which would create the Helm chart for Nginx, but then we’d have to delete several of the files and most of their content, so it is easier to create the files manually, since this implies less keystrokes and, in turn, a longer finger joint lifetime. These are almost as many files as what we already had in our project, so it is worth going over their purpose one by one: Chart.yaml - metadata on the chart itself; values.yaml - contains the default values for our chart that will be injected by Helm into the templates; templates/_helpers.tpl - contains utilities to be used across templates; templates/deployment.yaml - this will be the template for our Kubernetes deployment; templates/service.yaml - and this will be the template for our Kubernetes service. We can already fill out the contents of Chart.yaml with the following: apiVersion: v2 name: askeladden description: A web server that barely does anything version: 0.0.1 appVersion: \"0.0.3\" Where each field has the following function: apiVersion specifies the Helm version we are using; name is the name of the chart. Here I named it the same as our application because naming things is hard; description is self-explanatory; version is the chart version; appVersion is the application version, which is “0.0.3” since this is the third post in the series. However, for the remaining files, it is required that we use yo-ho-Golang’s template syntax , since is that is what confers Helm its flexibility. We have a bunch of repeated fields in our Kubernetes files such as the labels used for the matchSelector or the name, and we have some values that can be seen as overall configuration values and, as such, should be in the values.yaml file to be injected into the Kubernetes definition files by Helm. The objective is to have helm output a Kubernetes manifest that matches exactly what we previously had. Let us begin by writing the following into our values.yaml file: replicaCount: 1 image: localhost:32000/askeladden:slim labels: app: askeladden selectorLabels: app: askeladden deploymentLabels: env: dev service: type: NodePort port: 80 targetPort: 8090 nodePort: 30008 Then, let us define our helpers. These are simple reusable Go template shortcodes that can be used to improve readability of the templates. We have defined three different label dictionaries in our values.yaml, so let us create helpers that will fill the templates with the values of these dictionaries. Thus, our _helper.tpl has the following content: {{/* Get the selector labels dictionary */}} {{- define \"chart.selectorLabels\" }} {{- range $key,$value := .Values.selectorLabels }} {{- $key }}: {{ $value }} {{- end}} {{- end }} {{/* Get the labels dictionary */}} {{- define \"chart.labels\" }} {{- range $key,$value := .Values.labels }} {{- $key }}: {{ $value }} {{- end}} {{- end }} {{/* Get the deployment labels dictionary */}} {{- define \"chart.deploymentLabels\" }} {{- range $key,$value := .Values.labels }} {{- $key }}: {{ $value }} {{- end}} {{- end }} Now, i","date":"2023-03-26","objectID":"/overengineering_adventures_3/:2:0","tags":null,"title":"Adventures in Overengineering 3: manning the Helm","uri":"/overengineering_adventures_3/"},{"categories":["Adventures in Overengineering"],"content":"Swab the project’s structure Notice that we had three Kubernetes resource definition files but we left one of them out of our Helm chart. This is because the namespace is not a part of our web server, but a part of our cluster, and, as such, we want to keep it out of our chart. For that matter, we will create an infrastructure directory which will in turn have a k8s directory and we will place our namespace.yaml file there. Then, we create a services/askeladden directory and place the remaining contents there, except the go.mod file. I am a big fan of documenting stuff properly (as this blog highlights), so I also created a README.md for the askeladden service. Our directory tree ends up as follows: askeladden/ ├── go.mod ├── infrastructure │ └── k8s │ └── namespace.yaml ├── README.md └── services └── askeladden ├── chart │ └── askeladden │ ├── Chart.yaml │ ├── templates │ │ ├── deployment.yaml │ │ ├── _helpers.tpl │ │ └── service.yaml │ └── values.yaml ├── Dockerfile ├── main.go └── README.md ","date":"2023-03-26","objectID":"/overengineering_adventures_3/:3:0","tags":null,"title":"Adventures in Overengineering 3: manning the Helm","uri":"/overengineering_adventures_3/"},{"categories":["Adventures in Overengineering"],"content":"Land ho! Deploy app! Now we need to use Helm to deploy our service! We can see that our service is still running if we run mk get all, we get the following output: NAME READY STATUS RESTARTS AGE pod/deployment-askeladden-dev-6b766588c5-hkn2x 1/1 Running 3 (139m ago) 5d11h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/askeladden-dev-service NodePort 10.152.183.55 \u003cnone\u003e 80:30008/TCP 11d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/deployment-askeladden-dev 1/1 1 1 11d NAME DESIRED CURRENT READY AGE replicaset.apps/deployment-askeladden-dev-6b766588c5 1 1 1 5d11h replicaset.apps/deployment-askeladden-dev-75c45dc7d9 0 0 0 11d There is something odd, we have our first ever replica set still hanging around. Kubernetes keeps old replica sets around because that is what it uses in case we need to rollback our deployment. However, we are now using Helm, so we do not need that anymore, since Helm has its own revision history. Let us delete our deployment with mk delete deployment.apps/deployment-askeladden-dev. mk delete deployment.apps/deployment-askeladden-dev We would usually have to write this command in the mk delete \u003cresource_type\u003e \u003cresource_name\u003e. However, since we are passing the delete command the name of the deployment in the \u003cresource_type\u003e/\u003cresource_name\u003e format, we only require one argument for the command. Similarly, we will delete our service with mk delete service/askeladden-dev-service. Running mk get all now shows that we have no resources in our namespace: No resources found in askeladden-dev namespace. We are good to go to use Helm now! However, just to be on the safe side of things, we want to test our chart a bit more. We already know that it is a valid YAML file, so now we want to know if it generates any possible errors in our Kubernetes cluster. For that matter, we can use the following command: microk8s helm install askeladden chart/askeladden --dry-run microk8s helm install askeladden chart/askeladden --dry-run The helm install \u003cname\u003e \u003cchart_path\u003e command install a given chart on the cluster with the given name. The trick here is the --dry-run flag, which validates the manifests by communicating with the Kubernetes cluster. Pretty neat! This command outputs the following: NAME: askeladden LAST DEPLOYED: Mon Mar 26 11:02:41 2023 NAMESPACE: askeladden-dev STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: askeladden/templates/service.yaml apiVersion: v1 kind: Service metadata: name: askeladden labels: app: askeladden spec: type: NodePort ports: - port: 80 targetPort: 8090 nodePort: 30008 protocol: TCP selector: app: askeladden --- # Source: askeladden/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: askeladden labels: app: askeladden spec: replicas: 1 selector: matchLabels: app: askeladden template: metadata: labels: app: askeladden spec: containers: - name: askeladden image: localhost:32000/askeladden:slim Everything seems in order, meaning that we can board the ship and deploy by running the same command without the --dry-run flag. NAME: askeladden LAST DEPLOYED: Mon Mar 26 11:03:41 2023 NAMESPACE: askeladden-dev STATUS: deployed REVISION: 1 TEST SUITE: None No error messages seems like a good sign. Let us check if we have any Kubernetes resources on our namespace with mk get all: NAME READY STATUS RESTARTS AGE pod/askeladden-6b766588c5-xjm7t 1/1 Running 0 6s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/askeladden NodePort 10.152.183.151 \u003cnone\u003e 80:30008/TCP 6s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/askeladden 1/1 1 1 6s NAME DESIRED CURRENT READY AGE replicaset.apps/askeladden-6b766588c5 1 1 1 6s All the necessary resources have been create, meaning that we have only one validation step left: navigating to http://localhost:30008/. Opening the browser with that link shows the following message: Strange women lying in ponds distributing swords is no basis for a system of government. We have successfully converted o","date":"2023-03-26","objectID":"/overengineering_adventures_3/:4:0","tags":null,"title":"Adventures in Overengineering 3: manning the Helm","uri":"/overengineering_adventures_3/"},{"categories":["Adventures in Overengineering"],"content":"Deploying a slim Docker image into a microk8s kubernetes local cluster.","date":"2023-03-21","objectID":"/overengineering_adventures_2/","tags":null,"title":"Adventures in Overengineering 2: a better Docker image","uri":"/overengineering_adventures_2/"},{"categories":["Adventures in Overengineering"],"content":"Last post, we managed to do the following: created a Kubernetes clusters using microk8s; created a very simple Golang web server and its Docker image; created a Kubernetes deployment for the web server and a service to expose it. In this post, we will improve on the Docker image we previously built because we can always do better and that is the point of this whole series: each post must represent an improvement on the previous post, either by enhancing capabilities or employing better practices. And, if I didn’t do this post now, I would very quickly run out of space in my hard drive. Link to the code https://github.com/ornlu-is/askeladden/tree/v0.2 ","date":"2023-03-21","objectID":"/overengineering_adventures_2/:0:0","tags":null,"title":"Adventures in Overengineering 2: a better Docker image","uri":"/overengineering_adventures_2/"},{"categories":["Adventures in Overengineering"],"content":"Better Docker image? It isn’t immediately clear what is meant by a “better Docker image”, so let’s recap, it will help us make some sense out of this and understand why it is a good change to make. Last time, the Dockerfile that we created was the following: FROM golang:1.19 ADD . /askeladden WORKDIR /askeladden RUN go build ENTRYPOINT ./askeladden This resulted in a Docker image based on the official Golang 1.19 image with out application shipped alongside it. We still have this image saved on our local registry so let’s take a look at it using docker images: REPOSITORY TAG IMAGE ID CREATED SIZE askeladden latest 8324675e97c1 6 days ago 1e+03MB It seems that our Docker image is about 1Gb in size, which is a tad bit too much. The reason the image is so large is because it has Golang installed, which we don’t actually require for any purpose after the web server is built. The idea is simple: we create a container with Golang 1.19 and build our binary, just as before. But after building it, we create another container, based on a Debian image, and simply inject our binary into it. We can then discard the container we used for building the web server and we are left with a slimmer Docker image. So the first step is to pick the Debian image that we’ll use. We have no special preference for the Debian version, so we can use the latest version, which is called bookworm. However, we will pick the bookworm-slim variant, which is a Debian bookworm version, but without extra files that aren’t usually required by containers, such as manual pages and documentation. FROM golang:1.19 AS builder ADD . /askeladden WORKDIR /askeladden RUN go build FROM debian:bookworm-slim COPY --from=builder /askeladden/askeladden /usr/bin/askeladden ENTRYPOINT ./usr/bin/askeladden Note that we gave the alias builder to our first container by specifying AS builder and then injected the binary built inside it into the slimmed docker image with COPY --from=builder. Running docker images now gives us: REPOSITORY TAG IMAGE ID CREATED SIZE askeladden latest c2c2327d336a 5 minutes ago 81.1MB Which means that our new image is only 81.1Mb in size, 10 times less than what the previous image was. To check if this image is working as excepted, let us create a container based on it and publish it’s port so we can see our web server working by running: docker run -d -p 8080:8090 askeladden Navigating to http://localhost:8080/ in a browser shows that this is working as expected. ","date":"2023-03-21","objectID":"/overengineering_adventures_2/:1:0","tags":null,"title":"Adventures in Overengineering 2: a better Docker image","uri":"/overengineering_adventures_2/"},{"categories":["Adventures in Overengineering"],"content":"Deploying a container with the slim Docker image This is great, now we have to get our image into the cluster’s registry. So first we tag it: docker tag \u003cimage_ID\u003e localhost:32000/askeladden:slim Then we push it into the cluster’s registry: docker push localhost:32000/askeladden:slim We can check if this image was successfully pushed into the cluster’s registry by running: curl http://localhost:32000/v2/askeladden/tags/list | jq '.' curl and jq If you do not have curl nor jq installed you will obviously not be able to run the command above. But you should install them, they come in handy. curl http://localhost:32000/v2/askeladden/tags/list | jq '.' curl, which stands for “Client for URL”, is simply a command-line tool that can be used to transfer data to or from a web server. In this case, if we had simply written curl http://localhost:32000/v2/askeladden/tags/list, we would’ve obtained the desired result in JSON format but not “prettified”. This is a fairly short output, so it isn’t really required to “prettify” it, but it’s a good practice nonetheless. For that matter, we pipe the result, using the pipe | operator, into jq which is a simple JSON processor. The '.' part tells jq that the way we want our JSON to be processed, is that we do not want it processed at all, we are merely here for the cosmetic enhancements. Alternatively, we could’ve just navigated to http://localhost:32000/v2/askeladden/tags/list using a web browser, but it wouldn’t look as cool as using the terminal. The above command outputs the following: { \"name\": \"askeladden\", \"tags\": [ \"registry\", \"slim\" ] } Which means that our image is in the cluster’s registry! However, our pod still isn’t running the latest image. We can see that our pod is running with mk get pods: NAME READY STATUS RESTARTS AGE deployment-askeladden-dev-75c45dc7d9-r9hcf 1/1 Running 1 (112m ago) 6d And we can inspect this pod using: mk describe pod deployment-askeladden-dev-75c45dc7d9-r9hcf kubectl describe pod [PodName] The describe pod [PodName] command prints out a bunch of information on the existing pod. In fact, its usage is more general than this, since we can use it to describe any type of Kubernetes object. Its syntax is fairly simple and intuitive, it’s always describe [ObjectType] [ObjectName]. This produces the following output: Name: deployment-askeladden-dev-75c45dc7d9-r9hcf Namespace: askeladden-dev Priority: 0 Service Account: default Node: luis-thinkpad-t490/192.168.1.83 Start Time: Wed, 15 Mar 2023 21:56:14 +0000 Labels: app=askeladden pod-template-hash=75c45dc7d9 Annotations: cni.projectcalico.org/containerID: a1ce6f36e0f31e62f1d8574b0e02579f441490c66c2c22f60f8440c69aabfe84 cni.projectcalico.org/podIP: 10.1.68.19/32 cni.projectcalico.org/podIPs: 10.1.68.19/32 Status: Running IP: 10.1.68.19 IPs: IP: 10.1.68.19 Controlled By: ReplicaSet/deployment-askeladden-dev-75c45dc7d9 Containers: askeladden: Container ID: containerd://3806e5e9dffe4fd9e9519e1dea1149af619d2d6bda25b9b39155699b383f65b1 Image: localhost:32000/askeladden:registry Image ID: localhost:32000/askeladden@sha256:319994c8bbc2f0b69b5bdd26bfe3b0967e7858bd6426697b97c9c35b4da0b76a Port: \u003cnone\u003e Host Port: \u003cnone\u003e State: Running Started: Tue, 21 Mar 2023 20:23:02 +0000 Last State: Terminated Reason: Unknown Exit Code: 255 Started: Wed, 15 Mar 2023 21:56:15 +0000 Finished: Tue, 21 Mar 2023 20:22:42 +0000 Ready: True Restart Count: 1 Environment: \u003cnone\u003e Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4cv2j (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-4cv2j: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: \u003cnil\u003e DownwardAPI: true QoS Class: BestEffort Node-Selectors: \u003cnone\u003e Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type","date":"2023-03-21","objectID":"/overengineering_adventures_2/:2:0","tags":null,"title":"Adventures in Overengineering 2: a better Docker image","uri":"/overengineering_adventures_2/"},{"categories":["Adventures in Overengineering"],"content":"Creating a local kubernetes cluster with microk8s, creating a Golang web server and its Docker image and then deploying it to the microk8s kubernetes cluster.","date":"2023-03-15","objectID":"/overengineering_adventures_1/","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"There is a wide variety of blog posts on the internet teaching one how to create a simple web server in Golang. But most of them are incredibly boring and minor variations of each other. So I’m going to take a different approach: I’m going to deploy a simple REST API in Golang. Notice that I said deploy, not create, which means that my first step is a tiny bit different. Link to the code https://github.com/ornlu-is/askeladden/tree/v0.1 ","date":"2023-03-15","objectID":"/overengineering_adventures_1/:0:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"Creating a Kubernetes Cluster Yes, you read that right, the first step is to create our own Kubernetes cluster locally. Why? Because I’m too cheap to use any of the cloud providers out there and this has the upside of being a cool learning opportunity. After some research, which is just a fancy way of saying “looking at the first page of Google results”, I found that microk8s is the solution I am looking for getting a local Kubernetes cluster up and running. The official install guide seems straightforward enough so let’s give it a go. First step: sudo snap install microk8s --classic That was quick enough. The second step in the documentation is to run microk8s status --wait-ready Let’s do that. And this is where we find our first hurdle. The result of the above command is: Insufficient permissions to access MicroK8s. You can either try again with sudo or add the user luis to the 'microk8s' group: sudo usermod -a -G microk8s luis sudo chown -R luis ~/.kube After this, reload the user groups either via a reboot or by running 'newgrp microk8s'. Running the above as sudo does work, and it outputs the following: microk8s is running high-availability: no datastore master nodes: 127.0.0.1:19001 datastore standby nodes: none addons: enabled: ha-cluster # (core) Configure high availability on the current node helm # (core) Helm - the package manager for Kubernetes helm3 # (core) Helm 3 - the package manager for Kubernetes disabled: cert-manager # (core) Cloud native certificate management community # (core) The community addons repository dashboard # (core) The Kubernetes dashboard dns # (core) CoreDNS gpu # (core) Automatic enablement of Nvidia CUDA host-access # (core) Allow Pods connecting to Host services smoothly hostpath-storage # (core) Storage class; allocates storage from host directory ingress # (core) Ingress controller for external access kube-ovn # (core) An advanced network fabric for Kubernetes mayastor # (core) OpenEBS MayaStor metallb # (core) Loadbalancer for your Kubernetes cluster metrics-server # (core) K8s Metrics Server for API access to service metrics minio # (core) MinIO object storage observability # (core) A lightweight observability stack for logs, traces and metrics prometheus # (core) Prometheus operator for monitoring and logging rbac # (core) Role-Based Access Control for authorisation registry # (core) Private image registry exposed on localhost:32000 storage # (core) Alias to hostpath-storage add-on, deprecated I wish all obstacles in life were that easy to overcome. But it is cumbersome to write a few extra characters every time I want to interact with microk8s, so I’m going to also perform the other step suggested by microk8s, which is to add myself to the microk8s group. usermod -a -G microk8s luis In case you are not aware what usermod is, running man usermod on your Linux machine will elucidate you (man stands for manual, as in, instruction book, not as in manual labour, even though some of these things do sometimes feel like a chore). The manual gives us the following short description of usermod: “modify a user account”. Seems intuitive enough. However, the command microk8s is instructing us to run is sudo usermod -a -G microk8s luis, so we have to look a bit more carefully into the documentation to understand what these flags are. The -a flag is shorthand notation for --append which is used to add a user to the group, and -G stands for --groups which is used to specify to which groups we want to add our user. So the command does exactly what one would expect: it adds the user luis (hey, that’s me!) to the microk8s. chown -R luis ~/.kube man chown tells us that this command is used to change the user and/or group ownership of a given file. We are running it with the -R flag which stands for --recursive and give it the user luis (hey, that’s me again!) and the ~/.kube directory. Basically, this sets us are the owners of everything inside the given directory. I hope that I’m not the only one tha","date":"2023-03-15","objectID":"/overengineering_adventures_1/:1:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"Enabling the Kubernetes Dashboard We are running our own local cluster, so we need some easy way to get visibility on what is inside it and what its state is. For that matter, we can use the Kubernetes dashboard add-on. Apparently, it is supposed to be super easy to enable a new add-on with microk8s, so let’s try getting the Kubernetes dashboard up and running by typing microk8s enable dashboard into our terminal. This outputs the following: Infer repository core for addon dashboard Enabling Kubernetes Dashboard Infer repository core for addon metrics-server Enabling Metrics-Server serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrolebinding.rbac.authorization.k8s.io/microk8s-admin created Metrics-Server is enabled Applying manifest serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created secret/microk8s-dashboard-token created If RBAC is not enabled access the dashboard using the token retrieved with: microk8s kubectl describe secret -n kube-system microk8s-dashboard-token Use this token in the https login UI of the kubernetes-dashboard service. In an RBAC enabled setup (microk8s enable RBAC) you need to create a user with restricted permissions as shown in: https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md So a bunch of stuff happened. At least microk8s told us what happened instead of making all of these operations under the hood which means that now we have the pleasure of figuring out what the hell just happened! On line 1, microk8s seems to just be searching for the repository with the desired add-on and line 2 begins the process of enabling it. This is where the fun really begins. The first step in enabling the Kubernetes dashboard is, on line 3, is to search for a repository for the metrics-server and begin enabling it on line 4. Each Kubernetes node has a kubelet which is a service responsible for managing pod deployments and instructing the container runtime to start or stop containers are required, and the metrics-server scrapes resource metrics from each existing kubelet and exposes them in the Kubernetes API server via the metrics API to be used by the Horizontal and Vertical Pod Autoscalers (HPA and VPA). It should be noted that its purpose is not to serve as a monitoring solution, its metrics are to be used by HPA and VPA, but the metrics it collects do show up in the Kubernetes dashboard, so that is why it is has to be enabled. It is only after the metrics-server is enabled that microk8s will begin creating the objects required to enable the dashboard. To access the dashboard, run microk8s dashboard-proxy, which outputs: Checking if Dashboard is running. Infer repository core for addon dashboard Infer repository core for addon metrics-server Waiting for Dashboard to come up. Trying to get token from microk8s-dashboard-token Waiting for secret token (attempt 0)","date":"2023-03-15","objectID":"/overengineering_adventures_1/:2:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"Creating the web server We are doing this in Go, so the first step is to create a directory and a Go module by running go mod init. In my case, since my GitHub name is ornlu-is and I decided to name the package askeladden, this equates to running: go mod init github.com/ornlu-is/askeladden This will create a go.mod file that is used by Go to track the code’s dependencies. Now we can create our main.go file and get to coding! The code we need to add to this file is the following: package main import ( \"fmt\" \"net/http\" ) func rootPathHandler(w http.ResponseWriter, req *http.Request) { fmt.Fprintf(w, \"Strange women lying in ponds distributing swords is no basis for a system of government.\\n\") } func main() { http.HandleFunc(\"/\", rootPathHandler) err := http.ListenAndServe(\":8090\", nil) if err != nil { panic(err) } } You might be thinking: “This web server does nothing except print out a Monty Python quote”. Yes, that is exactly what it does. I did say this was a simple web server, and it doesn’t get much easier than this. You can test this out by running go run main.go and navigating to http://localhost:8090/. ","date":"2023-03-15","objectID":"/overengineering_adventures_1/:3:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"Containerizing our web server Docker This section assumes that you have Docker installed and configured on your machine. If not, go check out the official documentation, it’s pretty high quality. Kubernetes is defined as being a system for automating deployment, scaling, and management of containerized applications. We have a Kubernetes cluster, we have an application, but we still need to containerize it! The first step is to write a Dockerfile in our root directory. I am going to be using the image for Golang 1.19, to match the version I am running locally, and have the image simply build and run the application. FROM golang:1.19 ADD . /askeladden WORKDIR /askeladden RUN go build ENTRYPOINT ./askeladden But before testing this in our cluster, we should do a local sanity check to be sure everything is working as expected. Firstly, we have to build our image, which is as simple as being in the same directory of the Dockerfile and running docker build . -f Dockerfile -t 'askeladden' docker build . -f Dockerfile -t 'askeladden' When we give the Docker CLI the build command, we are telling it to build an image. Images are later used to create containers, which are running instances of a given image. The . refers to the path of the context, which, in the context (ha-ha, funny guy) of Docker, refers to the set of files that can be used by the build process. The -f is shorthand notation for --file and is the path to the Dockerfile, while -t is short for --tag and allows us to name/tag our image. In this case, I only named it. We can check our image by listing all images built by Docker using docker images: REPOSITORY TAG IMAGE ID CREATED SIZE askeladden latest 8324675e97c1 57 seconds ago 1e+03MB Great, the image is there! So now we have to create and run a container based on this image to check if our web server is working. This can be achieved by running: docker run -d -p 88:8090 askeladden docker run -d -p 88:8090 askeladden The run command creates and runs a container. The -d flag, which stands for --detach, will run the container in the background of the terminal, meaning it will not block the terminal window. Since the container has its own network, we have to expose the port 8090 where our web server can be accessed inside the container to outside the container. This is achieved by publishing the port and mapping it into a port on the host machine, i.e., my computer, with the -p flag followed by the specification of the host port and container port in the \u003chost_port\u003e:\u003ccontainer_port\u003e format. Finally, we specify the name of the image we want to build our container from which, in this case, is just askeladden. Docker will first look for this image locally and, if it doesn’t find it, it will attempt to retrieve it from a public image repository. We can see the list of running containers by typing docker ps, and we can check that it is properly running by navigating to http://localhost:88/. So, now we’re fairly certain that our web server is containerized as desired, so we can stop the container using the docker stop \u003ccontainer_ID\u003e command (you can retrieve the container ID from the output of docker ps). Now the output of docker ps doesn’t show anything but the container wasn’t actually deleted, it was merely stopped. If we type docker ps -a, where -a stands for --all, we can see that our container is in an Exited status. To avoid taking up resources, let’s just delete it with docker rm \u003ccontainer_ID\u003e. ","date":"2023-03-15","objectID":"/overengineering_adventures_1/:4:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"Creating a Kubernetes Namespace Kubernetes dashboard In this section, most, if not all, of the commands that I will run that get or describe a given Kubernetes objectives can be replaced by using the Kubernetes dashboard that was previously enabled. But it will not make you look as cool as if you did everything in the terminal. We want to deploy the web server into our cluster in a way that isolates it from the remaining pods that are running in it, which means that we have to deploy it into a separate namespace. To get a list of what namespaces are already configured on the cluster, we can simply run mk get namespaces: NAME STATUS AGE kube-system Active 23h kube-public Active 23h kube-node-lease Active 23h default Active 23h We can see that there are several namespaces whose name starts with kube, meaning that these are Kubernetes system namespaces and we do not want to tinker with those, at least for now. We could deploy our web server to the default namespace (which would be where it would go if we did not specify any namespace), but it will be helpful in the future to have this running in its own namespace. So, let’s start creating Kubernetes definition files! Create a directory called k8s and a file inside it named namespace.yaml with the following content: apiVersion: v1 kind: Namespace metadata: name: askeladden-dev To create a Kubernetes object from a .yaml file, all we have to do is run mk create -f k8s/namespaces.yaml We can easily check if it has been created by running mk get namespaces again: NAME STATUS AGE kube-system Active 23h kube-public Active 23h kube-node-lease Active 23h default Active 23h askeladden-dev Active 5h51m However, Kubernetes objects will still be created into the default namespace if the target namespace is not specified. Again, in typical software engineering fashion, we will go to great lengths to save a few keystrokes. We can check our Kubernetes config with mk config view to look for what contexts are configured. Contexts are merely aliases for cluster parameters to make our lives easier. apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://127.0.0.1:16443 name: microk8s-cluster contexts: - context: cluster: microk8s-cluster user: admin name: microk8s current-context: microk8s kind: Config preferences: {} users: - name: admin user: token: REDACTED There is only one context, microk8s, so it is obvious which context we are using. But if it weren’t as obvious, we could get that information by running mk config current-context The context that we are using by default does not have any namespace associated with which means, you’ve guessed it, the namespace that we default to is the default namespace. This also means we have to create our own context, which can be readily achieve through: mk config set-context askeladden-dev --namespace=askeladden-dev --cluster=microk8s-cluster --user=admin It is simply a context that, when used, will automatically fill unspecified cluster parameters with the ones in the context. We can now see that our config has a new context: apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://127.0.0.1:16443 name: microk8s-cluster contexts: - context: cluster: microk8s-cluster namespace: askeladden-dev user: admin name: askeladden-dev - context: cluster: microk8s-cluster user: admin name: microk8s current-context: askeladden-dev kind: Config preferences: {} users: - name: admin user: token: REDACTED All that is needed is to switch to it using: mk config use-context askeladden-dev The context? askeladden-dev. The keystrokes? 20% less. Great success. ","date":"2023-03-15","objectID":"/overengineering_adventures_1/:5:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"Deploying our containerized web server into our Kubernetes cluster So what is running in the namespace we just created? We can figure this out by running: mk get all Which readily lets us know that No resources found in askeladden-dev namespace. There is one issue that I haven’t addressed yet. We built the container image locally, but the cluster does not have access to the Docker daemon that is running on my machine and thus it cannot use the image we have built to create the container. To first step to taking care of this is to enable the registry in microk8s via: microk8s enable registry Which outputs the following: Infer repository core for addon registry Infer repository core for addon hostpath-storage Enabling default storage class. WARNING: Hostpath storage is not suitable for production environments. deployment.apps/hostpath-provisioner created storageclass.storage.k8s.io/microk8s-hostpath created serviceaccount/microk8s-hostpath created clusterrole.rbac.authorization.k8s.io/microk8s-hostpath created clusterrolebinding.rbac.authorization.k8s.io/microk8s-hostpath created Storage will be available soon. The registry will be created with the size of 20Gi. Default storage class will be used. namespace/container-registry created persistentvolumeclaim/registry-claim created deployment.apps/registry created service/registry created configmap/local-registry-hosting configured Again, a bunch of stuff happened. microk8s created all the objects required to have its own registry. So now we have to push our local image into the registry that is in the cluster, so that microk8s can use that image to create our container. For that matter, we first get our image ID from the output of docker images, and then we tag it with: docker tag \u003cimage_ID\u003e localhost:32000/askeladden:registry Note that localhost:32000 is a port on our host machine that is connected to the cluster’s registry. Now all that is left to do is to push this image into the registry with: docker push localhost:32000/askeladden:registry Now we are ready to deploy our container to the cluster! All that we need now is to create a Kubernetes definition file k8s/deployment.yaml, with the following contents: apiVersion: apps/v1 kind: Deployment metadata: name: deployment-askeladden-dev labels: env: dev spec: replicas: 1 selector: matchLabels: app: askeladden template: metadata: labels: app: askeladden spec: containers: - name: askeladden image: localhost:32000/askeladden:registry This will create three different Kubernetes objects: a Pod with our container running inside it, a ReplicaSet which ensures that we always have a given number of copies of the Pod running (in this case, it’s just one), and a Deployment object, which handles how Pods are updated, rolled out, scaled, etc. We can see all of these objects by running: mk get all Which outputs this OCD triggering table formatted result: NAME READY STATUS RESTARTS AGE pod/deployment-askeladden-dev-75c45dc7d9-r9hcf 1/1 Running 0 78s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/deployment-askeladden-dev 1/1 1 1 78s NAME DESIRED CURRENT READY AGE replicaset.apps/deployment-askeladden-dev-75c45dc7d9 1 1 1 78s But (there’s always a “but”), our web server is running inside the container, which is inside the pod, which is inside the cluster. We cannot access it as we have before because we have not exposed a port in the cluster with which to communicate with the server. To solve this, we have to create a Kubernetes Service of the NodePort kind (this is not the only option, it is just the option I felt like doing right now). As before, we create a new Kubernetes definition file, k8s/service.yaml, with the following contents: apiVersion: v1 kind: Service metadata: name: askeladden-dev-service labels: app: askeladden spec: type: NodePort ports: - port: 80 targetPort: 8090 nodePort: 30008 protocol: TCP selector: app: askeladden There are a few things to note here. The targetPort is the port on the Pod that we want to expose. In ou","date":"2023-03-15","objectID":"/overengineering_adventures_1/:6:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":["Adventures in Overengineering"],"content":"Future Overengineering Steps It’s all about saving those precious keystrokes, and there are two major keystroke consumers as of now: Having to push the Docker images into the cluster’s registry manually; Having to create the Kubernetes objects manually. So the next objective might be to reduce one of those. Maybe. I don’t know. ","date":"2023-03-15","objectID":"/overengineering_adventures_1/:7:0","tags":null,"title":"Adventures in Overengineering 1: deploying a Golang web server","uri":"/overengineering_adventures_1/"},{"categories":null,"content":"Hi, I’m Luís! ","date":"2023-02-16","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work I currently work as a Systems Engineer for Cloudflare. ","date":"2023-02-16","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Academic studies I have a BSc in Engineering Physics and a MSc in Applied Mathematics, both granted by University of Lisbon - Instituto Superior Técnico. ","date":"2023-02-16","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work-related interests My main interests are backend engineering, infrastructure, statistical algorithms for problems at scale, and cryptography. ","date":"2023-02-16","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Socials GitHub LinkedIn ","date":"2023-02-16","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"}]