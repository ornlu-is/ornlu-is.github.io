[{"categories":["Adventures in Overengineering"],"content":"How to install Salt on Raspberry Pi machines.","date":"2023-10-10","objectID":"/overengineering_3/","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"I have three machines with Ubuntu installs up and running but there is one thing that I really want to avoid: having to ssh into them to install software, since the effort of installing a given package/software is always multiplied by three. As such, I thought this was an appropriate time to refresh my Salt knowledge, which is an event-driven framework/automation tool whose name probably derives from the high blood pressure it induces whenever it is used to perform changes on production environment machines. Just a reminder that all this is still for the same initial goal: over-engineering a basic Golang web server. ","date":"2023-10-10","objectID":"/overengineering_3/:0:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Installing salt-master I do not want to use any of the Raspberry Pis as a salt-master, so I’ll be using my laptop for it. First, we import the Salt Project repository key using: sudo curl -fsSL -o /etc/apt/keyrings/salt-archive-keyring-2023.gpg https://repo.saltproject.io/salt/py3/ubuntu/22.04/amd64/SALT-PROJECT-GPG-PUBKEY-2023.gpg And then we create the apt sources list file using: echo \"deb [signed-by=/etc/apt/keyrings/salt-archive-keyring-2023.gpg arch=amd64] https://repo.saltproject.io/salt/py3/ubuntu/22.04/amd64/latest jammy main\" | sudo tee /etc/apt/sources.list.d/salt.list As usual with installing something new, we first update our packages with: sudo apt-get update To install the salt-master, we simply run: sudo apt-get install salt-master Since I do not want to have to start the salt-master service every time I want to do any Salt related operations, I’m going to have it start when my system boots up with: sudo systemctl enable salt-master \u0026\u0026 sudo systemctl start salt-master Finally, we can start the service with: sudo systemctl start salt-master ","date":"2023-10-10","objectID":"/overengineering_3/:1:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Configuring salt-master The default configuration that comes with the install can be found in /etc/salt/master. We will not, for the time being deviate from it. ","date":"2023-10-10","objectID":"/overengineering_3/:2:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Installing salt-minion on the Raspberry Pis Now we need to install Salt in the minion nodes, i.e., on the Raspberry Pi machines. The process is fairly similar to installing salt-master. I’m going to connect to zeus (one of my Raspberry Pi) via ssh and import the Salt Project repository key as before: sudo curl -fsSL -o /etc/apt/keyrings/salt-archive-keyring-2023.gpg https://repo.saltproject.io/salt/py3/ubuntu/22.04/arm64/SALT-PROJECT-GPG-PUBKEY-2023.gpg Followed by creating the apt sources list file with: echo \"deb [signed-by=/etc/apt/keyrings/salt-archive-keyring-2023.gpg arch=arm64] https://repo.saltproject.io/salt/py3/ubuntu/22.04/arm64/latest jammy main\" | sudo tee /etc/apt/sources.list.d/salt.list Note that, if you are copying the commands from the official documentation for ARM64 architectures, the docs have a typo where the install commands for ARM64 are the same as those for AMD64. To get around this, all you have to do is switch all occurrences of amd64 to arm64. If you do not do this, you’ll get the following error when you attempt to install salt-minion: N: Skipping acquire of configured file 'main/binary-amd64/Packages' as repository 'https://repo.saltproject.io/salt/py3/ubuntu/22.04/arm64/latest jammy InRelease' doesn't support architecture 'amd64' Update the packages and install salt-minion with: sudo apt-get update sudo apt-get install salt-minion Exactly the same as before, I want this to run on start up, so I’ll enable it with systemctl, and start the service because I want to use it now: sudo systemctl enable salt-minion \u0026\u0026 sudo systemctl start salt-minion ","date":"2023-10-10","objectID":"/overengineering_3/:3:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Configuring salt-minion In theory, the salt-master service should be discoverable by the name salt. However, when I check the status of the salt-minion service with systemctl, I see a bunch of messages stating: salt DNS lookup or connection check of 'salt' failed The way I found to get around this is fairly simple. In zeus, I navigated to /etc/salt/minion.d and created as master.conf file with the following contents: master: \u003cMASTER_IP\u003e Restarted the salt-minion service with: sudo systemctl restart salt-minion And voilà, the error message is gone! ","date":"2023-10-10","objectID":"/overengineering_3/:4:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Accepting keys Alas, an error has been replaced by another. It seems that my Salt minion is having trouble connecting to the master due to authentication issues. This is because of how Salt operates: minions and masters generate RSA keys on their first start up and these are then used for private key infrastructure (PKI) based authentication. On the Salt master, there is a client running called salt-key that allows for managing minion authentication. If we run: sudo salt-key We get the following output: Accepted Keys: Denied Keys: Unaccepted Keys: zeus Rejected Keys: Now it has become extremely obvious why I was still getting an error: I needed my Salt master to accept zeus key! This boils down to a single command: sudo salt-key -a zeus Wonderful, I can now finally proceed to verifying the installation. ","date":"2023-10-10","objectID":"/overengineering_3/:5:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Verification We’ll follow the usual process of verifying installations: attempting to get the version of the install. In Salt, we can achieve this with: sudo salt '*' test.version Which outputs the following: zeus: 3006.3 Meaning that zeus is running version 3006.3 and that this whole process was successful! Great, now rinse and repeat this process for poseidon and ares, and I now have three Salt minions up and running! ","date":"2023-10-10","objectID":"/overengineering_3/:6:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Next steps I can now install stuff on my Raspberry Pi machines without having to ssh and do it manually, which is fantastic but I am still not ready to install and configure my Kubernetes cluster. Before that, I need a way to monitor the health of machines from my laptop so the next post will probably be about Grafana (and related software). ","date":"2023-10-10","objectID":"/overengineering_3/:7:0","tags":null,"title":"Adventures in Overengineering 3: Installing Salt to manage Raspberry Pi machines","uri":"/overengineering_3/"},{"categories":["Adventures in Overengineering"],"content":"Installing operating systems on the Raspberry Pi machines, verifying network access with ping, and connecting via ssh.","date":"2023-09-28","objectID":"/overengineering_2/","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"Now that I have some hardware available, I need to install an operating system on these machines so that I can then install some software that will enable me to actually do something with these things. Since I do not want to have to connect a monitor to these machines every time I need to use them, I also have to set up some type of remote access. ","date":"2023-09-28","objectID":"/overengineering_2/:0:0","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"Preparing the microSD cards This part is extremely simple. On my Windows machine, I downloaded the Raspberry Pi imager, a software that burns Raspberry Pi compatible OS images onto storage devices. Flashing the OS is straightforward with this software: just plug in the storage device, select the operating system in the imager (in my case, I chose 64-bit Ubuntu Server 23.04), name the computers, enable ssh and configure Wi-Fi connection in the advanced configurations, and wait until the imager finished burning the OS image. Rinse and repeat for all three microSD cards, and then just plug them onto the Raspberry Pi machines. ","date":"2023-09-28","objectID":"/overengineering_2/:1:0","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"Checking the OS installs During my childhood, I developed an obsession on Greek mythology thanks to the popular RTS game called Age of Mythology. As such, I decided to name my Raspberry Pi machines after Greek gods: Zeus, Poseidon, and Ares. After turning on the machines, these should be discoverable in my home network. To check if these machines are working properly, I’ll make use of the ping Linux utility. ","date":"2023-09-28","objectID":"/overengineering_2/:2:0","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"ICMP This is an excellent opportunity to review the ICMP protocol. The Internet Control Message Protocol is a network layer protocol used by network devices to diagnose network communication issues. It is connectionless in the sense that it is not required to have an open connection between two devices for one of them to send an ICMP message. Broadly speaking, an ICMP message looks like this: ICMP message The type field provides some broad categorization about the control message while the code contains additional context on what is being reported. The data section contains a copy of the IPv4 header as well as some bits of the IPv4 packet that caused the error, hence why it has variable (but limited) length. This protocol has two main uses: Error reporting: when a device attempts to send data to another and something goes wrong, ICMP generates error messages to share with the sending device; Network diagnostics: the protocol can also be used to assert the health of network connections between devices; ","date":"2023-09-28","objectID":"/overengineering_2/:3:0","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"Pinging the machines So, armed with this basic knowledge of ICMP, we can now look closely at the ping Linux utility. man ping reveals that all this does is send a ICMP ECHO_REQUEST message to the given host, so let’s do that with ping poseidon.local, which yeilds the following result: PING poseidon.local (192.168.1.76) 56(84) bytes of data. 64 bytes from 192.168.1.76: icmp_seq=1 ttl=64 time=71.8 ms 64 bytes from 192.168.1.76: icmp_seq=2 ttl=64 time=67.4 ms 64 bytes from 192.168.1.76: icmp_seq=3 ttl=64 time=18.4 ms 64 bytes from 192.168.1.76: icmp_seq=4 ttl=64 time=26.5 ms 64 bytes from 192.168.1.76: icmp_seq=5 ttl=64 time=12.2 ms 64 bytes from 192.168.1.76: icmp_seq=6 ttl=64 time=6.84 ms 64 bytes from 192.168.1.76: icmp_seq=7 ttl=64 time=8.46 ms To check the remaining machines, I just have to run the same command for those hosts (ares.local and poseidon.local). And they are all working properly, yey! ","date":"2023-09-28","objectID":"/overengineering_2/:4:0","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"Connecting to the machines Note that I selected the imager configurations that would enable ssh and configure. This means that I can use these machines from my laptop simply using my Wi-Fi connection! Let’s check what we get when I try to ssh into one of them: Welcome to Ubuntu 23.04 (GNU/Linux 6.2.0-1004-raspi aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Sun Oct 1 19:25:35 WEST 2023 System load: 0.3 Temperature: 34.1 C Usage of /: 4.7% of 58.21GB Processes: 147 Memory usage: 2% Users logged in: 1 Swap usage: 0% IPv4 address for wlan0: 192.168.1.76 100 updates can be applied immediately. 66 of these updates are standard security updates. To see these additional updates run: apt list --upgradable Last login: Thu Sep 28 22:22:07 2023 Great, we’re all set to start using these machines! ","date":"2023-09-28","objectID":"/overengineering_2/:5:0","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"Next steps Now that I have infrastructure, it is probably time I start thinking about how I will manage it. I have to do some research for this, so it will probably take some time until my next post. ","date":"2023-09-28","objectID":"/overengineering_2/:6:0","tags":null,"title":"Adventures in Overengineering 2: Installing an Operating System","uri":"/overengineering_2/"},{"categories":["Adventures in Overengineering"],"content":"Acquiring a bunch of hardware to take my overengineering adventure to the next level.","date":"2023-09-28","objectID":"/overengineering_1/","tags":null,"title":"Adventures in Overengineering 1: Inventory","uri":"/overengineering_1/"},{"categories":["Adventures in Overengineering"],"content":"A few months ago, I started a series of posts about my attempt to completely overengineer a simple Go web server. After a few posts, I had to archive that series. I was not pleased with the result, it wasn’t… enough. Last time, I began this adventure by running a Kubernetes instance on my laptop. This time, I’ve gone deeper into the overengineering madness. Keep in mind that the end goal is still to deploy a very basic Golang web server. ","date":"2023-09-28","objectID":"/overengineering_1/:0:0","tags":null,"title":"Adventures in Overengineering 1: Inventory","uri":"/overengineering_1/"},{"categories":["Adventures in Overengineering"],"content":"Acquiring hardware Yep, you read that right, the first step is to acquire some hardware. How much hardware? Well, I want to create a three node Kubernetes cluster. As such, I need three machines. As with any engineering project, budget is something to keep in mind as I’d rather not go broke over this. This means that the most in-budget option for me is to buy a some Raspberry Pi machines. In particular, I bought 3 Raspberry Pi 4 Model B machines with the following specs: Processor: Broadcom BCM2711, quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz Memory: 8GB LPDDR4 Wireless: 2.4GHz and 5.0GHz IEEE 802.11b/g/n/ac There are obviously some more details in the specs for these machines, but these are the main details I cared about. Additionally, for a first pass at this, I’ll be using microSD storage for these machines, meaning I also had to buy an adapter for my laptop to be able to write into these cards. Each Raspberry Pi also requires its own power source and, for the first time OS install, I’ll also need an HDMI to micro-HDMI cable. The full shopping list can be seen below: Item Quantity Price/item (€) Raspberry Pi Model B 1.5GHz 8GB 3 85.49 Joy-IT two fan acrylic case 3 14.35 Sandisk microSD 64GB 3 15.75 USB-C power source 3.0A 15.3W black 3 9.75 USB2.0 microSD reader 1 7.68 HDMI 1.4 \u003c-\u003e micro-HDMI cable 1 3.75 Which means I ended up spending a grand total of 387.45€. My wallet weeps but the thirst for overengineering must be properly quenched. Here is a badly taken picture of the whole material: Hardware for overengineering ","date":"2023-09-28","objectID":"/overengineering_1/:1:0","tags":null,"title":"Adventures in Overengineering 1: Inventory","uri":"/overengineering_1/"},{"categories":["Adventures in Overengineering"],"content":"Assembling the hardware Putting these things together is quite simple. To my surprise, the acrylic cases also came with heat sinks, which is a nice touch. Here is a picture of what the Raspberry Pi looks like after adding the heat sinks, which are the things inside the red circles: Raspberry Pi with heat sinks To connect the fans, I had to look up what the GPIO pin layout was for this model. It seems that the outermost second pin is 5V DC power, while the pin immediately to its right is the ground pin. Thus, connecting the fans is as simple as the picture below demonstrates: Connecting the case fans to the Raspberry Pi Then I attempted to place the Raspberry Pi inside the case. I say attempted because the board didn’t fit inside the case. This is because the board had some imperfections around its edges. No problem, this is very easy to solve. All I had to do was very calmly file these imperfections until they were gone and the board could be placed in the case. Assembling the acrylic case is easy since it’s just three pieces that are held together with magnets. The end result looks something like this: Raspberry Pi machines inside their cases ","date":"2023-09-28","objectID":"/overengineering_1/:2:0","tags":null,"title":"Adventures in Overengineering 1: Inventory","uri":"/overengineering_1/"},{"categories":["Adventures in Overengineering"],"content":"Next steps I have inventory. The inventory is assembled. The next step will be to install an operating system in each of the Raspberry Pi boards. ","date":"2023-09-28","objectID":"/overengineering_1/:3:0","tags":null,"title":"Adventures in Overengineering 1: Inventory","uri":"/overengineering_1/"},{"categories":["Golang"],"content":"Why io.Reader interface takes a byte slice instead of returning a byte slice","date":"2023-08-22","objectID":"/go_io_reader_interface/","tags":null,"title":"Why does Go's io.Reader interface take a slice of bytes as argument?","uri":"/go_io_reader_interface/"},{"categories":["Golang"],"content":"I rencetly watched a GopherCon talk titled “Understanding Allocations: The Stack and the Heap” by Jacob Walker, and found it really interesting, especially the final conclusion on why the io.Reader interface is the way it is. As it turns out, it is related to how memory allocation works in Go. More specifically, where the memory is allocated. ","date":"2023-08-22","objectID":"/go_io_reader_interface/:0:0","tags":null,"title":"Why does Go's io.Reader interface take a slice of bytes as argument?","uri":"/go_io_reader_interface/"},{"categories":["Golang"],"content":"The io.Reader interface There isn’t much to say here, everyone that has been programming in Go has surely found this interface out in the wild multiple times, and it looks like this: type Reader interface { Read(b []byte) (n int, err error) } But why does it return the number of bytes read instead of returning a slice of bytes? It would make sense that if I tell something to read, it would just give me what it read, instead of requiring me to allocate a slice where it will write to. To understand this, we have to go on a little detour. ","date":"2023-08-22","objectID":"/go_io_reader_interface/:1:0","tags":null,"title":"Why does Go's io.Reader interface take a slice of bytes as argument?","uri":"/go_io_reader_interface/"},{"categories":["Golang"],"content":"Stack and Heap memory in Go Broadly speaking, there are two places where variables can be allocated in your Go program. There is the heap, and then there are the stacks, usually one for each goroutine that exists. For the purpose of this post, the main difference between a stack and a heap, is that whatever is in the heap eventually gets garbage collected. This prompts one immediate question: where will my variables be allocated? In essence, there is no way of knowing this while you are writing code, only the compiler can answer this. However, there are some instances where your variables are typically written to the heap. Usually, whatever is shared down, i.e., passing pointers to things, stays on the stack and whatever is shared up, i.e., returning pointers, references, or things with pointers in them, tends to go on the heap. Additionally, there are some more use cases where memory is typically allocated on the heap: When a value could possibly be referenced after the function that constructed the value returns; When the value is too large to fit on the stack; When the size of a value is unknown at compile time; When values are shared with pointers; When variables are stored in interface variables; When variables store anonymous functions, or are variables captured by a closure. Again, this is usually, only the compiler really knows where stuff gets allocated. ","date":"2023-08-22","objectID":"/go_io_reader_interface/:2:0","tags":null,"title":"Why does Go's io.Reader interface take a slice of bytes as argument?","uri":"/go_io_reader_interface/"},{"categories":["Golang"],"content":"A worked example Let us look at a practical example. Below I have a simple piece of code, with a main function that calls the function read, assigns its return value to a variable, and then prints it out using println (using fmt.Println will yield different results). The read function creates a slice of two bytes, assigns a value to each of those, and then returns it. package main func read() []byte { b := make([]byte, 2) b[0] = 1 b[1] = 2 return b } func main() { b := read() println(b) } So what is going on the heap and what is staying in the stack? We can investigate this by passing some flags to our go build command. When we use the go build tool, the underlying tool is the compile tool. And since we are looking for a flag that allows us to investigate what optimisation decisions are being performed by the compiler, we look for those flags using go tool compile -h, which uncovers the following suspect: -m print optimization decisions Nice, we’ve found what we were looking for. Running go build -gcflags \"-m\" in our directory gives us the following output: ./main.go:3:6: can inline read ./main.go:10:6: can inline main ./main.go:11:11: inlining call to read ./main.go:4:11: make([]byte, 2) escapes to heap ./main.go:11:11: make([]byte, 2) does not escape From the fourth of the output, we can see that, in line 4 of our code, the variable allocated has been allocated to the heap. Why? On line 4, we are creating a slice of bytes inside the function read. A slice is essentially a pointer to an underlying array, and two integers, thus, when the function read returns and assigns its return value to b, we now have two pointers to the same memory position. However, the pointer created inside read cannot be used by anything in our code, it is unreachable. As such, it is garbage and must be garbage collected. Hence, it goes on the heap. So what happens if, instead of having our read function create and return a slice of bytes, we have it receive a slice of bytes as argument and populate it with data? For that purpose, I wrote the following program: package main func read(b []byte) { b[0] = 1 b[1] = 2 } func main() { b := make([]byte, 2) read(b) println(b) } For which we have to following output of the optimisation decisions: ./main.go:3:6: can inline read ./main.go:8:6: can inline main ./main.go:10:6: inlining call to read ./main.go:3:11: b does not escape ./main.go:9:11: make([]byte, 2) does not escape In line 3, b does not escape because it is a copy of the slice defined in the main function, which means that it is a copy of the pointer (and two integers for the slice capacity and length), not of the underlying array, that allocation has already been performed in main. ","date":"2023-08-22","objectID":"/go_io_reader_interface/:3:0","tags":null,"title":"Why does Go's io.Reader interface take a slice of bytes as argument?","uri":"/go_io_reader_interface/"},{"categories":["Golang"],"content":"So why does io.Reader.Read receive a byte slice as argument? Now we are ready to answer this question. And the answer is extremely simple: because if it returned a slice of bytes instead, a lot more garbage collection would have to be performed and this would result in increased latency. ","date":"2023-08-22","objectID":"/go_io_reader_interface/:4:0","tags":null,"title":"Why does Go's io.Reader interface take a slice of bytes as argument?","uri":"/go_io_reader_interface/"},{"categories":["Golang"],"content":"Final note Please do not write your program with the mindset of minimizing how much stuff gets written into the heap. Strive for correctness and readability, and only go for this type of optimisation if you require your program to go faster and you have data to support your hypothesis that latency is being caused by garbage collection. ","date":"2023-08-22","objectID":"/go_io_reader_interface/:5:0","tags":null,"title":"Why does Go's io.Reader interface take a slice of bytes as argument?","uri":"/go_io_reader_interface/"},{"categories":["Golang"],"content":"How to implement the tee channel concurrency pattern in Go","date":"2023-08-21","objectID":"/go_tee_channel_pattern/","tags":null,"title":"Go Concurrency Patterns: Tee Channel","uri":"/go_tee_channel_pattern/"},{"categories":["Golang"],"content":"If you’ve ever used the Linux tee command, you can probably guess what this pattern is about. At a first glance, this might seem similar to the fan-out concurrency pattern and, in a way, it is. But there is one crucial difference. The fan-out concurrency pattern splits the input from one channel into several channels for concurrent processing, while the tee channel pattern creates two channels with the exact same data as the original one. Link to the code https://github.com/ornlu-is/go_tee_channel_pattern ","date":"2023-08-21","objectID":"/go_tee_channel_pattern/:0:0","tags":null,"title":"Go Concurrency Patterns: Tee Channel","uri":"/go_tee_channel_pattern/"},{"categories":["Golang"],"content":"The Tee Channel pattern in Go Tee channel pattern I think this is a really cool pattern. Not because of what it does, but because it forces us to reason about concurrency in a careful manner. First, let us set the stage. I have written a very simple generator below which we will use as our data stream: func numberStream() \u003c-chan float64 { ch := make(chan float64) numberStrings := []float64{1., 2., 3., 4., 5., 6., 7., 8., 9., 10.} go func() { for _, numberString := range numberStrings { ch \u003c- numberString } close(ch) return }() return ch } Now we are ready to start reasoning on how we can tee our channel into two. Fundamentally, what we want is to create a function, teeChannel, that takes one channel as input, and returns two channels, both with the same data in them. Based on this, we can write the following naïve implementation: func teeChannel(c \u003c-chan float64) (\u003c-chan float64, \u003c-chan float64) { tee1 := make(chan float64) tee2 := make(chan float64) go func() { defer func() { close(tee1) close(tee2) }() for val := range c { tee1 \u003c- val tee2 \u003c- val } return }() return tee1, tee2 } In our main function, we’d call this function in the following way: func main() { done := make(chan struct{}) defer close(done) dataStream := numberStream() teedStream1, teedStream2 := teeChannel(dataStream) for val1 := range teedStream1 { fmt.Printf(\"tee1: %f\\n\", val1) fmt.Printf(\"tee2: %f\\n\", \u003c-teedStream2) } } When we run this, everything seems fine! But it really isn’t. Note that, in teeChannel, we are writing first to tee1 and only then to tee2. What happens if we swap out the order of the writes and then try to run our code? We get the following error: fatal error: all goroutines are asleep - deadlock! So why is this happening? The reason is fairly simple. tee2 (the same applies to tee1) is an unbuffered channel, meaning that, when we write to it, if there is nothing that reads from it, the goroutine will block until there is something that reads the data in this channel before continuing execution. However, we are reading from tee1 first. So our program is stuck trying to read from tee1 while waiting from something to read from tee2, hence the deadlock. We can solve this issue by employing a for-select loop, coupled with setting a local copy of each channel to nil if it has already read a value. This will ensure that writes to one channel will not block writes to another and that both channels get written into. The end results looks like this: func teeChannel(c \u003c-chan float64) (\u003c-chan float64, \u003c-chan float64) { tee1 := make(chan float64) tee2 := make(chan float64) go func() { defer func() { close(tee1) close(tee2) }() for val := range c { for i := 0; i \u003c 2; i++ { var tee1, tee2 = tee1, tee2 select { case tee1 \u003c- val: tee1 = nil case tee2 \u003c- val: tee2 = nil } } } return }() return tee1, tee2 } Now we are free from deadlocks, but there is one limitation to our implementation. We are still using unbuffered channels which means that, when we tee our channel into two, have to read one value from one channel, and one value from the other. If we just keep reading from one channel, we’ll end up just writing the data to that channel twice. In essence, we not only have to care about the writing process, but also the reading process when using this pattern, which makes it prone to errors. ","date":"2023-08-21","objectID":"/go_tee_channel_pattern/:1:0","tags":null,"title":"Go Concurrency Patterns: Tee Channel","uri":"/go_tee_channel_pattern/"},{"categories":["Golang"],"content":"How to implement the pipeline concurrency pattern in Go","date":"2023-08-21","objectID":"/go_pipeline_pattern/","tags":null,"title":"Go Concurrency Patterns: Pipeline","uri":"/go_pipeline_pattern/"},{"categories":["Golang"],"content":"Yet another Go concurrency pattern! This particular pattern is extremely helpful in composing several transformations from data incoming from a stream, and is know as the pipeline concurrency pattern. In a pipeline, we define several stages, which are nothing more than objects that take data in, perform some operation on it, and then output the transformed data. Link to the code https://github.com/ornlu-is/go_pipeline_pattern ","date":"2023-08-21","objectID":"/go_pipeline_pattern/:0:0","tags":null,"title":"Go Concurrency Patterns: Pipeline","uri":"/go_pipeline_pattern/"},{"categories":["Golang"],"content":"The Pipeline pattern in Go Pipeline pattern Translating the above definition to Go, the pipeline pattern is simply a function that takes a channel, performs some operation on the data from that channel, and outputs it to another channel. Usually, these functions are easily composable, meaning you can chain several calls that will represent exactly what is going on with your data. For this example, I created a simple function that writes a bunch of floats to a channel via goroutine and returns that channel: func numberStream() \u003c-chan float64 { ch := make(chan float64) numberStrings := []float64{1., 2., 3., 4., 5., 6., 7., 8., 9., 10.} go func() { for _, numberString := range numberStrings { ch \u003c- numberString } close(ch) return }() return ch } This will act as our data stream. Suppose that, for some reason, you need to perform two operations on the values of your stream: you need to take each number’s power, and then multiply that by two. From our pipeline pattern definition, we can easily deduce what it will look like in code. Firstly, and obviously, the function will take as argument a channel and return a channel. Then, since we want to perform an operation on each value from the input channel and write it to the output channel, we need to spawn a goroutine that will handle that. And that’s it! Pretty simple, right? To take the power of each input we write the following function: func power(data \u003c-chan float64) \u003c-chan float64 { ch := make(chan float64) go func() { defer close(ch) for value := range data { ch \u003c- value * value } }() return ch } Likewise, to duplicate each input, our function looks very much like the previous one: func duplicate(data \u003c-chan float64) \u003c-chan float64 { ch := make(chan float64) go func() { defer close(ch) for value := range data { ch \u003c- 2. * value } }() return ch } Let’s put this all together in our main function. To see this in action, we simply have to create our data stream, feed it as input to the power function, which in turn is fed as input to the duplicate function, thus forming our pipeline: func main() { dataStream := numberStream() for value := range duplicate(power(dataStream)) { fmt.Println(value) } fmt.Println(\"bye\") } Note We can range over the output of these functions because they are built as generators. If you are curious about this design pattern, check out this other post that I’ve written: https://ornlu-is.github.io/go_design_pattern_generator/ Now, if we run the above code, we get exactly what we expected, which is pretty cool: 2 8 18 32 50 72 98 128 162 200 bye ","date":"2023-08-21","objectID":"/go_pipeline_pattern/:1:0","tags":null,"title":"Go Concurrency Patterns: Pipeline","uri":"/go_pipeline_pattern/"},{"categories":["Golang"],"content":"How to implement the fan-out concurrency pattern in Go","date":"2023-08-21","objectID":"/go_fan_out_pattern/","tags":null,"title":"Go Concurrency Patterns: Fan-Out","uri":"/go_fan_out_pattern/"},{"categories":["Golang"],"content":"I have written a blog post about the fan-in concurrency pattern and, unlike most texts on this matter, I left its counterpart, the fan-out concurrency pattern, to have its own post. While these two patterns are mostly used in tandem, I believe that it is fundamental to understand them separately, so as to not create any mental blockers that would coherce us to only use one pattern when the other is also required. Link to the code https://github.com/ornlu-is/go_fan_out_pattern ","date":"2023-08-21","objectID":"/go_fan_out_pattern/:0:0","tags":null,"title":"Go Concurrency Patterns: Fan-Out","uri":"/go_fan_out_pattern/"},{"categories":["Golang"],"content":"The Fan-Out pattern in Go Fan-Out pattern While the fan-in concurrency pattern equates to multiplexing, i.e., combining several data streams into a single one, the fan-out pattern does the opposite, it takes a single data stream and creates several concurrent streams. The obvious application for this pattern is when you want to concurrently process a data stream and you do not need your data to be processed in order. This last observation is crucial, since this pattern does not preserve the order by which the data is handled. As with the fan-in example, I’ll be creating a data stream from strings with the names of numbers, like so: func someNumberStrings() \u003c-chan string { ch := make(chan string) numberStrings := []string{\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"} go func() { for _, numberString := range numberStrings { ch \u003c- numberString } close(ch) return }() return ch } Demultiplexing this is incredibly straightforward. All we need is to read from the original channel into as many new channels as we’d like. As such, we’ll create a function, demultiplexer, which will spawn a new goroutine that will read from the original channel. Additionally, it will return a channel just so that we have some signal of when the goroutine stopped reading from the original stream. We end up with something like this: func demultiplexer(worker string, ch \u003c-chan string) chan struct{} { stop := make(chan struct{}) go func() { defer close(stop) for v := range ch { fmt.Println(worker, v) } }() return stop } Now, if we want to split our original stream into two streams, it suffices to call demultiplexer twice. We can then read from these new streams with a for-select loop, as shown below: func main() { originalStream := someNumberStrings() demuxedStream1 := demultiplexer(\"demux1\", originalStream) demuxedStream2 := demultiplexer(\"demux2\", originalStream) for { if demuxedStream1 == nil \u0026\u0026 demuxedStream2 == nil { break } select { case _, ok := \u003c-demuxedStream1: if !ok { demuxedStream1 = nil } case _, ok := \u003c-demuxedStream2: if !ok { demuxedStream2 = nil } } } fmt.Println(\"bye\") } Note that, due to the nature of goroutines, whenever you run this program, you’ll get a different result. For example, I obtained the following: demux2 one demux2 two demux2 three demux2 four demux2 five demux1 six demux1 eight demux1 nine demux1 ten demux2 seven bye But you can, and most certainly will, obtain a different one. This is inline with what I stated at the beginning of this post: this pattern is only useful if we do not care about the order in which our data is handled. ","date":"2023-08-21","objectID":"/go_fan_out_pattern/:1:0","tags":null,"title":"Go Concurrency Patterns: Fan-Out","uri":"/go_fan_out_pattern/"},{"categories":["Golang"],"content":"How to implement the fan-in concurrency pattern in Go","date":"2023-08-21","objectID":"/go_fan_in_pattern/","tags":null,"title":"Go Concurrency Patterns: Fan-In","uri":"/go_fan_in_pattern/"},{"categories":["Golang"],"content":"It is not uncommon to have a piece of software that is concurrently reading from multiple streams of data. However, for a multitude of possible reasons, we might want to aggregate these streams into a single one, for example, to send the data to another service. Fortunately, this is not a new problem, and the solution for it is well known as the Fan-In pattern. Link to the code https://github.com/ornlu-is/go_fan_in_pattern ","date":"2023-08-21","objectID":"/go_fan_in_pattern/:0:0","tags":null,"title":"Go Concurrency Patterns: Fan-In","uri":"/go_fan_in_pattern/"},{"categories":["Golang"],"content":"The Fan-In pattern in Go Fan-In pattern As stated before, the idea behind this is incredibly simple: the fan-in pattern combines several data streams into one. You’ve possibly seen this defined in terms of “multiplexing”. Fret not, that is just a fancy word for “merging multiple streams into one”. There is nothing like learning by doing, so let us get right to it. The first thing we need is to emulate multiple streams of data. For that matter, I created two functions someNumberStrings and someNumbers which both spawn a goroutine and return a channel to where the aforementioned goroutine will write some strings: func someNumberStrings() \u003c-chan string { ch := make(chan string) numberStrings := []string{\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"} go func() { for _, numberString := range numberStrings { ch \u003c- numberString } close(ch) return }() return ch } func someNumbers() \u003c-chan string { ch := make(chan string) numbers := []string{\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"} go func() { for _, number := range numbers { ch \u003c- number } close(ch) return }() return ch } Before diving into the code for multiplexing these data streams, let us think about what we need. The two data streams will be combined into one, so we need to create a new channel, which will be our multiplexed channel (or stream). Then, we’ll need a function, multiplex, that reads from the two channels and writes to our multiplexed channel. Since we are writing concurrently, we must also read concurrently, so we’ll require two goroutines calling our multiplex function. As usual with channels, we’ll have to close the multiplexed channel once we are done reading. Finally, we need to synchronize all of this. For that purpose, we’ll employ a WaitGroup from the sync package. Putting it all together yields the following: func fanIn(channels ...\u003c-chan string) \u003c-chan string { // create WaitGroup var wg sync.WaitGroup // create multiplexed stream multiplexedStream := make(chan string) // function that takes data from one of the two streams // and writes it into our multiplexed stream multiplex := func(c \u003c-chan string) { // when we finish reading from the data stream, tell // our WaitGroup that we are no longer reading from that stream defer wg.Done() // write from the data stream to the multiplexed stream for i := range c { multiplexedStream \u003c- i } } // tell our WaitGroup that we are waiting for two stream to finish wg.Add(len(channels)) // read concurrently from the data streams into the multiplexed stream for _, c := range channels { go multiplex(c) } go func() { // wait for both streams to close and then close the multiplexed stream wg.Wait() close(multiplexedStream) }() return multiplexedStream } Now that we are armed with all the necessary pieces, we can put this together in our main function: func main() { ch1 := someNumberStrings() ch2 := someNumbers() exit := make(chan struct{}) mergedCh := fanIn(ch1, ch2) go func() { for val := range mergedCh { fmt.Println(val) } close(exit) }() \u003c-exit fmt.Println(\"bye\") } When I run this, I get the following output: one 1 two three 2 3 four 4 5 five six 6 seven eight 7 8 nine ten 9 10 bye Note that there is no specific ordering of the output, i.e., we are not processing one input from each stream at a time. Moreover, you might get a different output depending on which computer you run this in. As is, this pattern does not care about the ordering of the data in the multiplexed stream. ","date":"2023-08-21","objectID":"/go_fan_in_pattern/:1:0","tags":null,"title":"Go Concurrency Patterns: Fan-In","uri":"/go_fan_in_pattern/"},{"categories":["Golang"],"content":"Makefiles are a popular way of making the development process easier, since they can be used to chain several commands that allow developers to build, test, run, etc. their code. Additionally, they can also be used to create a make-based build/test system. In this post, I’m going to cover something how to set up a Makefile rule to test Golang code and enforce test coverage, i.e., have the rule fail if a predefined test coverage threshold is not met. ","date":"2023-08-17","objectID":"/go_makefile_code_coverage/:0:0","tags":null,"title":"Enforcing test coverage in Go with Makefile","uri":"/go_makefile_code_coverage/"},{"categories":["Golang"],"content":"TL;DR, just let me copy it Let us take a look at the Makefile and then we’ll walk through it step by step: SHELL=/bin/bash TEST_COVERAGE_THRESHOLD=80.0 test: go test ./... -coverprofile=coverage.out coverage=$$(go tool cover -func=coverage.out | grep total | grep -Eo '[0-9]+\\.[0-9]+') ;\\ rm coverage.out ;\\ if [ $$(bc \u003c\u003c\u003c \"$$coverage \u003c $(TEST_COVERAGE_THRESHOLD)\") -eq 1 ]; then \\ echo \"Low test coverage: $$coverage \u003c $(TEST_COVERAGE_THRESHOLD)\" ;\\ exit 1 ;\\ fi ","date":"2023-08-17","objectID":"/go_makefile_code_coverage/:1:0","tags":null,"title":"Enforcing test coverage in Go with Makefile","uri":"/go_makefile_code_coverage/"},{"categories":["Golang"],"content":"Okay, I regret just copying it, how does this work? The first two lines are defining global variables. The SHELL variable is present in all Makefiles and specifies which shell we are using. By default, Makefile uses sh but we want more functionalities so we want our Makefile to use bash, so we place the path to bash in our SHELL variable. The second variable is our desired minimum test coverage, i.e., the value below which our make rule should fail. Line 4 simply specifies the rule name, and is followed by its definition. We want to keep our Makefile simple and intuitive, so we name our rule test, meaning that this can be ran by writing make test. On line 5 we test our Go code. In case any test fails, our rule execution terminates promptly, printing the test report. ./... tells the go tool to recursively look for test files in our project directory and run all files of the format *_test.go. Finally, the -coverprofile=coverage.out, outputs the test coverage profile into a file called coverage.out. We do not directly care about what is written in this file. Line 6 is where we make use of the coverage.out file. By running go tool cover -func=coverage.out we now calculate the percentage of test coverage for each function as well as the total coverage of our project. We are defining a simple global threshold, so we have to extract only the line of the pertaining to the total coverage. Fortunately, since this line starts with total, we can simply pipe the result from go tool cover -func=coverage.out into grep and tell it to extract the line containing that string using grep total. However, this line still needs a bit more processing since we have to remove everything except for the percentage of total coverage. In comes grep once again, we just pipe the result of the previous grep command to grep -Eo '[0-9]+\\.[0-9]+', where the E flag tells grep that our pattern is a regular expression, and the o flag is to guarantee we fetch only nonempty parts of the result. Finally, we save all of this in a local coverage variable. Since we no longer have any use for our coverage.out file, we can safely delete it in line 7 using rm coverage.out. On Line 8, we check if the our test coverage is lower than our defined threshold. Both the obtained test coverage and the test coverage threshold are given as floating point numbers, which means that they cannot be natively compared when using bash. However, we can use bc for this purpose. bc stands for basic calculator and, if given a condition, it returns 1 if that confition is true and 0 otherwise. Which means that all that is left is check if the output we obtain from bc is equal to 1, which is achieved through -eq 1. On lines 9 and 10, we print an informative message that the test coverage is too low and terminate the rule execution with error code 1. On the last line, we close our if block. It took me more time than I care to admit to find out how to do this, so I am writting it down so I do not forget it. ","date":"2023-08-17","objectID":"/go_makefile_code_coverage/:2:0","tags":null,"title":"Enforcing test coverage in Go with Makefile","uri":"/go_makefile_code_coverage/"},{"categories":["Golang"],"content":"How to implement the generator design pattern in Golang","date":"2023-08-17","objectID":"/go_design_pattern_generator/","tags":null,"title":"Go Design Patterns: Generator","uri":"/go_design_pattern_generator/"},{"categories":["Golang"],"content":"I like the generator pattern. I hadn’t realized it, but I had already encountered this pattern before when I used to program in Python. I recently found myself requiring to loop over a large sequence of numbers. Naively, I created a slice with all the values I required and then I looped over them. However, I was not satisfied with this solution so I went digging and found the generator pattern. This is an incredibly simple pattern. It’s purpose is very direct: a generator is something that yields a sequence of values one at a time. Let us look at an example. ","date":"2023-08-17","objectID":"/go_design_pattern_generator/:0:0","tags":null,"title":"Go Design Patterns: Generator","uri":"/go_design_pattern_generator/"},{"categories":["Golang"],"content":"Without the generator pattern Let us assume that, in our project, something that we need to do quite regularly is to range over a set of linearly spaced floating point numbers, e.g., 0.1, 0.3, 0.5, etc. This is not that uncommon in scientific computing or statistics. The first time we implement this, we went with something like start := 0.1 end := 0.6 step := 0.1 for i := start; i \u003c end; i += step { // do something } But then we had to implement this again in another piece of code, and another, and the boilerplate code just started piling up. ","date":"2023-08-17","objectID":"/go_design_pattern_generator/:1:0","tags":null,"title":"Go Design Patterns: Generator","uri":"/go_design_pattern_generator/"},{"categories":["Golang"],"content":"With the generator pattern Below is the implementation of our generator function: func linearSpaceGenerator(start, end, step float64) chan float64 { ch := make(chan float64) go func(ch chan float64) { defer close(ch) for n := start; n \u003c end; n += step { ch \u003c- n } }(ch) return ch } We create a bidirectional unbuffered channel and spawn a goroutine to write into it. Since the channel is unbuffered, every single value that we write into it will block until another goroutine reads the channel. Then, after writing all our values into the channel, we simply call the built-in close function to let downstream processes know that we are done. Using this function is painfully easy and looks much more elegant: package main import \"fmt\" func linearSpaceGenerator(start, end, step float64) chan float64 { ch := make(chan float64) go func(ch chan float64) { defer close(ch) for n := start; n \u003c end; n += step { ch \u003c- n } }(ch) return ch } func main() { for i := range linearSpaceGenerator(0.1, 2., 0.1) { fmt.Printf(\"i: %f\\n\", i) } } So we have the goroutine spawned by linearSpaceGenerator sending values to a channel and in our main function we have the main goroutine (yep, it is a goroutine) reading from this channel. In essence, this is not the most necessary pattern ever, but it does look much better than constantly having to write C-style for loops. ","date":"2023-08-17","objectID":"/go_design_pattern_generator/:2:0","tags":null,"title":"Go Design Patterns: Generator","uri":"/go_design_pattern_generator/"},{"categories":["Golang"],"content":"How to use Go build tags to separate unit tests from integration or functional tests","date":"2023-08-16","objectID":"/go_build_tags/","tags":null,"title":"Using Go build tags for defining sets of tests","uri":"/go_build_tags/"},{"categories":["Golang"],"content":"Some time ago, I was looking for a way to define a clear separation in a Go project between my unit tests and my integration tests. At the time, the solution I came up with involved creating a submodule and some Makefile shenanigans to separate these into two sets. I was unhappy with this approach, since it ended up being convoluted and prone to error as the code base evolved. However, today I recently learned about Go’s build tags that seemlessly allow me to separate my integration tests from my unit tests. ","date":"2023-08-16","objectID":"/go_build_tags/:0:0","tags":null,"title":"Using Go build tags for defining sets of tests","uri":"/go_build_tags/"},{"categories":["Golang"],"content":"What are build tags in Go? Build tags are used by the Go compiler to know when and which code needs to be compiled. While some people use this when developing software that might have features enabled for a given operating system, e.g., Windows 10 vs. some Linux distro, but have not yet been implemented into other OSs. To add a build tag to a specific .go file, all you have to do is add the following magic comment to the top of the file: // +build \u003cyour tag goes here\u003e package something Note that an empty line was added after the build tag. This is not by accident, it is by design: if you forget to add the newline after the build tag, the Go compiler will interpret it as a comment instead, rendering your build tag useless. If you specified a build tag, say tagus, on some .go file, and you then run go build, that tagged file will not be compiled, build tags exclude your code from being compiled as their default behaviour. To include tagged files when compiling you simply have to add the -tags flag followed by the tags to add, as shown below: go build -tags tagus Files without any built tag are compiled irrespective of what tags you specify to the go build command. ","date":"2023-08-16","objectID":"/go_build_tags/:1:0","tags":null,"title":"Using Go build tags for defining sets of tests","uri":"/go_build_tags/"},{"categories":["Golang"],"content":"Build tag boolean logic You probably noticed that the flag we just used was a plural noun, and you are likely thinking that this must mean we can specify several tags when compiling. You are very much correct. Moreover, build tags in Go allow for, albeit limited, boolean logic. Three boolean operations are allowed with build tags: OR - by separating tags with a space: // +build one two AND - by separating tags with a comma: // +build one,two NOT - by prefixing tags with an exclamation mark: // +build !one ","date":"2023-08-16","objectID":"/go_build_tags/:2:0","tags":null,"title":"Using Go build tags for defining sets of tests","uri":"/go_build_tags/"},{"categories":["Golang"],"content":"How can these be used for testing? When you run Go tests using go test, you are actually compiling the code in your _test.go files, as well as any code used, directly or indirectly, by these files. Then the resulting binary is ran, your test output is produced, and then the binary is deleted (although there is some caching going on to build the test binary faster the next time you run go test). If it compiles, then surely it can also make use of build tags. In the beginning of this blog post, I said I used this to separate unit from integration tests. The first question you might want to ask is why I want to do that. The main reason is because unit tests are fast, but integration tests tend to be very slow. In my case, my unit tests only need to build the test binary, while my integration tests require a Docker Compose stack to be built and started, which is a bit cumbersome when you are writing code that can be easily covered by unit tests. As such, on my integration_test.go file, I added a simple build tag, via // +build integration, and then I added the following two Makefile targets: integration-tests: go test -tags integration -v ./... unit-tests: go test -v ./... This means that, to only run unit tests, make unit-tests is the way to go. However, if I want to run all of my tests, including integration tests, I can simply call make integration-tests and go grab a cup of coffee because my integration tests stack takes forever to run. ","date":"2023-08-16","objectID":"/go_build_tags/:3:0","tags":null,"title":"Using Go build tags for defining sets of tests","uri":"/go_build_tags/"},{"categories":["Interesting Bugs"],"content":"An investigation on why a failed Kubernetes deploy job still deployed a newer service version","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"A while back, I was investigating a bug where my deployment job had failed, but the service had been deployed. At first I thought this was weird, afterwards I thought this was extremely concerning: had this happened before and I was just noticing now by accident? Since I did not want to have failed deployment jobs actually deploying my services, I took a closer look at this issue. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:0:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"The situation What happened was pretty simple. I updated the Kubernetes manifest files for my service, opened the PR, got it reviewed and approved, the CI builds that ran on my PR were all successful, life was good. So I merged the PR and, after a few minutes, I checked the version of the running service in the staging environment and saw that it was the most recent version. Everything seemed fine (little did he know that everything was not fine). I went about my life and opened a release PR, got it approved, CI builds looked good so I proceeded with the merge and consequent deployment. After a few minutes of celebrating my victory over yet-another-JIRA-ticket, I get an automated message on the chat: “Production deployment job failed”. I had just snatched defeat from the jaws of victory. The first thing I did after getting that message, was to check what was the version of the service running in production, and, to my surprise, it was the newer version. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:1:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"What actually happened I had to dig deeper into this, meaning I had to understand how the deployment job worked. The actual deployment job pretty much only did one thing: it ran kubectl apply on the manifests from my repository. And this was the command that caused the job to fail. So the underlying action of the deployment job failed, the deployment job itself failed, but the deployment happened. Just like in the Inception movie, “we need to go deeper”. There was only one suspect: kubectl apply. For my Kubernetes resources, I had several files in my repository that were combined into a single manifest file and kubectl apply was called to apply them all. However, one of the resources specified in the Kubernetes resource failed to be created. And I assumed that kubectl rolled back on the other resource it created since it failed to create all of them. This was my mistake: kubectl apply does not undo its changes if one of the resources failed to be created. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:2:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Interesting Bugs"],"content":"How could this have been prevented Ideally, the deployment job would begin by performing kubectl apply, but with the --dry-run flag set. This flag can take two main values: client or server. If used with the former, the kubectl tool will attempt to validate the manifest without actually performing any request to the Kubernetes cluster. On the other hand, if used with the server value, then an actual request is performed to the Kubernetes cluster, which will attempt to validate it without creating any of the specified resources. However, operating under the constraints of different parts of software/infrastructure being owned by different teams, means that not all can be changed. Nonetheless, we can have the next best thing. Notice that I only got a chat alert message when the production deployment job failed. What about the staging deployment job? Had it failed too? Turns out, it had, but I did not get any chat message for a failed staging deployment because we hadn’t configured such alerts for our staging environment. It is a sub-optimal solution, but I can’t force other teams to change their stuff, I can only shrug and walk it off. ","date":"2023-08-15","objectID":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/:3:0","tags":null,"title":"Kubernetes deploy job failed but the service was deployed","uri":"/kubernetes_deploy_job_failed_but_my_service_was_deployed/"},{"categories":["Golang"],"content":"A deep dive on the internals of Go slices","date":"2023-08-14","objectID":"/go_slices/","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Slices are Go’s bread and butter. However, more often than not, small mistakes happen because it isn’t exactly clear what actually is a slice. Since they are so prevalent in Go code, I decided to dive a bit deeper into their internal structure, how they are handled in different situations, and how some of the issues that arise can be avoided. ","date":"2023-08-14","objectID":"/go_slices/:0:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"But first, arrays Before diving into slices, it is important to know that Go also has the concept of arrays. An array is basically a numbered sequence of elements of the same type, known as the element type, and are useful when you are sure of how many memory positions you require. You can declare an array in the following ways. x := [3]int{} y := [3]int{1, 2, 3} z := [...]int{4, 5, 6, 7} The first example creates an array of size three with all its elements initialised as the element type’s default value. One very important detail here is that the size of an array is part of its type, which consequently means that you cannot specify an array’s size via a variable because its type must be resolved at compile time, and it also means that you cannot write functions that work with arrays of any size. The second example creates an array of size three with the given values and the last example foregoes the explicit integer specification of the array’s size in detriment of inferring it from the number of elements in the given slice literal. The last two details that we should be aware when working with arrays is that, in Go, arrays are values, which means that if you assign one array to another, you are copying all of its elements. Moreover, arrays in Go are comparable, meaning you can write the following: x := [...]int{2, 3} y := [...]int{3, 4} if x == y { // do something } ","date":"2023-08-14","objectID":"/go_slices/:1:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"What is a slice in Go? In Go, arrays are rarely used and are primarily a building block for the concept of slices. So… what are slices? A slice isn’t one thing, it’s actually three things: A pointer to an underlying array; The length of the underlying array; And the capacity of the underlying array. When you create a slice, you declare the slice’s element type, its length, and, optionally, its capacity. The Go runtime takes the given element type and capacity and allocates a contiguous memory segment of capable of holding a number of elements equal to the given capacity, and addresses the pointer to the beginning of this memory segment. Note that if you only specify the length and not the capacity of the slice, Go will assume that the capacity is equal to the length. Unlike arrays, slices are not comparable except with the nil value, which is the default value for a slice. ","date":"2023-08-14","objectID":"/go_slices/:2:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Initialising slices There are a few ways one can declare a slice: a := []int{1, 2} b := []int{} var c []int d := make([]int, 3) e := make([]int, 0, 2) Let us go through these one by one. The first declaration uses a slice literal to initialise the slice, meaning that it creates a slice populated with the given values in the order they are presented. This will infer both the length and capacity of the slice from the number of elements in the slice literal. The second one behaves much like the first one, but we are initialising a slice with an empty slice literal, which means that, if we print out the contents of b, we get the following: [] which is an empty slice. You might think that the third slice declaration produces a similar result to the second one, and that’s where you’re wrong. The third only declares a slice of integers, it does not perform any initialisation, meaning that its value is the slice default value, which is nil. The last two use the built-in make function to create slices. When creating slices, make expects either two or three arguments. The first is the type of slice you are trying to create, the second is that slice’s length, and the third is its capacity. When you use make to create a slice with non-zero length, keep in mind that you will be creating a slice with that number of elements with their values set to the element type’s default value. In other words, the output of the fourth slice declaration is: [0, 0, 0] However, if you just wish to allocate the memory for a slice, you can specify zero length and some non-zero capacity, much like the last declaration. Note that this will create an empty slice, on par with the second declaration, but the underlying allocated array will have the specified capacity. ","date":"2023-08-14","objectID":"/go_slices/:3:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Growing slices and why you should initialise them with make Let us say you have a slice of integers that is currently holding the numbers 1 and 2, and you want to add a third one. In Go, this is performed via the built-in append function, which takes as arguments the slice to which you wish to append new elements, and a variable number of new elements to append, and returns a copy of the resulting slice, which you tipically will use to override the original one: x := []int{1, 2} x = append(x, 3) If x was initialised from a slice literal that had two elements, then x has capacity and length both equal to two. But when I appended 3 to it, it now has length three which is more than the previous capacity. Which prompts the question: so what is the role of a slice’s capacity? To understand this, we need to know what goes on under the hood when we append something to a slice that is already at its capacity limit. When you append to a slice, you are adding one or more values to it, and each of these values will logically increase the slice’s length by one. The interesting bit happend when the length is already equal to the capacity. In this case, your slice has run out of space in the underlying array’s memory to add new elements. As such, the Go runtime will allocate a new slice with larger capacity, copy the original slice to the new one, add the new elements to it, and the new slice is then returned by append. Logically, these operations all take time. You are no longer just adding one element to a slice, you are creating a new slice, allocating memory, copying all elements to the new slice and only then you add a new element to it. Moreover, Go’s garbage collector will now have the additional task of freeing up the memory used by the old slice. As such, it is a good practice to create a slice with an upper bound on its capacity whenever possible: x := make([]int, 0, upperBound) This will avoid performing all the aforementioned extra computations and help you squeeze a tiny bit more of performance from your application. ","date":"2023-08-14","objectID":"/go_slices/:4:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Reslicing, a.k.a., how you’ll probably shoot yourself in the foot One operation on slices that is supported in Go is the slicing operation. This allows you to obtain a subset of a given slice: x := []int{1, 2, 3, 4} // [1, 2, 3, 4] y := x[:2] // [1, 2] But there is one caveat with slicing: it does not create a copy of the data. Instead, the new slice object created by slicing has a new length, but the same capacity as the original slice and its pointer is pointing to an element of the same underlying array. This effectively means that if you rewrite one of the elements of y, e.g., y[1]=666, it will also change the element in the same position for x. And the same holds true if you take a slice of a slice of a slice (and so on). Slicing is a powerful tool, but must be used with care, espectially when performing value assignments, since it might result in some unexpected behaviour. ","date":"2023-08-14","objectID":"/go_slices/:5:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"Using copy We have seen slicing as a way of copying parts of a slice and potentially shoot ourselves in the foot. The obvious question that arises is: how can I safely copy the contents of a slice, to another slice, without having to deal will all these pointer shenanigans? Go has got your back with the built-in copy function. This function takes two arguments, a destination slice and a source slice, and copies as many elements of the source slice to the destination slice as the destination slice’s length allows. Additionally, it also returns the number of copied elements. For example, if you want to copy just the first two elements of a given slice into an entirely independent slice, you’d simply write: x := []int{1, 2, 3, 4} y := make([]int, 2) _ = copy(y, x) Now you can freely manipulate y without having to worry about what will happen to x. ","date":"2023-08-14","objectID":"/go_slices/:6:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Golang"],"content":"References https://go.dev/ref/spec#Slice_types https://go.dev/blog/slices-intro https://go.dev/doc/effective_go#slices Jon Bodner, “Learning Go”, O’Reilly, 1st Edition ","date":"2023-08-14","objectID":"/go_slices/:7:0","tags":null,"title":"A deep dive on Golang slices","uri":"/go_slices/"},{"categories":["Computer Networks"],"content":"An overview of the NetFlow v5 protocol","date":"2023-08-13","objectID":"/netflow_v5/","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"NetFlow is a protocol developed by Cisco that has had several iterations over the years. This network protocol’s main focus is to collect IP traffic information and export it for monitoring purposes. A router configured to collect NetFlow will aggregate data packets into what is known as a flow, which is basically a summary of the traffic passing through the device in a given time span. Of the multiple iterations of the NetFlow protocol that Cisco has produced, two have cemented themselves as the most commonly used protocols: NetFlow v5 and NetFlow v9. Today, I’ll focus on the NetFlow v5 description. ","date":"2023-08-13","objectID":"/netflow_v5/:0:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"The NetFlow v5 datagram When a device is configured to collect and send (which is usually performed via UDP) flow data according to the NetFlow v5 protocol, that device will periodically send NetFlow v5 datagrams which as composed of a header and a variable number of flows, as depicted in the image below: NetFlow v5 datagram Meaning that for each UDP packet you receive from your collecting device, you might get several flows worth of information. ","date":"2023-08-13","objectID":"/netflow_v5/:1:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"Header datagram specification The NetFlow v5 header is composed of 24 bytes, with each byte corresponding to the following information: Bytes Field Description 0-1 version NetFlow export format version number 2-3 count Number of flows exported in this packet 4-7 SysUptime Current time in milliseconds since the export device booted 8-11 unix_secs Current count of seconds since 0000 UTC 1970 12-15 unix_nsecs Residual nanoseconds since 0000 UTC 1970 16-19 flow_sequence Sequence counter of total flows seen 20 engine_type Type of flow-switching engine 21 engine_id Slot number of the flow-switching engine 22-23 sampling_interval First two bits hold the sampling mode; remaining 14 bits hold value of sampling interval While some of these fields may seem obvious, other require a bit more clarification. Obviously, since we are dealing with NetFlow v5, the version field will always correspond to the number five. The count field, while it is made out of two bytes, which might lead some to deduce that it is a number between zero and 65,536, this is not the case. In fact, the maximum number of flow in each NetFlow v5 datagram is 30. The engine_type field provides an identifier of how the collecting device handles flow-switching. Since NetFlow v5 is a proprietary protocol from Cisco, this is mapped into a Cisco-specific flow-switching technology. For monitoring purposes, this field requires a great deal of understanding of the underlying Cisco technologies and how your devices are configured to extract any type of meaningful information. For that reason, flow monitoring services tend to ignore this field. The engine_id field is a configurable identifier for the flow collector. Finally, the last field that requires a bit more understanding is the sampling_interval. A NetFlow collector doesn’t necessarily export flows that contain absolutely exact information on the observed traffic. For devices that handle large loads, this is usually not computationally efficient. As such, NetFlow collectors can be configured to employ sampling, where the flows will be calculated based on a reduced sample of the network’s IP traffic. Broadly speaking, there are two main sampling modes: Sampled NetFlow - flows are calculated based on x-th packet, i.e., if this is set to an interval of 10, then the flows reported contain are calculated based on the 1st, 10th, 20th, etc., packets. This sampling method is usually advised against, since it might hide periodic traffic patterns; Random Sampled NetFlow - flows are calculated based on random draws of incoming packets, much like random sampling. It provides on average consistency on the number of packets used to calculate the flows and is overall more statistically accurate than the Sampled NetFlow scheme. ","date":"2023-08-13","objectID":"/netflow_v5/:2:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"Flow datagram specification The flow datagram is a fair bit larger than the header, with each flow containing 48 bytes of data organised as follows: Bytes Field Description 0-3 srcaddr Source IP address 4-7 dstaddr Destination IP address 8-11 nexthop IP address of next hop router 12-13 input SNMP index of input interface 14-15 output SNMP index of output interface 16-19 dPkts Packets in the flow 20-23 dOctets Total number of Layer 3 bytes in the packets of the flow 24-27 First SysUptime at start of flow 28-31 Last SysUptime at the time the last packet of the flow was received 32-33 srcport TCP/UDP source port number or equivalent 34-35 dstport TCP/UDP destination port number or equivalent 36 pad1 Unused (zero) bytes 37 tcp_flags Cumulative OR of TCP flags 38 prot IP protocol type 39 tos IP type of service (ToS) 40-41 src_as Autonomous system number of the source, either origin or peer 42-43 dst_as Autonomous system number of the destination, either origin or peer 44 src_mask Source address prefix mask bits 45 dst_mask Destination address prefix mask bits 46-47 pad2 Unused (zero) bytes Let us clarify some of these fields. As you might’ve already noticed, the srcaddr and dstaddr fields, which correspond, respectively, to the source and destination IP addresses of the traffic, are only four bytes long. This means that NetFlow v5 is limited uniquely to IPv4 traffic, which is a testament to the fact that it was released a long time ago. The tcp_flags field is a single byte used to represent TCP flags via a cumulative OR. If you are not familiar with TCP, it is sufficient to understand that each TCP datagram contains information on the TCP flags, which describe what operation or outcome was performed/observed. There are eight possible flags, which makes it fairly straight-forward to one-hot encode these. Then, to get a single byte representation of all possible combinations, we simply take the cumulative OR function of their bytes. One of the most important fields in NetFlow v5 is the prot field, which pertains to the possible protocol. While it might seem confusing that the protocol is a single number, we can easily get the corresponding name via the mapping published by the Internet Assigned Numbers Authority (IANA). ","date":"2023-08-13","objectID":"/netflow_v5/:3:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Computer Networks"],"content":"Some considerations on using NetFlow v5 for analytics There are two major considerations to take when building analytics with NetFlow v5. The first one is to understand that a flow is not an instant snapshot of your traffic. It is a summary of your traffic, collected over a given period of time, which is where the First and Last fields come in. These allow you to know for how long was data collected to calculate the given flow. However, you get absolutely no information on how the traffic was distributed inside that time range. This means that you have to make an assumption. A sensible one is to assume that, for short flow collection periods, traffic was more or less constant and, as such, you can uniformly distribute it across the time granularity you are using. There is nothing stopping you from considering more complicated schemes, but know that there is very little to gain from not taking the uniformly distributed approach. The last consideration pertains to the sampling_interval. This value is basically the inverse of a sampling rate applied by the flow collector, and it is important because you cannot calculate the amount of bytes or packets in a flow without it. If a flow has dOctets = 24 and the header has sampling_interval=10, then the number of bytes you have to report is 240, because your flow was calculated using only 10% of your network’s traffic. You will obviously incur in an approximation error with this, but your scope of operation is after the flow is collected, so you have no control over its sampling strategy. And even if you had, keep in mind that flow exports have to be produced at very high speeds and with minimal computational effort, which immediately discards the vast majority, if not all, sampling schemes that are not simple random sampling. ","date":"2023-08-13","objectID":"/netflow_v5/:4:0","tags":null,"title":"Understanding Netflow v5","uri":"/netflow_v5/"},{"categories":["Docker","Grafana"],"content":"How to add Loki, promtail, and Grafana to a local docker compose stack","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"When testing software locally, one of the main tools at a software engineer’s disposal is Docker, more particularly, the Docker Compose tool. This tool allows engineers to define and run a multi-container setup using YAML files. The vast majority of software produces a form of output known as logs, which provide information on what is happening in a running application, such as errors, possible warnings, general information, and more. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:0:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"The issue with inspecting Docker Compose log messages When you run a Docker Compose setup locally, you’ll usually see all container logs being printed in rapid succession in your terminal, and, depending on the amount of logs your multi-container stack produces, this might be very difficult to navigate, reason about, and correlate log messages from different containers. Moreover, if you are interested in a single container’s logs, you’d have to get the container ID from the output of docker ps and then get the logs with docker logs \u003cCONTAINER ID\u003e. And even then, you’d only see them in plaintext format printed on the terminal, which is a poor way of navigating logs if there are many of them. Surely there must be a better way. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:1:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Your favourite super hero to the rescue: Loki and his sidekick Promtail Loki is a log aggregation system designed by Grafana Labs and, most of all, it’s open source software, which is important because I don’t like paying for stuff that I can get for free. To add Loki to a Docker Compose stack, we have to first understand how it works. In itself, Loki is just an aggregation and storage system, it does not collect the logs by itself, nor does it provide a nice user interface to explore them. This means that we need to other components: one for collecting the logs, and another to query and visualise them. To collect the logs, we’ll use a piece of software called Promtail, which is an agent that takes local logs and sends them to a Loki instance. Additionally, it also handles target discovery and attaching labels to log streams. Finally, to query and visualise the logs, we’ll simply use a Grafana instance where we’ll use our Loki instance as a data source. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:2:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"The application we are monitoring Since this is just an example to showcase how to set up this particular monitoring stack, I created the most basic application I could think of. All it does is start and print a Monty Python quote every second: package main import ( \"log\" \"time\" ) func main() { for { log.Println(\"Oh! Now we see the violence inherent in the system! Help, help, I'm being repressed!\") time.Sleep(1 * time.Second) } } Containerising the application To run this application with Docker Compose, it has to be containerised. For this, I used a build-step container and you can read more about this on a previous post that I wrote: https://ornlu-is.github.io/slim_docker_images/ ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:3:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Setting up the log monitoring stack with Docker Compose There are several steps to this process. The first one is to create a docker-compose.yaml file with the following contents: version: \"3.8\" services: loki: container_name: loki hostname: loki image: grafana/loki ports: - 3100:3100 command: -config.file=/etc/loki/local-config.yaml volumes: - ./loki:/etc/loki/ promtail: container_name: promtail hostname: promtail image: grafana/promtail command: -config.file=/etc/promtail/docker-config.yaml volumes: - ./promtail/docker-config.yaml:/etc/promtail/docker-config.yaml - /var/lib/docker/containers:/var/lib/docker/containers:ro - /var/run/docker.sock:/var/run/docker.sock depends_on: - loki grafana: container_name: grafana hostname: grafana image: grafana/grafana environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin - GF_AUTH_DISABLE_LOGIN_FORM=true ports: - 3000:3000 volumes: - ./grafana/provisioning/:/etc/grafana/provisioning/ depends_on: - promtail app: container_name: app hostname: app build: context: . dockerfile: Dockerfile labels: logging: \"promtail\" logging_jobname: \"container_logs\" depends_on: - grafana If you didn’t limit yourself to copying the YAML file above and actually read it, you probably have some questions. There are a bunch of bind mounts defined in our docker-compose.yaml file. Each of them serves a different purpose and we’ll go through them one by one. Bind mounts Bind mounts are basically files or directories on the host machine that are mounted into a container, meaning that the container can access these files/directories. For Loki to know how it is configured, it needs, you’ve guessed it, a configuration file! So we create a directory aptly named loki, and place a local-config.yaml file inside with the following configuration: auth_enabled: false server: http_listen_port: 3100 common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2020-10-24 store: boltdb-shipper object_store: filesystem schema: v11 index: prefix: index_ period: 24h Note that this is just the default Loki configuration for a Loki instance running locally, so there is nothing particularly noteworthy here. However, the same is not true for Promtail! As before, we create a promtail directory with a docker-config.yaml inside it: server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: scraper docker_sd_configs: - host: unix:///var/run/docker.sock refresh_interval: 5s filters: - name: label values: [\"logging=promtail\"] relabel_configs: - source_labels: [\"__meta_docker_container_name\"] regex: \"/(.*)\" target_label: \"container\" - source_labels: [\"__meta_docker_container_log_stream\"] target_label: \"logstream\" - source_labels: [\"__meta_docker_container_label_logging_jobname\"] target_label: \"job\" Let us unpack this. In our docker-compose.yaml file we have added two labels to our application: the logging and the logging_jobname labels, with values promtail and container_logs, respectively. This is what is going to be used by Promtail to know what to scrape, i.e., what logs to collect, and how to relabel them. Additionally, we also have to specify the host under docker_sd_configs carefully, because it must be unix:///var/run/docker.sock, which is the socket to which Docker writes the application logs. There is only one thing left to configure, and that is Grafana. We will create yet another directory with the most unexpected name ever, grafana, and we’ll create two additional directories inside it: datasources and provisioning. Inside provisioning, we’ll create a dashboard.yaml file with the following contents: apiVersion: 1 providers: - name: 'Loki' orgId: 1 folder: '' type: file editable: true options: path: /etc/grafana/provisioning/dashboards And, inside the datasources direc","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:4:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Pre-built dashboards in a Docker Compose Grafana instance It is obviously very boring and laborious to always have to create the dashboard from scratch just so we can explore the log messages properly. Fortunately, this actually only needs to be done once! Spin up the Docker Compose environment and navigate to your Grafana instance at http://localhost:3000. Create a dashboard as you normally would and then, when you save it, copy its JSON definition to the clipboard. To have this dashboard created by default, just paste this JSON into a my_dashboard.json file and place it inside the grafana/provisioning directory. And you’re done, you’ll never have to create that dashboard ever again. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:5:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Dealing with the ‘Unauthorized’ error from the Grafana dashboard There is a chance that you get an “Unauthorized” error when attempting to access the Grafana dashboard on your Docker Compose Grafana instance. Fret not because this is exceedingly easy to solve. Simply open http://localhost:3000 in a private/incognito browser and you’ll no longer have this issue. This error pops up whenever you have already opened another Grafana instance in your browser before and thus your browser saved some Grafana session cookies that it then attempts to use for the instance you are running with Docker Compose. Obviously/hopefully this fails because these cookies are not a valid authorization method for the Grafana instance you just spun up. ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:6:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Grafana"],"content":"Solving the ’too many outstanding requests’ error At the time of writing this, Loki is not shipped with sensible defaults. This translates to, if you build a few panels in a single dashboard, you start getting empty panels and a “Too many outstanding requests” error in Grafana (I got this error after adding 6 panels, which is very little). This is because Loki is shipped with a few limits on how many requests from Grafana it can handle. After tinkering with these limits for a while, I found that adding the following lines to the Loki config file solved that issue: limits_config: split_queries_by_interval: 24h max_query_parallelism: 100 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 And now you’re all set to efficiently explore logs from Docker Compose environments. Link to the code https://github.com/ornlu-is/docker_compose_loki_example ","date":"2023-08-11","objectID":"/docker_compose_promtail_loki_grafana/:7:0","tags":null,"title":"Hooking Promtail, Loki, and Grafana to your Docker Compose stack","uri":"/docker_compose_promtail_loki_grafana/"},{"categories":["Docker","Clickhouse"],"content":"A simple way to run SQL scripts on Clickhouse Docker container startup.","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"This is a neat little trick I learned recently. I was in need of creating a Clickhouse container to attach to another application that would write to it. And I found myself thinking: it would be really convenient if there were a way to create a Clickhouse container that already comes with a set of tables created. Most of the solutions I found only were severely convoluted and relied on scripts to make this happen, so I thought that there must surely be a better way. As it turns out, there is, and this post is precisely about that. ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:0:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"The Docker image The first step is to specify our Clickhouse Dockerfile: FROM yandex/clickhouse-server ADD ./docker-entrypoint-initdb.d/ /docker-entrypoint-initdb.d Basically, all we do is take the base Clickhouse image and copy a local directory called docker-entrypoint-initdb.d into a folder of the same name inside the container. This is a special folder that Clickhouse will peer inside when being started as a Docker container. It will look for SQL scripts inside this directory and execute them on start up. Which means that, if we want a Clickhouse Docker image that already has a few existing tables, all we have to do is specify a few SQL scripts and write an exceedingly simple Dockerfile! ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:1:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"The SQL script This brings us to the second step: writing a basic SQL script so we can see this in action. Inside the docker-entrypoint-initdb.d directory, let’s write the following script: CREATE TABLE IF NOT EXISTS random_table ( `field1` Float, `field2` Float, `field3` Float ) ENGINE = Memory() We will name our script 1_create_random_table.sql so that we know what it does and we can be sure that it is the first script to run (assuming other scripts are also prefixed by numbers). Another important detail is that we added the IF NOT EXISTS modifier to our CREATE TABLE statement. This was no accident. To understand its purpose, consider the following scenario. You build and start your container and you see your tables have been written to Clickhouse, and, to celebrate this success, you stop the container a go grab a quick beer. While drinking the aforementioned beer, you brag to someone about your most recent accomplishment, but they do not believe you. With a fiery smirk, you prompt that someone to follow you to your computer. And there you go, half empty beer in hand, confidently walking to your computer. You sit down, type the command to start the container and… your container exits with a failure code immediately after starting. Your beer is suddenly warm, your confidence is gone, and the embarassment you feel from this debacle will keep you awake at night and eventually be the main cause of your divorce. Now, the lesson here is simple: without the IF NOT EXISTS modifier, when you run the container a second time without removing the leftover container from the first time you ran it, it will attempt to create the table again, and fail because it already exists, and then you’ll end up going through a ravaging divorce. Nobody wants that, so use this modifier. ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:2:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Docker","Clickhouse"],"content":"Inspecting the container Considering our SQL script is now divorce-proof, all that is left is for us is to test this out. In the same directory as our Dockerfile, simply run: docker build . -t 'ch-sql' where ch-sql is the tag I decided to give this image, but you can call it whatever you’d like. Now, when Docker finishes building the image, we can check its existence by inspecting the output of the docker images command: REPOSITORY TAG IMAGE ID CREATED SIZE ch-sql latest c3ec2b44d1e8 46 seconds ago 826MB Great, since our image is there, we are all set to create the container using docker run ch-sql, which outputs the following: Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Logging trace to /var/log/clickhouse-server/clickhouse-server.log Logging errors to /var/log/clickhouse-server/clickhouse-server.err.log Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/config.xml'. Processing configuration file '/etc/clickhouse-server/users.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/users.xml'. /entrypoint.sh: running /docker-entrypoint-initdb.d/1_create_random_table.sql Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Logging trace to /var/log/clickhouse-server/clickhouse-server.log Logging errors to /var/log/clickhouse-server/clickhouse-server.err.log Processing configuration file '/etc/clickhouse-server/config.xml'. Merging configuration file '/etc/clickhouse-server/config.d/docker_related_config.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/config.xml'. Processing configuration file '/etc/clickhouse-server/users.xml'. Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/users.xml'. I did not add the empty lines that separate the our script’s execution, Clickhouse did that itself, which is pretty convenient. All that is left is to exec into the container and examine if our table was actually created. Grab the container ID from the output of docker ps, and open an interactive terminal session inside the container with: docker exec -it \u003cCONTAINER-ID\u003e bash If you’ve managed to get here without any mistakes, you should now be inside the container image we built. Starting the Clickhouse client is very simple, all you have to do is type clickhouse-client and press Enter, and you’ll be presented with a Clickhouse prompt: ClickHouse client version 22.1.3.7 (official build). Connecting to localhost:9000 as user default. Connected to ClickHouse server version 22.1.3 revision 54455. CONTAINER_ID :) To check if our table exists, simply write SHOW TABLES FROM default, where default is the default database where we’ve created our table, well, by default. This will give us the following: SHOW TABLES FROM default Query id: 8052df88-1463-487a-a31d-9e8c504181c3 ┌─name─────────┐ │ random_table │ └──────────────┘ 1 rows in set. Elapsed: 0.005 sec. So now you’re all set to execute start up SQL scripts on a Clickhouse Docker container, hopefully this will end up saving your marriage! Link to the code https://github.com/ornlu-is/clickhouse_docker_start_up_scripts ","date":"2023-08-07","objectID":"/clickhouse_docker_precreated_tables/:3:0","tags":null,"title":"Hassle-free table creation on start up for Clickhouse Docker containers","uri":"/clickhouse_docker_precreated_tables/"},{"categories":["Golang"],"content":"A Git hook example for Go developers","date":"2023-07-31","objectID":"/git_hooks_for_go/","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"I mostly code in Go, which comes with the handy go tool. This tool has a bunch of functionalities with one of the most important (at least for me) being the ability to run tests. I love writing tests for my code because I hate being paged when I am on call. However, whenever I open a PR, sometimes I forget to run the tests locally before pushing my code and then my code ends up failing the CI builds, which overall results in a slower development process. Fortunately, I’m in the business of automation, and there is one particular tool that I can leverage so that I do not have to remember to run the tests every time: `git`` hooks. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:0:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"git hooks git hooks is just a fancy name for scripts that get executed when certain git events occur. It really is that simple. However, it is important to take a while to understand when these scripts are triggered. We will focus our attention on local hooks, which are ones that run on git events that happen in our machine and not in the remote repository. There are four hooks that allow us to have scripts being executed at each step of a commit’s lifecycle: pre-commit - after running git commit but before being prompted for a commit message; prepare-commit-msg - after the previous hook and is used to populated the commit message; commit-msg - after the commit message is entered; post-commit - immediately after the previous hook, but has the downside of not being able to change the outcome of git commit. Out of all of these options, we want one that will block the git commit command if any of the tests fails, which immediately excludes the last hook. And, to be honest, it is not necessary to have a the hook run after entering the commit message because if then the tests fail, I’ll have to fix them and enter the commit message once more. Since there is no need to populate the commit message with anything special, I’ll be going with a pre-commit hook. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:1:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"A Go example I need an example project to show this in action. So I came up with the following main.go file: package main import \"fmt\" func isEven(num int) bool { if num%2 == 0 { return true } return false } func main() { fmt.Println(\"1 is even?\", isEven(1)) fmt.Println(\"32 is even?\", isEven(32)) } As you can see, this Go application simply prints if 1 and 32 are even numbers. But it has an isEven function, for which we have written the following unit tests in the main_test.go file: package main import \"testing\" func TestIsEven(t *testing.T) { for _, tc := range []struct { name string givenNumber int expectedResult bool }{ { name: \"given an even number returns true\", givenNumber: 666, expectedResult: true, }, { name: \"given an odd number returns false\", givenNumber: 333, expectedResult: false, }, } { t.Run(tc.name, func(t *testing.T) { res := isEven(tc.givenNumber) if res != tc.expectedResult { t.Errorf(\"expected result %t, but got %t\", tc.expectedResult, res) } }) } } This is just an illustrative example, hence why the Go application is so uninteresting. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:2:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"Implementing a Python git hook We can can choose from several scripting languages to implement our hook. For this example, I chose to implement it in Python, for no particular reason. For that matter, I created a directory hooks/ and a file within it called pre-commit with the following code: #!/usr/bin/python3 import sys import subprocess process_output = subprocess.run([\"go\", \"test\", \"github.com/ornlu-is/go_git_hooks_example\"], text=True, capture_output=True) print(process_output.stdout) sys.exit(process_output.returncode) If you are not familiar with this, the first line of the script tells our computer which interpreter the script has to be ran with. In this case, it is Python 3, so if you do not have Python 3 installed, this Git hook will fail. Keep in mind that git hooks fails for any non-zero exit code, hence the last line of the script. ","date":"2023-07-31","objectID":"/git_hooks_for_go/:3:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Golang"],"content":"Adding Git hooks to our Go program But how do we actually run this? If you’ve explored the .git/ directory before, you probably know that it contains a hooks/ directory where, you’ve guessed it, git hooks usually live. But this makes it very hard to share hooks with your team members, since the contents of the .git folder are not added to the remote repository. My hooks are in the ./hooks/ directory, so I have to somehow tell git that this is where they are. Most recommendations for this paradigm revolve around using creating symlinks, but there is one very neat (and simple) one-liner that completely solves our issue: git config core.hooksPath hooks/ This line basically rewires git so that it looks for hooks in the ./hooks directory instead of in the ./.git/hooks/ directory! Moreover, this only affects the repository you are working on, meaning that if you have some different behaviour for another repository, it will be preserved. There is only one last step: our need to be executable, meaning that we just need to run: chmod +x hooks/pre-commit And we now have a functioning pre-commit git hook that will run our Go tests for us whenever we use the git commit command! Link to the code https://github.com/ornlu-is/go_git_hooks_example ","date":"2023-07-31","objectID":"/git_hooks_for_go/:4:0","tags":null,"title":"A pre-commit git hook for running Go unit tests","uri":"/git_hooks_for_go/"},{"categories":["Interesting Bugs"],"content":"CIDR handling in Go and Postgres do not match and that might cause bugs","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"I came across an interesting bug in the past few days. I had a very simple Go program that had a single purpose: it would take some user input, process that input, and then write it to a database. However, the program would sometimes fail, when given input that apparently was valid. And I thought that this bug was interesting enough to write about it, so here we are. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:0:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"Understanding CIDR notation Before diving into the actual bug, it is fundamental that we understand CIDR notation. Classless Internet Domain Routing, or CIDR, is notation used to define subnetworks, which is composed of an IP address followed by a forward slash and a number between zero and thirty two, e.g., 6.6.6.0/24. From a given CIDR, we can extract some information about the subnetwork: Network ID - this is what allows us to identify the network and corresponds to the IP address represented in the CIDR. In the example CIDR given above, this would be 6.6.6.0; Broadcast IP - this is the IP address that is used to broadcast messages to IP addresses in the network and corresponds to the last IP address in the IP range. for our example, it would be 6.6.6.255; First useable host IP - obviously, the actual first IP is the network ID, but that IP is not useable. However, the IP immediately after that one can actually be used and, in the context of our example, it would 6.6.6.1; Last useable host IP - similartly to the first useable host IP, the actual last IP is not useable as a host, but the one immediately before it is! So, for our running example, the last useable host IP would be 6.6.6.255; Number of IP addresses - this is fairly easy to calculate from the CIDR notation. Simply take the number that follows the forward slash, subtract it to 32, then take the power of 2 of the corresponding result. E.g., $32 - 24 = 8$, $2^8 = 256$, so for a /24 subnetwork, we have 256 IP addresses. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:1:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"The bug The functioning of my program was very simple. Programmed in Go, it would take a user given CIDR, parse it, and then write that CIDR into a Postgres database. Most of the times, this program would work perfectly, as expected. But, sometimes, it would fail. For example, for the input “6.6.6.0/24”, the program would function normally, but, for the input “3.3.1.0/16”, Postgres would throw an error. Now, there are two possible places where this error might be coming from: either from the Go application or the Postgres database. The Postgres table where the data was being written had a column of type cidr. From inspecting the documentation on Postgres network address types, we see that we really only had two options for this: cidr or inet, with the main difference between the two of them being that cidr does not allow for non-zero bits to the right of the network mask. With this bit of information, it should already be obvious what was going on: “3.3.1.0/16” is not a proper CIDR! The correct CIDR for the network it is trying to represent would be “3.3.0.0/16”. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:2:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"Golang CIDR parsing It is now very clear why Postgres was throwing an error, but why did this get through our Go application? The code snippet responsible for checking the CIDRs looks something like this: func checkCIDR(givenCIDR string) (*net.IPNet, error) { addr, inet, err := net.ParseCIDR(givenCIDR) if err != nil { return nil, fmt.Errorf(\"error parsing CIDR: %w\", err) } return \u0026net.IPNet{ IP: addr, Mask: inet.Mask, }, nil } If you take this code snippet and try to run it with the improper CIDR “3.3.1.0./16”, you’ll see that the code runs without returning an error! This is because the ParseCIDR function of the net package infers the subnet the CIDR refers to even if it is not a proper CIDR. If you look carefully at its returned values, we get both an IP address and an IP subnet. The IP address will correspond to the IP address given in the, sometimes improper, CIDR while the IP network will correspond to the inferred network. Meaning that Go would calculate the inferred network, but when we returned the value, we would return the improper CIDR, which then led to a Postgres error. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:3:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"The fix There were two possible ways of fixing this: either allowing users to specify improper CIDRs and having the application infer the correct network, or have the application reject all improper CIDRs. Ultimately, this is more of a user experience question. I prefer forcing users to know what they are playing around with, so, in my opinion, the fix should be to reject all improper CIDRs. This means that, we need to change the function that checks the CIDR to the following: func checkCIDR(givenCIDR string) (*net.IPNet, error) { addr, inet, err := net.ParseCIDR(givenCIDR) if err != nil { return nil, fmt.Errorf(\"error parsing CIDR: %w\", err) } if !addr.Equal(inet.IP) { return nil, fmt.Errorf(\"invalid CIDR provided\") } return \u0026net.IPNet{ IP: addr, Mask: inet.Mask, }, nil } You might, obviously, not want to take the same approach to fix this as I did, and that is completely fine: there is no universally correct approach to tackle this bug. Nonetheless, I thought this bug was a great learning opportunity and hopefully so do you. ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:4:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Interesting Bugs"],"content":"References Postgres documentation on network address types - https://www.postgresql.org/docs/current/datatype-net-types.html Golang net package documentation - https://pkg.go.dev/net ","date":"2023-07-30","objectID":"/go_postgres_cidr_mismatch/:5:0","tags":null,"title":"CIDRs and how they are handled by different systems","uri":"/go_postgres_cidr_mismatch/"},{"categories":["Golang"],"content":"When writing software, one might, accidentally (or not), ship the software with vulnerabilities, which are broadly defined as flaws or weaknesses in code that can be exploited by an attacker. We do not want to have those in our Go code so we need some way of minimizing the number of vulnerabilities our code has. Fortunately there are tools built by the Go community and team that can be leveraged for this. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:0:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"CWEs and CVEs Before introducing Go tools to make your code more secure, it is important to clarify two concepts that are commonly used in the infosec industry: CWE and CVE. The first stands for Common Weakness Enumeration and refers to a catalog of weaknesses in software components. It is important to keep in mind that this does not refer to specific instances of vulnerabilities, but rather types of weaknesses that are usually found in software, hardware or firmware. The second term stands for Common Vulnerabilities and Exposures, and is a standard for identifying and distributing information on vulnerabilities on specific instances of products or systems. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:1:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"gosec A popular Go tool that is sometimes ran as a linter, gosec scans your code’s abstract syntax tree for security problems. Since this works by examining an AST and is programmed as a set of rules, it is limited in its detection scope. In fact, at the time of writing this article, it can only detect 34 issues. Moreover, each of these possible detections is mapped into a CWE, as described in gosec’s GitHub repository. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:2:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"govulncheck Created by the Go security team, govulncheck is a bit more sophisticated than gosec. The Go security team gather data on known CVEs from multiple sources, puts these through a curation process, and makes this information publicly available. Moveover, this team also built the govulncheck tool, which allows us to check for these known vulnerabilities via source code inspection. However, if we really want to, it can also analyse binaries, but at the expense of information on call stacks. ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:3:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"Why you should run both these tools gosec looks for known CWEs, which may or may not result in CVEs, while govulncheck looks for known CVEs, making this pair a powerful stack to improve the security of you code. Let us craft an example where we can see these tools in action and how they differ. Create a new Go project, with version 1.19 (this is important, otherwise you will not be able to reproduce this), and place the following code in your main.go file: package main import ( \"fmt\" \"math/rand\" \"golang.org/x/text/unicode/norm\" ) func main() { word := \"cão\" // \"dog\" in portuguese nfc := norm.NFC.String(word) nfd := norm.NFD.String(word) fmt.Printf(\"NFC/NFD: %s/%s\\n\", nfc, nfd) fmt.Printf(\"This is a random number: %f\", rand.Float64()) } This will be our test subject. At a glance, there seems to be nothing wrong with our code, but let us see what gosec and govulncheck have to say about that. After following the documentation and installing these tools, they are fairly easy to run. Let us start by looking for CWEs with gosec, which can be done via the following command: gosec ./... When we run this, we get the following output: [gosec] 2023/07/26 23:44:50 Including rules: default [gosec] 2023/07/26 23:44:50 Excluding rules: default [gosec] 2023/07/26 23:44:50 Import directory: /home/luis/Projects/go_vulnerabilities_example [gosec] 2023/07/26 23:44:51 Checking package: main [gosec] 2023/07/26 23:44:51 Checking file: /home/luis/Projects/go_vulnerabilities_example/main.go Results: [/home/luis/Projects/go_vulnerabilities_example/main.go:17] - G404 (CWE-338): Use of weak random number generator (math/rand instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH) 16: fmt.Printf(\"NFC/NFD: %s/%s\\n\", nfc, nfd) \u003e 17: fmt.Printf(\"This is a random number: %f\", rand.Float64()) 18: } Summary: Gosec : 2.16.0 Files : 1 Lines : 18 Nosec : 0 Issues : 1 This tool immediately flagged the piece of code we were using to display a random number. Of course, for what the code does, this is a false positive result, but that isn’t always the case (and this is for illustrative purposes only). We can just as easily run govulncheck with: govulncheck ./... Which, in turn, outputs the following report: Using go1.19.1 and govulncheck@v1.0.0 with vulnerability data from https://vuln.go.dev (last modified 2023-07-24 16:24:24 +0000 UTC). Scanning your code and 44 packages across 1 dependent module for known vulnerabilities... Vulnerability #1: GO-2023-1840 Unsafe behavior in setuid/setgid binaries in runtime More info: https://pkg.go.dev/vuln/GO-2023-1840 Standard library Found in: runtime@go1.19.1 Fixed in: runtime@go1.20.5 Example traces found: #1: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.Callers #2: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.CallersFrames #3: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.Frames.Next #4: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.GOMAXPROCS #5: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.KeepAlive #6: main.go:4:2: go_vulnerabilities_example.init calls fmt.init, which eventually calls runtime.SetFinalizer #7: main.go:16:12: go_vulnerabilities_example.main calls fmt.Printf, which eventually calls runtime.TypeAssertionError.Error #8: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.defaultMemProfileRate #9: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.efaceOf #10: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.findfunc #11: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.float64frombits #12: main.go:5:2: go_vulnerabilities_example.init calls rand.init, which eventually calls runtime.forcegche","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:4:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"References gosec - https://github.com/securego/gosec govulncheck - https://go.googlesource.com/vuln ","date":"2023-07-25","objectID":"/go_detecting_vulnerabilities/:5:0","tags":null,"title":"Detecting Vulnerabilities in Go Code","uri":"/go_detecting_vulnerabilities/"},{"categories":["Golang"],"content":"How to implement the functional options design pattern in Golang","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Golang"],"content":"The Functional Options pattern is a rather elegant manner of implementing a Golang struct constructor that allows for custom default values, which means that users of the API we are implementing will only need to specify the struct attribute values that the users deem that shouldn’t take their default values. For our example, let us consider the very simple use case where we have a package named person containing a Person struct, which will look like this: type Person struct { ID int Name string Age int Email string Address string } ","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/:0:0","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Golang"],"content":"Before Functional Options To initialize such an instance of the aforementioned struct, one would usually implement a function called New, which would take a bunch of values as parameters and return the struct with the given values, like so: func New(id int, name string, age int, email string, address string) *Person { return \u0026Person{ ID: id, Name: name, Age: age, Email: email, Address: address, } } To create a new Person, one would have to call that function and pass in every single value: p := person.New(1, \"John Doe\", 25, \"johndoe@example.com\", \"Nowhere\") This might get cumborsome in the case where there are a lot of attributes and some of them might require more intricate knowledge of the inner workings of the package. ","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/:1:0","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Golang"],"content":"Functional Options Pattern Functional options to the rescue. The idea behind this pattern is fairly simple, we will have New become a variadic function which will accept any number of arguments of the type Option, which we define as: type Option func(*Person) Then, for every struct attribute that should have a default value, we implement a function of the following form: func WithAttributeName(attributeValue attributeType) Option { return func(p *Person) { p.AttributeName = attributeValue } } For our case, let us say we want all attributes except ID to have default values. In that case, we’d end up with something like: func WithName(name string) Option { return func(p *Person) { p.Name = name } } func WithAge(age int) Option { return func(p *Person) { p.Age = age } } func WithEmail(email string) Option { return func(p *Person) { p.Email = email } } func WithAddress(address string) Option { return func(p *Person) { p.Address = address } } Finally, all we need is adapt our New function to handle these other functions as parameters and to set some default values. This is simply boils down to creating the Person struct with the default values we want and then looping over and calling the given Options: func New(id int, opts ...Option) *Person { p := \u0026Person{ ID: id, Name: \"John Doe\", Age: 25, Email: \"johndoe@example.com\", Address: \"Nowhere\", } for _, opt := range opts { opt(p) } return p } Note that this method sets all attributes as optional except for the ID of the Person. Below is an example of initializing Person instances with functional options: func main() { unknownPerson := person.New(1) aragorn := person.New( 2, person.WithName(\"Aragorn II\"), person.WithAddress(\"Rivendell\"), person.WithAge(118), person.WithEmail(\"aragorn@mithrilmail.com\"), ) fmt.Printf(\"%+v\\n\", *unknownPerson) fmt.Printf(\"%+v\\n\", *aragorn) } This produces the following output: {ID:1 Name:John Doe Age:25 Email:johndoe@example.com Address:Nowhere} {ID:2 Name:Aragorn II Age:118 Email:aragorn@mithrilmail.com Address:Rivendell} Link to the code https://github.com/ornlu-is/go_functional_options ","date":"2023-07-20","objectID":"/go_design_pattern_functional_options/:2:0","tags":null,"title":"Go Design Patterns: Functional Options","uri":"/go_design_pattern_functional_options/"},{"categories":["Docker"],"content":"How to get slim Docker images using build-step containers.","date":"2023-07-15","objectID":"/slim_docker_images/","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Docker"],"content":"Docker images are supposed to be as small as possible, containing only what is absolutely required for the application inside them to run. In this post, I’ll go over build-step containers and how to use them with Docker. For that matter let us consider an example Go application, nothing fancy, like the one given by the code snippet below: package main import ( \"fmt\" \"net/http\" ) func rootPathHandler(w http.ResponseWriter, req *http.Request) { fmt.Fprintf(w, \"Strange women lying in ponds distributing swords is no basis for a system of government.\\n\") } func main() { http.HandleFunc(\"/\", rootPathHandler) err := http.ListenAndServe(\":8090\", nil) if err != nil { panic(err) } } This application simply starts a web server on port 8090 that prints a simple Monty Python quote. ","date":"2023-07-15","objectID":"/slim_docker_images/:0:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Docker"],"content":"Containerizing the application The first step is to write a Dockerfile in our root directory. I am going to be using the image for Golang 1.19, to match the version I am running locally, and have the image simply build and run the application. FROM golang:1.19 ADD . /askeladden WORKDIR /askeladden RUN go build ENTRYPOINT ./askeladden Firstly, we have to build our image, which is as simple as being in the same directory of the Dockerfile and running docker build . -f Dockerfile -t 'not-so-slim-shady' docker build . -f Dockerfile -t 'not-so-slim-shady' When we give the Docker CLI the build command, we are telling it to build an image. Images are later used to create containers, which are running instances of a given image. The . refers to the path of the context, which, in the context (ha-ha, funny guy) of Docker, refers to the set of files that can be used by the build process. The -f is shorthand notation for --file and is the path to the Dockerfile, while -t is short for --tag and allows us to name/tag our image. In this case, I only named it. We can check our image by listing all images built by Docker using docker images: REPOSITORY TAG IMAGE ID CREATED SIZE not-so-slim-shady latest 3673b4f2f4c8 14 seconds ago 1.07GB Great, the image is there and taking up 1.07GB! So now we have to create and run a container based on this image to check if our web server is working. This can be achieved by running: docker run -d -p 88:8090 not-so-slim-shady docker run -d -p 88:8090 not-so-slim-shady The run command creates and runs a container. The -d flag, which stands for --detach, will run the container in the background of the terminal, meaning it will not block the terminal window. Since the container has its own network, we have to expose the port 8090 where our web server can be accessed inside the container to outside the container. This is achieved by publishing the port and mapping it into a port on the host machine, i.e., my computer, with the -p flag followed by the specification of the host port and container port in the \u003chost_port\u003e:\u003ccontainer_port\u003e format. Finally, we specify the name of the image we want to build our container from which, in this case, is just not-so-slim-shady. Docker will first look for this image locally and, if it doesn’t find it, it will attempt to retrieve it from a public image repository. We can see the list of running containers by typing docker ps, and we can check that it is properly running by navigating to http://localhost:88/. So, now we’re fairly certain that our web server is containerized as desired, so we can stop the container using the docker stop \u003ccontainer_ID\u003e command (you can retrieve the container ID from the output of docker ps). Now the output of docker ps doesn’t show anything but the container wasn’t actually deleted, it was merely stopped. If we type docker ps -a, where -a stands for --all, we can see that our container is in an Exited status. To avoid taking up resources, let’s just delete it with docker rm \u003ccontainer_ID\u003e. ","date":"2023-07-15","objectID":"/slim_docker_images/:1:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":["Docker"],"content":"Using a build-step container The procedure above resulted in a Docker image based on the official Golang 1.19 image with our application shipped alongside it and, as we’ve seen above, our Docker image is about 1Gb in size, which is a tad bit too much. The reason the image is so large is because it has Golang installed, which we don’t actually require for any purpose after the web server is built. The idea is simple: we create a container with Golang 1.19 and build our binary, just as before. But after building it, we create another container, based on a Debian image, and simply inject our binary into it. We can then discard the container we used for building the web server and we are left with a slimmer Docker image. So the first step is to pick the Debian image that we’ll use. We have no special preference for the Debian version, so we can use the latest version, which is called bookworm. However, we will pick the bookworm-slim variant, which is a Debian bookworm version, but without extra files that aren’t usually required by containers, such as manual pages and documentation. FROM golang:1.19 AS builder ADD . /repo WORKDIR /repo RUN go build -o bin/example FROM debian:bookworm-slim COPY --from=builder /repo/bin/example /usr/bin/example ENTRYPOINT ./usr/bin/example Note that we gave the alias builder to our first container by specifying AS builder and then injected the binary built inside it into the slimmed docker image with COPY --from=builder. As before, we can build the image with: docker build . -f Dockerfile.buildstep -t 'slim-shady' Running docker images now gives us: REPOSITORY TAG IMAGE ID CREATED SIZE slim-shady latest 7c20d0afa6c3 4 seconds ago 81.3MB not-so-slim-shady latest 74f9c0e9c60d 15 minutes ago 1.07GB Which means that our new image is only 81.3Mb in size, 10 times less than what the previous image was. To check if this image is working as excepted, let us create a container based on it and publish it’s port so we can see our web server working by running: docker run -d -p 88:8090 slim-shady Navigating to http://localhost:8080/ in a browser shows that this is working as expected. Link to the code https://github.com/ornlu-is/slim-docker-image-example ","date":"2023-07-15","objectID":"/slim_docker_images/:2:0","tags":null,"title":"Slim Docker Images via Build-step Containers","uri":"/slim_docker_images/"},{"categories":null,"content":"Hi, I’m Luís! This is where I write about projects I’ve been working on, and also where I post some of my study notes, to keep everything organized and easily shareable. ","date":"2023-02-16","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work I currently work as a Systems Engineer for Cloudflare and previously I worked for freiheit.com technologies as a Software Engineer. ","date":"2023-02-16","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Academic studies I have a BSc in Engineering Physics and a MSc in Applied Mathematics, both granted by University of Lisbon - Instituto Superior Técnico. ","date":"2023-02-16","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Work-related interests My main interests are backend engineering, infrastructure, and statistical algorithms for problems at scale. ","date":"2023-02-16","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Socials GitHub LinkedIn ","date":"2023-02-16","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"}]